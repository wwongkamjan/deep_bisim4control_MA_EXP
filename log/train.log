{"episode_reward": 0.0, "episode": 1.0, "duration": 17.418400526046753, "step": 250}
{"episode_reward": 8.03772363506006, "episode": 2.0, "duration": 0.43480467796325684, "step": 500}
{"episode_reward": 17.16626315104322, "episode": 3.0, "duration": 0.42641758918762207, "step": 750}
{"episode_reward": 12.073072643141685, "episode": 4.0, "duration": 0.4398972988128662, "step": 1000}
{"episode_reward": 11.095187414981499, "episode": 5.0, "batch_reward": 0.04879455870275406, "critic_loss": 0.003421179564454234, "ae_transition_loss": 0.008791874389531757, "ae_encoder_loss": 0.0010019773568220218, "actor_loss": -0.38865587712527383, "actor_target_entropy": -6.0, "actor_entropy": 7.104933739132728, "alpha_loss": 0.0597969222502299, "alpha_value": 0.006386361129280925, "duration": 283.56485986709595, "step": 1250}
{"episode_reward": 17.681632142150942, "episode": 6.0, "batch_reward": 0.054675003901124, "critic_loss": 0.0022717047552578153, "ae_transition_loss": 0.0016643273960798978, "ae_encoder_loss": 0.001021120453486219, "actor_loss": -0.6380312051773072, "actor_target_entropy": -6.0, "actor_entropy": 5.754895065307617, "alpha_loss": 0.021973856538534165, "alpha_value": 0.004044551026696332, "duration": 54.39419174194336, "step": 1500}
{"episode_reward": 18.67527058583879, "episode": 7.0, "batch_reward": 0.057810245603322985, "critic_loss": 0.0025693819778971373, "ae_transition_loss": 0.0016247910931706429, "ae_encoder_loss": 0.00093442413629964, "actor_loss": -0.7260364727973938, "actor_target_entropy": -6.0, "actor_entropy": 5.563949600219726, "alpha_loss": 0.018024274438619613, "alpha_value": 0.0038516565241742988, "duration": 54.27337670326233, "step": 1750}
{"episode_reward": 16.890365111745997, "episode": 8.0, "batch_reward": 0.059606806486845014, "critic_loss": 0.0024473669957369567, "ae_transition_loss": 0.001603743554558605, "ae_encoder_loss": 0.0009974223768804222, "actor_loss": -0.8067521343231201, "actor_target_entropy": -6.0, "actor_entropy": 5.540420101165772, "alpha_loss": 0.015484733521938325, "alpha_value": 0.003691552597517539, "duration": 54.35047221183777, "step": 2000}
{"episode_reward": 17.42896162153824, "episode": 9.0, "batch_reward": 0.058531824678182603, "critic_loss": 0.0026606554365716873, "ae_transition_loss": 0.0015058846436440945, "ae_encoder_loss": 0.0010154705229215323, "actor_loss": -0.860557168006897, "actor_target_entropy": -6.0, "actor_entropy": 5.4510926895141605, "alpha_loss": 0.012925445899367333, "alpha_value": 0.0035497968090952136, "duration": 54.46274995803833, "step": 2250}
{"episode_reward": 9.52983661312882, "episode": 10.0, "batch_reward": 0.05705993142724037, "critic_loss": 0.0030376598653383555, "ae_transition_loss": 0.001435972997918725, "ae_encoder_loss": 0.0009546749929431826, "actor_loss": -0.9228606357574463, "actor_target_entropy": -6.0, "actor_entropy": 5.415002388000488, "alpha_loss": 0.01139407828450203, "alpha_value": 0.0034223836216032643, "duration": 54.41511416435242, "step": 2500}
