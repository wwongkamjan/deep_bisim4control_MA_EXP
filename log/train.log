{"episode_reward": 0.0, "episode": 1.0, "duration": 16.763075828552246, "step": 250}
{"episode_reward": 8.03772363506006, "episode": 2.0, "duration": 0.44202351570129395, "step": 500}
{"episode_reward": 17.16626315104322, "episode": 3.0, "duration": 0.40683841705322266, "step": 750}
{"episode_reward": 12.073072643141685, "episode": 4.0, "duration": 0.4144172668457031, "step": 1000}
{"episode_reward": 11.095187414981499, "episode": 5.0, "batch_reward": 0.04904027787552441, "critic_loss": 0.0032877182179082893, "ae_transition_loss": 0.008676535480214301, "ae_encoder_loss": 0.0010092541777993551, "actor_loss": -0.395024938580126, "actor_target_entropy": -6.0, "actor_entropy": 7.074699348614309, "alpha_loss": 0.058890969918384564, "alpha_value": 0.006393648235017123, "duration": 282.8323152065277, "step": 1250}
{"episode_reward": 17.3939822066757, "episode": 6.0, "batch_reward": 0.0537787993401289, "critic_loss": 0.0026065185079351067, "ae_transition_loss": 0.0015863688080571593, "ae_encoder_loss": 0.0010438381154090166, "actor_loss": -0.6850592699050904, "actor_target_entropy": -6.0, "actor_entropy": 6.133491966247559, "alpha_loss": 0.021827939778566362, "alpha_value": 0.00411069799410921, "duration": 54.50588083267212, "step": 1500}
{"episode_reward": 16.96582420927382, "episode": 7.0, "batch_reward": 0.056027195438742634, "critic_loss": 0.0025403269533999266, "ae_transition_loss": 0.0016489892364479602, "ae_encoder_loss": 0.0009265041195321828, "actor_loss": -0.7721074137687683, "actor_target_entropy": -6.0, "actor_entropy": 6.120601280212402, "alpha_loss": 0.021562039747834206, "alpha_value": 0.003899942242772529, "duration": 54.55138349533081, "step": 1750}
{"episode_reward": 18.234133174623913, "episode": 8.0, "batch_reward": 0.05748484805226326, "critic_loss": 0.002240169343072921, "ae_transition_loss": 0.0015341409505344927, "ae_encoder_loss": 0.0009040219853632153, "actor_loss": -0.8530692882537841, "actor_target_entropy": -6.0, "actor_entropy": 6.07518478012085, "alpha_loss": 0.01737239033728838, "alpha_value": 0.0037066598374691116, "duration": 54.70699167251587, "step": 2000}
{"episode_reward": 11.461696911043768, "episode": 9.0, "batch_reward": 0.057796250134706496, "critic_loss": 0.003138630638830364, "ae_transition_loss": 0.0016829452412202954, "ae_encoder_loss": 0.0008942775689065456, "actor_loss": -0.9039064931869507, "actor_target_entropy": -6.0, "actor_entropy": 5.944964023590088, "alpha_loss": 0.013418830409646034, "alpha_value": 0.0035477124788831065, "duration": 55.09239459037781, "step": 2250}
{"episode_reward": 24.326020680946385, "episode": 10.0, "batch_reward": 0.06259117276966572, "critic_loss": 0.003246858880389482, "ae_transition_loss": 0.0016615973222069442, "ae_encoder_loss": 0.0010302294676657767, "actor_loss": -0.9829714155197143, "actor_target_entropy": -6.0, "actor_entropy": 5.566882823944092, "alpha_loss": 0.009176640570163727, "alpha_value": 0.003430987477001146, "duration": 54.64793562889099, "step": 2500}
{"episode_reward": 26.745439720877553, "episode": 11.0, "batch_reward": 0.06577911631762981, "critic_loss": 0.004060373936779797, "ae_transition_loss": 0.001778706172015518, "ae_encoder_loss": 0.0012511810308787972, "actor_loss": -1.0701728868484497, "actor_target_entropy": -6.0, "actor_entropy": 5.402975582122803, "alpha_loss": 0.004705494318157435, "alpha_value": 0.0033537995507421318, "duration": 71.09169673919678, "step": 2750}
