{"episode_reward": 0.0, "episode": 1.0, "batch_reward": -0.00039587035939968405, "critic_loss": 0.013217832005563894, "ae_transition_loss": 0.0008723780641756849, "ae_encoder_loss": 0.003230938107978228, "actor_loss": -0.1194590406569698, "actor_target_entropy": -6.0, "actor_entropy": 4.69688405896274, "alpha_loss": 0.025240963354635614, "alpha_value": 0.003516298377038178, "duration": 1762.0880761146545, "step": 6331}
