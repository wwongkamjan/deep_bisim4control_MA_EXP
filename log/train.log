{"episode_reward": 0.0, "episode": 1.0, "duration": 203.80087065696716, "step": 1}
{"episode_reward": 0.0, "episode": 2.0, "duration": 0.4811820983886719, "step": 251}
{"episode_reward": 0.0, "episode": 3.0, "duration": 0.48312926292419434, "step": 501}
{"episode_reward": 0.0, "episode": 4.0, "duration": 0.483443021774292, "step": 751}
{"episode_reward": 0.0, "episode": 5.0, "batch_reward": -0.000138671875, "critic_loss": 0.04645880352333188, "ae_transition_loss": 0.00782426380092238, "ae_encoder_loss": 0.011464734956622124, "actor_loss": 0.13538999137096108, "actor_target_entropy": -6.0, "actor_entropy": 6.356484622478485, "alpha_loss": 0.05584120444208383, "alpha_value": 0.006682251462697602, "duration": 63.967631101608276, "step": 1001}
{"episode_reward": 0.0, "episode": 6.0, "batch_reward": -0.000359375, "critic_loss": 0.042682161293923854, "ae_transition_loss": 3.8085771528130865e-05, "ae_encoder_loss": 0.010977452120743691, "actor_loss": 0.05806606560945511, "actor_target_entropy": -6.0, "actor_entropy": 5.42021240234375, "alpha_loss": 0.028741415485739708, "alpha_value": 0.004398237169977148, "duration": 219.05545282363892, "step": 1251}
{"episode_reward": -3.0, "episode": 7.0, "batch_reward": -0.002640625, "critic_loss": 0.04270009271800518, "ae_transition_loss": 2.4274164912640118e-05, "ae_encoder_loss": 0.010943993845954538, "actor_loss": 0.045165674015879634, "actor_target_entropy": -6.0, "actor_entropy": 5.6418322029113765, "alpha_loss": 0.029184516310691834, "alpha_value": 0.0040966519233139286, "duration": 15.555164098739624, "step": 1501}
{"episode_reward": -3.0, "episode": 8.0, "batch_reward": -0.0042421875, "critic_loss": 0.04246929135173559, "ae_transition_loss": 2.4295023402373772e-05, "ae_encoder_loss": 0.01085007385071367, "actor_loss": 0.03234834191948176, "actor_target_entropy": -6.0, "actor_entropy": 5.7786518020629885, "alpha_loss": 0.025256204530596734, "alpha_value": 0.0038008890831669256, "duration": 15.567271709442139, "step": 1751}
{"episode_reward": -3.0, "episode": 9.0, "batch_reward": -0.0058671875, "critic_loss": 0.04152636232972145, "ae_transition_loss": 2.5919516920112072e-05, "ae_encoder_loss": 0.01064888648595661, "actor_loss": 0.02923750104010105, "actor_target_entropy": -6.0, "actor_entropy": 5.634487678527832, "alpha_loss": 0.023428458213806153, "alpha_value": 0.0035569603636564297, "duration": 15.560287952423096, "step": 2001}
{"episode_reward": -3.0, "episode": 10.0, "batch_reward": -0.0057421875, "critic_loss": 0.041078891213983296, "ae_transition_loss": 3.547027998502017e-05, "ae_encoder_loss": 0.010418419456109405, "actor_loss": 0.020751103453338145, "actor_target_entropy": -6.0, "actor_entropy": 5.3595060577392575, "alpha_loss": 0.01973602494597435, "alpha_value": 0.0033336265610865263, "duration": 15.620081663131714, "step": 2251}
{"episode_reward": -1.0, "episode": 11.0, "batch_reward": -0.0056484375, "critic_loss": 0.040005613699555395, "ae_transition_loss": 4.012066943323589e-05, "ae_encoder_loss": 0.010189814738929272, "actor_loss": 0.01764213553443551, "actor_target_entropy": -6.0, "actor_entropy": 5.738568693161011, "alpha_loss": 0.02464384812116623, "alpha_value": 0.0031152099255042217, "duration": 219.05303502082825, "step": 2501}
{"episode_reward": -3.0, "episode": 12.0, "batch_reward": -0.006984375, "critic_loss": 0.040287965886294845, "ae_transition_loss": 4.029253319458803e-05, "ae_encoder_loss": 0.010220970458351075, "actor_loss": 0.01326625632494688, "actor_target_entropy": -6.0, "actor_entropy": 5.7500339012146, "alpha_loss": 0.024327184543013573, "alpha_value": 0.002872946815613621, "duration": 15.563311576843262, "step": 2751}
{"episode_reward": -3.0, "episode": 13.0, "batch_reward": -0.0076015625, "critic_loss": 0.04056920574977994, "ae_transition_loss": 4.6910522276448316e-05, "ae_encoder_loss": 0.010298986240290105, "actor_loss": 0.009176646910607815, "actor_target_entropy": -6.0, "actor_entropy": 5.266109043121338, "alpha_loss": 0.02191097818315029, "alpha_value": 0.0026487602608862722, "duration": 15.551732301712036, "step": 3001}
{"episode_reward": -1.0, "episode": 14.0, "batch_reward": -0.0064765625, "critic_loss": 0.039404457066208126, "ae_transition_loss": 3.730180643833592e-05, "ae_encoder_loss": 0.009894557814113795, "actor_loss": 0.004337500821740832, "actor_target_entropy": -6.0, "actor_entropy": 5.479188861846924, "alpha_loss": 0.021357934921979904, "alpha_value": 0.0024536887929274094, "duration": 15.552577495574951, "step": 3251}
{"episode_reward": -1.0, "episode": 15.0, "batch_reward": -0.006203125, "critic_loss": 0.04055124904960394, "ae_transition_loss": 4.9351571033184885e-05, "ae_encoder_loss": 0.010431465175934136, "actor_loss": 0.0018807731148554013, "actor_target_entropy": -6.0, "actor_entropy": 5.364439401626587, "alpha_loss": 0.01937333942949772, "alpha_value": 0.002269413426894745, "duration": 15.608344554901123, "step": 3501}
{"episode_reward": 0.0, "episode": 16.0, "batch_reward": -0.0063125, "critic_loss": 0.040637027004733685, "ae_transition_loss": 0.004483271248580422, "ae_encoder_loss": 0.010367964370874687, "actor_loss": -0.0004583817812381312, "actor_target_entropy": -6.0, "actor_entropy": 5.20386501121521, "alpha_loss": 0.017285200148820876, "alpha_value": 0.002112432732948635, "duration": 45.55753827095032, "step": 3751}
{"episode_reward": 0.0, "episode": 17.0, "batch_reward": -0.0060078125, "critic_loss": 0.04080939771234989, "ae_transition_loss": 0.0015334055960993282, "ae_encoder_loss": 0.010571764530614019, "actor_loss": -0.00250237935828045, "actor_target_entropy": -6.0, "actor_entropy": 6.558335273742676, "alpha_loss": 0.01605644840747118, "alpha_value": 0.0019718259117767166, "duration": 15.556569576263428, "step": 4001}
{"episode_reward": 0.0, "episode": 18.0, "batch_reward": -0.0051953125, "critic_loss": 0.03930675821751356, "ae_transition_loss": 0.003953110957983882, "ae_encoder_loss": 0.010306810303591192, "actor_loss": 0.002108740346156992, "actor_target_entropy": -6.0, "actor_entropy": 5.992142433166504, "alpha_loss": 0.015179086051881313, "alpha_value": 0.001838814483039649, "duration": 15.52748990058899, "step": 4251}
{"episode_reward": 0.0, "episode": 19.0, "batch_reward": -0.0041796875, "critic_loss": 0.03700819217413664, "ae_transition_loss": 0.006018234603106976, "ae_encoder_loss": 0.010311144495382904, "actor_loss": 0.004371376782655716, "actor_target_entropy": -6.0, "actor_entropy": 4.850600206375122, "alpha_loss": 0.01124598129466176, "alpha_value": 0.0017312437733571803, "duration": 15.569876194000244, "step": 4501}
{"episode_reward": 0.0, "episode": 20.0, "batch_reward": -0.0045234375, "critic_loss": 0.033968127999454735, "ae_transition_loss": 0.006021444986574352, "ae_encoder_loss": 0.010100193942897021, "actor_loss": 0.0003380632164189592, "actor_target_entropy": -6.0, "actor_entropy": 5.331734525680542, "alpha_loss": 0.011172129593789578, "alpha_value": 0.0016421710675662536, "duration": 15.590982913970947, "step": 4751}
{"episode_reward": 0.0, "episode": 21.0, "batch_reward": -0.0042890625, "critic_loss": 0.03445290642976761, "ae_transition_loss": 0.005046304241754115, "ae_encoder_loss": 0.010784469712525606, "actor_loss": -0.010707679670071229, "actor_target_entropy": -6.0, "actor_entropy": 4.878175886154175, "alpha_loss": 0.009950959786772727, "alpha_value": 0.0015548670343078267, "duration": 45.669172525405884, "step": 5001}
{"episode_reward": 0.0, "episode": 22.0, "batch_reward": -0.0042421875, "critic_loss": 0.030840310741215946, "ae_transition_loss": 0.005848694535903633, "ae_encoder_loss": 0.011018435264006258, "actor_loss": -0.03425697151944041, "actor_target_entropy": -6.0, "actor_entropy": 4.793654399871826, "alpha_loss": 0.006925025120377541, "alpha_value": 0.0014864605496831336, "duration": 15.562524557113647, "step": 5251}
{"episode_reward": -1.0, "episode": 23.0, "batch_reward": -0.0040859375, "critic_loss": 0.030361376088112593, "ae_transition_loss": 0.005889651528559625, "ae_encoder_loss": 0.009973710222169757, "actor_loss": -0.03565852125175297, "actor_target_entropy": -6.0, "actor_entropy": 5.42284148979187, "alpha_loss": 0.007287577325012535, "alpha_value": 0.0014299455309268573, "duration": 15.549500703811646, "step": 5501}
{"episode_reward": -1.0, "episode": 24.0, "batch_reward": -0.003796875, "critic_loss": 0.032380036532878874, "ae_transition_loss": 0.005988756006583572, "ae_encoder_loss": 0.009831069031730295, "actor_loss": -0.03739525708556175, "actor_target_entropy": -6.0, "actor_entropy": 5.096507284164429, "alpha_loss": 0.006595995968207717, "alpha_value": 0.0013700681330664204, "duration": 15.535245418548584, "step": 5751}
{"episode_reward": 0.0, "episode": 25.0, "batch_reward": -0.00378125, "critic_loss": 0.03185838408768177, "ae_transition_loss": 0.006176234534010291, "ae_encoder_loss": 0.009347065748646856, "actor_loss": -0.039437907204031945, "actor_target_entropy": -6.0, "actor_entropy": 4.607831470489502, "alpha_loss": 0.007742479179054499, "alpha_value": 0.0013093277536278616, "duration": 15.510235786437988, "step": 6001}
{"episode_reward": -1.0, "episode": 26.0, "batch_reward": -0.0033671875, "critic_loss": 0.03172034829482436, "ae_transition_loss": 0.00618205141928047, "ae_encoder_loss": 0.009228906302712859, "actor_loss": -0.051363014355301856, "actor_target_entropy": -6.0, "actor_entropy": 4.826836381912232, "alpha_loss": 0.005498918398283422, "alpha_value": 0.0012465682414701691, "duration": 218.99254965782166, "step": 6251}
{"episode_reward": -3.0, "episode": 27.0, "batch_reward": -0.00471875, "critic_loss": 0.033352041106671095, "ae_transition_loss": 0.006713200317695737, "ae_encoder_loss": 0.009156127602793277, "actor_loss": -0.06559054726362229, "actor_target_entropy": -6.0, "actor_entropy": 4.5007719192504885, "alpha_loss": 0.002984601204370847, "alpha_value": 0.0012122759440229744, "duration": 15.535290241241455, "step": 6501}
{"episode_reward": -3.0, "episode": 28.0, "batch_reward": -0.004640625, "critic_loss": 0.03405698424950242, "ae_transition_loss": 0.006819231757894158, "ae_encoder_loss": 0.008931921172887087, "actor_loss": -0.0774772400856018, "actor_target_entropy": -6.0, "actor_entropy": 4.190199607849121, "alpha_loss": 0.002886616626754403, "alpha_value": 0.0011818839441536514, "duration": 15.575491189956665, "step": 6751}
{"episode_reward": -3.0, "episode": 29.0, "batch_reward": -0.0051171875, "critic_loss": 0.03492157508060336, "ae_transition_loss": 0.006746440112590789, "ae_encoder_loss": 0.009203920177184046, "actor_loss": -0.081237194865942, "actor_target_entropy": -6.0, "actor_entropy": 5.225725179672241, "alpha_loss": 0.005521163390949368, "alpha_value": 0.0011477842458355232, "duration": 15.552296876907349, "step": 7001}
{"episode_reward": -3.0, "episode": 30.0, "batch_reward": -0.00459375, "critic_loss": 0.03454079683497548, "ae_transition_loss": 0.006395602147094905, "ae_encoder_loss": 0.0090677006887272, "actor_loss": -0.09708938300609589, "actor_target_entropy": -6.0, "actor_entropy": 4.923682676315307, "alpha_loss": 0.004137086300179362, "alpha_value": 0.001098899585209883, "duration": 15.591767311096191, "step": 7251}
{"episode_reward": -3.0, "episode": 31.0, "batch_reward": -0.00525, "critic_loss": 0.0335074169319123, "ae_transition_loss": 0.006053862617351115, "ae_encoder_loss": 0.008843799142166972, "actor_loss": -0.10687729078531265, "actor_target_entropy": -6.0, "actor_entropy": 4.795478761672974, "alpha_loss": 0.004308685782598331, "alpha_value": 0.0010631049298755787, "duration": 219.38167595863342, "step": 7501}
{"episode_reward": -3.0, "episode": 32.0, "batch_reward": -0.0050546875, "critic_loss": 0.03437282736226916, "ae_transition_loss": 0.0062628632104024294, "ae_encoder_loss": 0.009376521978527308, "actor_loss": -0.11215551668405532, "actor_target_entropy": -6.0, "actor_entropy": 5.302779722213745, "alpha_loss": 0.00434287653490901, "alpha_value": 0.0010170710579343752, "duration": 15.612038135528564, "step": 7751}
{"episode_reward": 0.0, "episode": 33.0, "batch_reward": -0.0055859375, "critic_loss": 0.03279931654036045, "ae_transition_loss": 0.006124176158569753, "ae_encoder_loss": 0.009360477197915315, "actor_loss": -0.11921479421854018, "actor_target_entropy": -6.0, "actor_entropy": 5.267770755767822, "alpha_loss": 0.005169372631236911, "alpha_value": 0.000968417237379529, "duration": 15.596276998519897, "step": 8001}
{"episode_reward": -1.0, "episode": 34.0, "batch_reward": -0.005390625, "critic_loss": 0.03266192526742816, "ae_transition_loss": 0.006031897791661322, "ae_encoder_loss": 0.00940350576583296, "actor_loss": -0.12110965645313262, "actor_target_entropy": -6.0, "actor_entropy": 4.97334009552002, "alpha_loss": 0.004212679043412208, "alpha_value": 0.0009196283329732085, "duration": 15.610665321350098, "step": 8251}
