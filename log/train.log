{"episode_reward": 0.0, "episode": 1.0, "batch_reward": -0.00023738240700692042, "critic_loss": 0.04789092899928559, "ae_transition_loss": 0.003434271858091256, "ae_encoder_loss": 0.012028336153965069, "actor_loss": 0.031231379268636218, "actor_target_entropy": -6.0, "actor_entropy": 5.530128829865064, "alpha_loss": 0.04574490570157298, "alpha_value": 0.005524157562370738, "duration": 779.508927822113, "step": 2313}
{"episode_reward": -1.0, "episode": 2.0, "batch_reward": -0.002080880065766086, "critic_loss": 0.05048344374216836, "ae_transition_loss": 6.52835422134431e-05, "ae_encoder_loss": 0.012989116146206363, "actor_loss": -0.045809057300577834, "actor_target_entropy": -6.0, "actor_entropy": 5.5259728658928475, "alpha_loss": 0.015126089004079041, "alpha_value": 0.0021611644585429325, "duration": 170.36589407920837, "step": 5126}
