{"episode_reward": 0.0, "episode": 1.0, "duration": 18.115428686141968, "step": 250}
{"episode_reward": 7.747346039045906, "episode": 2.0, "duration": 0.41481971740722656, "step": 500}
{"episode_reward": 4.427573337125385, "episode": 3.0, "duration": 0.4133737087249756, "step": 750}
{"episode_reward": 5.050386443524035, "episode": 4.0, "duration": 0.41248321533203125, "step": 1000}
{"episode_reward": 17.85499204884596, "episode": 5.0, "batch_reward": 0.036205128187222706, "critic_loss": 0.005919788387888152, "ae_transition_loss": 0.013458834222951879, "ae_encoder_loss": 0.0011905383100232455, "actor_loss": -0.22930113515639314, "actor_target_entropy": -6.0, "actor_entropy": 6.546252015222434, "alpha_loss": 0.05583870335269945, "alpha_value": 0.006468950648864262, "duration": 283.8064317703247, "step": 1250}
{"episode_reward": 16.87728160236647, "episode": 6.0, "batch_reward": 0.04157383023202419, "critic_loss": 0.0038145286557264625, "ae_transition_loss": 0.0032122697718441487, "ae_encoder_loss": 0.0021367621310055254, "actor_loss": -0.4648716344833374, "actor_target_entropy": -6.0, "actor_entropy": 5.3611163177490235, "alpha_loss": 0.02316752791404724, "alpha_value": 0.004219597457832686, "duration": 54.32870531082153, "step": 1500}
{"episode_reward": 13.231839383201848, "episode": 7.0, "batch_reward": 0.04627051255106926, "critic_loss": 0.005620907346718013, "ae_transition_loss": 0.003433460352011025, "ae_encoder_loss": 0.0020127320839092135, "actor_loss": -0.5427088575363159, "actor_target_entropy": -6.0, "actor_entropy": 5.12797964477539, "alpha_loss": 0.01626101955026388, "alpha_value": 0.004014702856393414, "duration": 54.3111093044281, "step": 1750}
{"episode_reward": 15.986516209871645, "episode": 8.0, "batch_reward": 0.04687345331907272, "critic_loss": 0.0047142926864326, "ae_transition_loss": 0.002655524220317602, "ae_encoder_loss": 0.001604125261772424, "actor_loss": -0.6316640033721924, "actor_target_entropy": -6.0, "actor_entropy": 4.86766370010376, "alpha_loss": 0.016091297164559364, "alpha_value": 0.0038431451882686607, "duration": 54.28701376914978, "step": 2000}
{"episode_reward": 13.406101472494225, "episode": 9.0, "batch_reward": 0.047566899701952936, "critic_loss": 0.004555857166647911, "ae_transition_loss": 0.0023743862216360867, "ae_encoder_loss": 0.0014601926140021533, "actor_loss": -0.7221299715042114, "actor_target_entropy": -6.0, "actor_entropy": 4.631768280029297, "alpha_loss": 0.013856551200151444, "alpha_value": 0.003676091494690852, "duration": 54.28584814071655, "step": 2250}
{"episode_reward": 11.99952755186522, "episode": 10.0, "batch_reward": 0.04744790500402451, "critic_loss": 0.009095754788257181, "ae_transition_loss": 0.0030079610077664255, "ae_encoder_loss": 0.0014401288509834557, "actor_loss": -0.768691252231598, "actor_target_entropy": -6.0, "actor_entropy": 4.583947969436646, "alpha_loss": 0.01168586192652583, "alpha_value": 0.0035329073264873828, "duration": 54.31914687156677, "step": 2500}
{"episode_reward": 11.76401667848549, "episode": 11.0, "batch_reward": 0.047081793755292894, "critic_loss": 0.005612181003205478, "ae_transition_loss": 0.0026481784330680967, "ae_encoder_loss": 0.0011449526264332235, "actor_loss": -0.8210206456184387, "actor_target_entropy": -6.0, "actor_entropy": 4.6518446063995365, "alpha_loss": 0.010426066502928735, "alpha_value": 0.0034036068042471823, "duration": 70.46304655075073, "step": 2750}
{"episode_reward": 12.823670662441074, "episode": 12.0, "batch_reward": 0.04892788414657116, "critic_loss": 0.003699439159594476, "ae_transition_loss": 0.0019406564394012094, "ae_encoder_loss": 0.0010453316702041776, "actor_loss": -0.8968362097740173, "actor_target_entropy": -6.0, "actor_entropy": 4.132917665481568, "alpha_loss": 0.007440577322617173, "alpha_value": 0.0032926325895918716, "duration": 54.27850317955017, "step": 3000}
{"episode_reward": 15.435322993998623, "episode": 13.0, "batch_reward": 0.0508410232514143, "critic_loss": 0.004691936821676791, "ae_transition_loss": 0.0018603542437776923, "ae_encoder_loss": 0.001073395939078182, "actor_loss": -0.9967574529647827, "actor_target_entropy": -6.0, "actor_entropy": 3.753447956085205, "alpha_loss": 0.004971118754241615, "alpha_value": 0.003212804777155697, "duration": 54.39436078071594, "step": 3250}
{"episode_reward": 23.906119130565685, "episode": 14.0, "batch_reward": 0.05432006645202637, "critic_loss": 0.004597488024272025, "ae_transition_loss": 0.001858683154452592, "ae_encoder_loss": 0.0011122150116134434, "actor_loss": -1.0511344618797303, "actor_target_entropy": -6.0, "actor_entropy": 3.6955650310516357, "alpha_loss": 0.003819769049063325, "alpha_value": 0.0031588679209126496, "duration": 54.65017628669739, "step": 3500}
{"episode_reward": 27.65574652517744, "episode": 15.0, "batch_reward": 0.05971181564033032, "critic_loss": 0.008015050742775202, "ae_transition_loss": 0.0023462364827282725, "ae_encoder_loss": 0.0014933633035980165, "actor_loss": -1.1333417558670045, "actor_target_entropy": -6.0, "actor_entropy": 3.8969868049621583, "alpha_loss": 0.0011188398419180886, "alpha_value": 0.0031127044872057657, "duration": 54.74450635910034, "step": 3750}
{"episode_reward": 27.003886912195, "episode": 16.0, "batch_reward": 0.06190639926493168, "critic_loss": 0.007411255558952689, "ae_transition_loss": 0.0023596437787637113, "ae_encoder_loss": 0.0016532937111333012, "actor_loss": -1.2237184658050537, "actor_target_entropy": -6.0, "actor_entropy": 3.729494724273682, "alpha_loss": -0.0022162379482761024, "alpha_value": 0.0031252780615675254, "duration": 54.76503252983093, "step": 4000}
{"episode_reward": 36.39231378229024, "episode": 17.0, "batch_reward": 0.06750478833913803, "critic_loss": 0.007485743183642626, "ae_transition_loss": 0.0025152852134779097, "ae_encoder_loss": 0.0019472850039601326, "actor_loss": -1.3255211296081544, "actor_target_entropy": -6.0, "actor_entropy": 3.8729654846191406, "alpha_loss": -0.003247686274233274, "alpha_value": 0.0031708306844090593, "duration": 54.79316711425781, "step": 4250}
{"episode_reward": 35.17620054220105, "episode": 18.0, "batch_reward": 0.07083076576888561, "critic_loss": 0.00891855561733246, "ae_transition_loss": 0.002775102428160608, "ae_encoder_loss": 0.0021822565905749796, "actor_loss": -1.4193691806793214, "actor_target_entropy": -6.0, "actor_entropy": 4.048833751678467, "alpha_loss": -0.00354042126564309, "alpha_value": 0.0032262821856324886, "duration": 54.831663846969604, "step": 4500}
{"episode_reward": 19.838409086613115, "episode": 19.0, "batch_reward": 0.07066113990545272, "critic_loss": 0.009024309424683451, "ae_transition_loss": 0.002683654636144638, "ae_encoder_loss": 0.0021522532105445863, "actor_loss": -1.5196172637939453, "actor_target_entropy": -6.0, "actor_entropy": 4.20913674736023, "alpha_loss": -0.0048376908064819875, "alpha_value": 0.003302618561221883, "duration": 54.82047963142395, "step": 4750}
{"episode_reward": 15.216327188409954, "episode": 20.0, "batch_reward": 0.0697871612906456, "critic_loss": 0.007551878610625863, "ae_transition_loss": 0.0026625016871839762, "ae_encoder_loss": 0.002184211350977421, "actor_loss": -1.6271248683929442, "actor_target_entropy": -6.0, "actor_entropy": 4.173439863204956, "alpha_loss": -0.005036822836846113, "alpha_value": 0.0034088730522530697, "duration": 54.82848358154297, "step": 5000}
{"episode_reward": 13.574754868877267, "episode": 21.0, "batch_reward": 0.07066620346903801, "critic_loss": 0.00922199278511107, "ae_transition_loss": 0.0026747809164226056, "ae_encoder_loss": 0.002403380104340613, "actor_loss": -1.7245139980316162, "actor_target_entropy": -6.0, "actor_entropy": 4.123910976409912, "alpha_loss": -0.004066799096297473, "alpha_value": 0.003510960546304223, "duration": 71.70807313919067, "step": 5250}
{"episode_reward": 21.342999553487406, "episode": 22.0, "batch_reward": 0.07261481474339962, "critic_loss": 0.010268405131995678, "ae_transition_loss": 0.0028792291013523936, "ae_encoder_loss": 0.002701684245839715, "actor_loss": -1.814239845275879, "actor_target_entropy": -6.0, "actor_entropy": 4.140343616485596, "alpha_loss": -0.0036054723029956223, "alpha_value": 0.0036045722747821917, "duration": 54.77818942070007, "step": 5500}
{"episode_reward": 33.28073962828036, "episode": 23.0, "batch_reward": 0.07289311569929123, "critic_loss": 0.01099990844167769, "ae_transition_loss": 0.002976359073072672, "ae_encoder_loss": 0.002875657670199871, "actor_loss": -1.8900216426849366, "actor_target_entropy": -6.0, "actor_entropy": 4.1474435405731205, "alpha_loss": -0.0025139621172565966, "alpha_value": 0.00369286836806399, "duration": 54.8070650100708, "step": 5750}
{"episode_reward": 17.365719036199224, "episode": 24.0, "batch_reward": 0.07295103096961975, "critic_loss": 0.011686828251928091, "ae_transition_loss": 0.002970921787433326, "ae_encoder_loss": 0.0028961612000130118, "actor_loss": -1.9815233097076417, "actor_target_entropy": -6.0, "actor_entropy": 3.982357074737549, "alpha_loss": -0.0016191496476531029, "alpha_value": 0.0037507172112757558, "duration": 54.81604337692261, "step": 6000}
{"episode_reward": 24.911445245317253, "episode": 25.0, "batch_reward": 0.07596476662158966, "critic_loss": 0.013840232498943806, "ae_transition_loss": 0.0031355103515088556, "ae_encoder_loss": 0.003276769483461976, "actor_loss": -2.080545732498169, "actor_target_entropy": -6.0, "actor_entropy": 3.9919002437591553, "alpha_loss": -0.0023713098011212425, "alpha_value": 0.0038134076228932318, "duration": 54.82291769981384, "step": 6250}
{"episode_reward": 25.9434604806226, "episode": 26.0, "batch_reward": 0.07464935337007046, "critic_loss": 0.012632383039221167, "ae_transition_loss": 0.0031016575060784815, "ae_encoder_loss": 0.0032419368932023645, "actor_loss": -2.1475767517089843, "actor_target_entropy": -6.0, "actor_entropy": 3.884467884063721, "alpha_loss": -0.0025251604543300343, "alpha_value": 0.003899703613264996, "duration": 54.817333698272705, "step": 6500}
{"episode_reward": 12.171573821957667, "episode": 27.0, "batch_reward": 0.07482935106754303, "critic_loss": 0.01408283095806837, "ae_transition_loss": 0.0030401123678311707, "ae_encoder_loss": 0.003142118659801781, "actor_loss": -2.2326976642608645, "actor_target_entropy": -6.0, "actor_entropy": 3.7013393058776853, "alpha_loss": -0.001562088807928376, "alpha_value": 0.00398271503375888, "duration": 54.83375144004822, "step": 6750}
{"episode_reward": 23.044453059856394, "episode": 28.0, "batch_reward": 0.07481075891852379, "critic_loss": 0.01629156092554331, "ae_transition_loss": 0.003103171609342098, "ae_encoder_loss": 0.0031711719408631326, "actor_loss": -2.3124069728851318, "actor_target_entropy": -6.0, "actor_entropy": 3.653020215988159, "alpha_loss": -0.0007855170428520069, "alpha_value": 0.004024110373639062, "duration": 54.832826375961304, "step": 7000}
{"episode_reward": 25.41001004373325, "episode": 29.0, "batch_reward": 0.07676802137494088, "critic_loss": 0.018186757948249577, "ae_transition_loss": 0.003180786610580981, "ae_encoder_loss": 0.0032592810550704596, "actor_loss": -2.4157034702301026, "actor_target_entropy": -6.0, "actor_entropy": 3.5771457672119142, "alpha_loss": -0.00033743621374014764, "alpha_value": 0.004044959095247303, "duration": 54.81527233123779, "step": 7250}
{"episode_reward": 37.14103836949541, "episode": 30.0, "batch_reward": 0.07905804401636124, "critic_loss": 0.02025497333332896, "ae_transition_loss": 0.0033430737322196363, "ae_encoder_loss": 0.003581466513685882, "actor_loss": -2.5153498077392578, "actor_target_entropy": -6.0, "actor_entropy": 3.475655590057373, "alpha_loss": -0.002044460004195571, "alpha_value": 0.004101163551588108, "duration": 54.823105335235596, "step": 7500}
{"episode_reward": 30.495829124893582, "episode": 31.0, "batch_reward": 0.08040061327815055, "critic_loss": 0.021088719829916953, "ae_transition_loss": 0.003288140246644616, "ae_encoder_loss": 0.0034857003921642898, "actor_loss": -2.619980525970459, "actor_target_entropy": -6.0, "actor_entropy": 3.405562738418579, "alpha_loss": -0.001693116827053018, "alpha_value": 0.004200990743664313, "duration": 71.63445043563843, "step": 7750}
{"episode_reward": 35.054565229855754, "episode": 32.0, "batch_reward": 0.08153651350736618, "critic_loss": 0.022776752542704344, "ae_transition_loss": 0.003317749229259789, "ae_encoder_loss": 0.003688553153537214, "actor_loss": -2.7071956195831297, "actor_target_entropy": -6.0, "actor_entropy": 3.507924060821533, "alpha_loss": -0.0010900888295145704, "alpha_value": 0.004271919259553159, "duration": 54.83087229728699, "step": 8000}
{"episode_reward": 18.455028364366726, "episode": 33.0, "batch_reward": 0.08072914323210716, "critic_loss": 0.02077076193317771, "ae_transition_loss": 0.0031253347313031556, "ae_encoder_loss": 0.0034784674858674407, "actor_loss": -2.79451314163208, "actor_target_entropy": -6.0, "actor_entropy": 3.655940279006958, "alpha_loss": -0.0013967985198250971, "alpha_value": 0.004345595943187153, "duration": 54.844205141067505, "step": 8250}
{"episode_reward": 12.152634114011404, "episode": 34.0, "batch_reward": 0.0817488417327404, "critic_loss": 0.02574426755309105, "ae_transition_loss": 0.0032188213756307957, "ae_encoder_loss": 0.003777409409172833, "actor_loss": -2.8643370113372804, "actor_target_entropy": -6.0, "actor_entropy": 3.827418710708618, "alpha_loss": -0.0008860611169366166, "alpha_value": 0.004424880803507756, "duration": 54.840301513671875, "step": 8500}
{"episode_reward": 36.8441568425107, "episode": 35.0, "batch_reward": 0.08218406483530999, "critic_loss": 0.02485175906866789, "ae_transition_loss": 0.0032762807700783014, "ae_encoder_loss": 0.0038440849604085086, "actor_loss": -2.9382278060913087, "actor_target_entropy": -6.0, "actor_entropy": 3.7944901237487794, "alpha_loss": -0.0008231694541173056, "alpha_value": 0.004480711201690938, "duration": 54.861775636672974, "step": 8750}
{"episode_reward": 22.264604770090386, "episode": 36.0, "batch_reward": 0.08480830356478691, "critic_loss": 0.030775581143796443, "ae_transition_loss": 0.003343495481647551, "ae_encoder_loss": 0.004135557876899838, "actor_loss": -3.0367141723632813, "actor_target_entropy": -6.0, "actor_entropy": 3.9444925079345703, "alpha_loss": -0.0009013449349440635, "alpha_value": 0.0045383454612115464, "duration": 54.86224699020386, "step": 9000}
{"episode_reward": 43.55562442641122, "episode": 37.0, "batch_reward": 0.08579947707057, "critic_loss": 0.029474605225026606, "ae_transition_loss": 0.003413052094168961, "ae_encoder_loss": 0.004266597956418991, "actor_loss": -3.1226774253845213, "actor_target_entropy": -6.0, "actor_entropy": 4.019062334060669, "alpha_loss": -0.0014633169093867765, "alpha_value": 0.004634336181175828, "duration": 54.83914375305176, "step": 9250}
{"episode_reward": 25.861007867411992, "episode": 38.0, "batch_reward": 0.08725691765546799, "critic_loss": 0.03596273674815893, "ae_transition_loss": 0.0034226870806887745, "ae_encoder_loss": 0.004478521746583283, "actor_loss": -3.2284186325073243, "actor_target_entropy": -6.0, "actor_entropy": 4.081888748168946, "alpha_loss": -0.0022804195106728, "alpha_value": 0.004788820047458619, "duration": 54.86813306808472, "step": 9500}
{"episode_reward": 42.26957376786877, "episode": 39.0, "batch_reward": 0.0891610541343689, "critic_loss": 0.03794493266940117, "ae_transition_loss": 0.0033891525203362106, "ae_encoder_loss": 0.004525389201939106, "actor_loss": -3.3314851303100586, "actor_target_entropy": -6.0, "actor_entropy": 4.0879508514404295, "alpha_loss": -0.0019652472675661557, "alpha_value": 0.005008366082233164, "duration": 54.837393045425415, "step": 9750}
{"episode_reward": 29.72379856721023, "episode": 40.0, "batch_reward": 0.08995188137888908, "critic_loss": 0.04177761736512184, "ae_transition_loss": 0.003494373423978686, "ae_encoder_loss": 0.004699365015141666, "actor_loss": -3.437972936630249, "actor_target_entropy": -6.0, "actor_entropy": 4.122095169067383, "alpha_loss": -0.0014442919309949503, "alpha_value": 0.005190888151071604, "duration": 54.849106550216675, "step": 10000}
{"episode_reward": 38.38394647289032, "episode": 41.0, "batch_reward": 0.09015368020534516, "critic_loss": 0.04454032396525145, "ae_transition_loss": 0.0034345358749851584, "ae_encoder_loss": 0.004657800051383674, "actor_loss": -3.537060411453247, "actor_target_entropy": -6.0, "actor_entropy": 4.063556798934936, "alpha_loss": -0.0009425339441513643, "alpha_value": 0.005308552125327561, "duration": 79.00732731819153, "step": 10250}
{"episode_reward": 21.3170266019417, "episode": 42.0, "batch_reward": 0.09070494884252549, "critic_loss": 0.04177723696827888, "ae_transition_loss": 0.0034299281472340228, "ae_encoder_loss": 0.004814780368469655, "actor_loss": -3.606129026412964, "actor_target_entropy": -6.0, "actor_entropy": 3.9787351665496824, "alpha_loss": -0.0015775584337534383, "alpha_value": 0.005474470883343158, "duration": 54.821667432785034, "step": 10500}
{"episode_reward": 27.081901024012414, "episode": 43.0, "batch_reward": 0.09286540621519089, "critic_loss": 0.04697511838376522, "ae_transition_loss": 0.003665439151227474, "ae_encoder_loss": 0.005172816677950323, "actor_loss": -3.6885388984680176, "actor_target_entropy": -6.0, "actor_entropy": 4.155014181137085, "alpha_loss": -0.000798480617813766, "alpha_value": 0.005631670589464482, "duration": 54.860451459884644, "step": 10750}
{"episode_reward": 48.52626744185389, "episode": 44.0, "batch_reward": 0.09396436640620232, "critic_loss": 0.04904576101899147, "ae_transition_loss": 0.0038883788716048, "ae_encoder_loss": 0.005493273822590709, "actor_loss": -3.7928797454833982, "actor_target_entropy": -6.0, "actor_entropy": 4.107275674819946, "alpha_loss": -0.0006828839286463336, "alpha_value": 0.005734073773309655, "duration": 54.75996232032776, "step": 11000}
{"episode_reward": 52.9640703732991, "episode": 45.0, "batch_reward": 0.09531715670228004, "critic_loss": 0.05301330268383026, "ae_transition_loss": 0.003936450771987439, "ae_encoder_loss": 0.0057119856365025045, "actor_loss": -3.8668149814605712, "actor_target_entropy": -6.0, "actor_entropy": 4.129278978347778, "alpha_loss": 0.0009651754194637761, "alpha_value": 0.005780018594508168, "duration": 54.58440160751343, "step": 11250}
{"episode_reward": 27.54672679422257, "episode": 46.0, "batch_reward": 0.09690173056721688, "critic_loss": 0.0668054486066103, "ae_transition_loss": 0.004198905386030674, "ae_encoder_loss": 0.006031458262354136, "actor_loss": -4.008023750305176, "actor_target_entropy": -6.0, "actor_entropy": 4.019719011306763, "alpha_loss": 0.0005168221074272879, "alpha_value": 0.005598354116821202, "duration": 54.59815764427185, "step": 11500}
{"episode_reward": 38.87043213876839, "episode": 47.0, "batch_reward": 0.09800635460019112, "critic_loss": 0.07244173657894135, "ae_transition_loss": 0.004270596778951585, "ae_encoder_loss": 0.006243602337315679, "actor_loss": -4.112412452697754, "actor_target_entropy": -6.0, "actor_entropy": 3.9576339645385743, "alpha_loss": -0.0005641776466509327, "alpha_value": 0.005620853156968155, "duration": 54.5583393573761, "step": 11750}
{"episode_reward": 30.294863290324237, "episode": 48.0, "batch_reward": 0.10005539134144784, "critic_loss": 0.07551485474407672, "ae_transition_loss": 0.004429339004680515, "ae_encoder_loss": 0.0066731827650219205, "actor_loss": -4.254373992919922, "actor_target_entropy": -6.0, "actor_entropy": 3.8012152309417724, "alpha_loss": -0.000821011321269907, "alpha_value": 0.00568161238633351, "duration": 54.59817814826965, "step": 12000}
{"episode_reward": 69.07228146678109, "episode": 49.0, "batch_reward": 0.10238115325570106, "critic_loss": 0.08120388865470886, "ae_transition_loss": 0.00436565544269979, "ae_encoder_loss": 0.006841186722740531, "actor_loss": -4.360726612091065, "actor_target_entropy": -6.0, "actor_entropy": 3.9257217636108397, "alpha_loss": -0.002496924855862744, "alpha_value": 0.006016706862092579, "duration": 54.61851692199707, "step": 12250}
{"episode_reward": 37.089165329048086, "episode": 50.0, "batch_reward": 0.10329032373428344, "critic_loss": 0.09676141135394573, "ae_transition_loss": 0.004280639480799437, "ae_encoder_loss": 0.006809547698125243, "actor_loss": -4.503204273223877, "actor_target_entropy": -6.0, "actor_entropy": 4.1151266860961915, "alpha_loss": -0.0013614573937375098, "alpha_value": 0.00644598820601984, "duration": 54.63161635398865, "step": 12500}
{"episode_reward": 50.116116696685644, "episode": 51.0, "batch_reward": 0.10612138277292252, "critic_loss": 0.0926082294434309, "ae_transition_loss": 0.004175168233923614, "ae_encoder_loss": 0.007031480848789215, "actor_loss": -4.662270656585694, "actor_target_entropy": -6.0, "actor_entropy": 4.248428995132446, "alpha_loss": -0.002412917665671557, "alpha_value": 0.006839404070795633, "duration": 70.8985013961792, "step": 12750}
{"episode_reward": 40.22107520444306, "episode": 52.0, "batch_reward": 0.10782700672745704, "critic_loss": 0.1026599270105362, "ae_transition_loss": 0.004393665005452931, "ae_encoder_loss": 0.007985775737091898, "actor_loss": -4.758986946105957, "actor_target_entropy": -6.0, "actor_entropy": 4.372613492965698, "alpha_loss": -0.0011924958860035986, "alpha_value": 0.007371683838329597, "duration": 54.59144878387451, "step": 13000}
{"episode_reward": 89.68385232177015, "episode": 53.0, "batch_reward": 0.11052700066566468, "critic_loss": 0.12161804032325745, "ae_transition_loss": 0.0047861247695982455, "ae_encoder_loss": 0.008786377063021064, "actor_loss": -4.883673313140869, "actor_target_entropy": -6.0, "actor_entropy": 4.51460461807251, "alpha_loss": 0.0006131805210607127, "alpha_value": 0.007377573501136262, "duration": 54.68141460418701, "step": 13250}
{"episode_reward": 25.14898335414232, "episode": 54.0, "batch_reward": 0.112577416151762, "critic_loss": 0.16205114802718162, "ae_transition_loss": 0.0051181925907731055, "ae_encoder_loss": 0.009490748628973962, "actor_loss": -5.011393337249756, "actor_target_entropy": -6.0, "actor_entropy": 4.706897384643555, "alpha_loss": -0.0016889569705817848, "alpha_value": 0.0073610096020519265, "duration": 54.87053894996643, "step": 13500}
{"episode_reward": 71.21192614602828, "episode": 55.0, "batch_reward": 0.11381698185205459, "critic_loss": 0.1869102817773819, "ae_transition_loss": 0.005738603195175529, "ae_encoder_loss": 0.009837558075785636, "actor_loss": -5.094251224517822, "actor_target_entropy": -6.0, "actor_entropy": 4.742232551574707, "alpha_loss": -2.5921137537807225e-05, "alpha_value": 0.00765931042916723, "duration": 54.86172795295715, "step": 13750}
{"episode_reward": 34.173303409553235, "episode": 56.0, "batch_reward": 0.11460553535819054, "critic_loss": 0.17937034478783606, "ae_transition_loss": 0.005766543209552765, "ae_encoder_loss": 0.009764875262975693, "actor_loss": -5.2100192756652834, "actor_target_entropy": -6.0, "actor_entropy": 4.6695962829589845, "alpha_loss": -0.0014090952966362238, "alpha_value": 0.007896936065236282, "duration": 54.873435258865356, "step": 14000}
{"episode_reward": 38.16617327830172, "episode": 57.0, "batch_reward": 0.11486012187600136, "critic_loss": 0.17357489785552024, "ae_transition_loss": 0.005445811972022057, "ae_encoder_loss": 0.009519202062860131, "actor_loss": -5.357433910369873, "actor_target_entropy": -6.0, "actor_entropy": 4.683302013397217, "alpha_loss": -0.0027589341460261493, "alpha_value": 0.008494927698264685, "duration": 54.88158941268921, "step": 14250}
{"episode_reward": 32.099111857811366, "episode": 58.0, "batch_reward": 0.11636687350273132, "critic_loss": 0.20024448812007903, "ae_transition_loss": 0.005311365880072117, "ae_encoder_loss": 0.009626262014731764, "actor_loss": -5.489995544433594, "actor_target_entropy": -6.0, "actor_entropy": 4.808868328094483, "alpha_loss": -0.0012331816125661136, "alpha_value": 0.009200632947210559, "duration": 54.87121653556824, "step": 14500}
{"episode_reward": 71.43026879372547, "episode": 59.0, "batch_reward": 0.11897398617863655, "critic_loss": 0.20264308285713195, "ae_transition_loss": 0.005460838017985224, "ae_encoder_loss": 0.009808283559978008, "actor_loss": -5.591940788269043, "actor_target_entropy": -6.0, "actor_entropy": 4.918881519317627, "alpha_loss": -0.0003843301425222307, "alpha_value": 0.009453783868843131, "duration": 54.887022733688354, "step": 14750}
{"episode_reward": 64.77650388038246, "episode": 60.0, "batch_reward": 0.12096881565451623, "critic_loss": 0.238667426943779, "ae_transition_loss": 0.005699027609080076, "ae_encoder_loss": 0.010477895334362984, "actor_loss": -5.7117022590637205, "actor_target_entropy": -6.0, "actor_entropy": 4.964849807739258, "alpha_loss": -0.0011123589291237295, "alpha_value": 0.009598419443245047, "duration": 54.885857820510864, "step": 15000}
{"episode_reward": 51.02126826912688, "episode": 61.0, "batch_reward": 0.12205028849840165, "critic_loss": 0.2539868642091751, "ae_transition_loss": 0.005923968672752381, "ae_encoder_loss": 0.010978436641395093, "actor_loss": -5.845550014495849, "actor_target_entropy": -6.0, "actor_entropy": 5.037532096862793, "alpha_loss": 0.00037466604821383956, "alpha_value": 0.009762599202048515, "duration": 71.91538286209106, "step": 15250}
{"episode_reward": 29.43513759621335, "episode": 62.0, "batch_reward": 0.12168015637993812, "critic_loss": 0.27677767968177797, "ae_transition_loss": 0.006322335951030255, "ae_encoder_loss": 0.011155078530311585, "actor_loss": -5.967366867065429, "actor_target_entropy": -6.0, "actor_entropy": 4.920665271759034, "alpha_loss": 0.0006745463868137449, "alpha_value": 0.009562392758968502, "duration": 54.88896584510803, "step": 15500}
{"episode_reward": 51.69828446735409, "episode": 63.0, "batch_reward": 0.12354949590563774, "critic_loss": 0.31122639429569243, "ae_transition_loss": 0.006511184813454747, "ae_encoder_loss": 0.011437416434288025, "actor_loss": -6.070640789031982, "actor_target_entropy": -6.0, "actor_entropy": 5.0157706108093265, "alpha_loss": -0.0001371037686476484, "alpha_value": 0.009559278777481726, "duration": 54.86635184288025, "step": 15750}
{"episode_reward": 29.695380257503228, "episode": 64.0, "batch_reward": 0.12345332595705986, "critic_loss": 0.30453921645879745, "ae_transition_loss": 0.006387677589431405, "ae_encoder_loss": 0.011533997297286987, "actor_loss": -6.178792282104492, "actor_target_entropy": -6.0, "actor_entropy": 4.930051330566406, "alpha_loss": -0.0013934893382247537, "alpha_value": 0.009807243813678087, "duration": 54.8699848651886, "step": 16000}
{"episode_reward": 71.59127153708017, "episode": 65.0, "batch_reward": 0.12494063484668731, "critic_loss": 0.37509412729740144, "ae_transition_loss": 0.0068323134277015925, "ae_encoder_loss": 0.012425632607191801, "actor_loss": -6.283085166931152, "actor_target_entropy": -6.0, "actor_entropy": 4.985819160461426, "alpha_loss": 0.0011962238169508056, "alpha_value": 0.00991659302389292, "duration": 54.886908769607544, "step": 16250}
{"episode_reward": 31.944362615325904, "episode": 66.0, "batch_reward": 0.12570670080184937, "critic_loss": 0.43960217815637587, "ae_transition_loss": 0.006855852477252483, "ae_encoder_loss": 0.01228859046474099, "actor_loss": -6.423309600830078, "actor_target_entropy": -6.0, "actor_entropy": 4.968288356781006, "alpha_loss": 0.00045733777759596704, "alpha_value": 0.009534395812592031, "duration": 54.897488594055176, "step": 16500}
{"episode_reward": 35.41649036327221, "episode": 67.0, "batch_reward": 0.12574093037843703, "critic_loss": 0.4000864615440369, "ae_transition_loss": 0.0067450656127184625, "ae_encoder_loss": 0.011971188444644213, "actor_loss": -6.522624927520752, "actor_target_entropy": -6.0, "actor_entropy": 4.955088386535644, "alpha_loss": -0.0010137515313690527, "alpha_value": 0.009493083826162681, "duration": 54.90531015396118, "step": 16750}
{"episode_reward": 30.86827461949871, "episode": 68.0, "batch_reward": 0.1256807689368725, "critic_loss": 0.5027004190683365, "ae_transition_loss": 0.007203301850706339, "ae_encoder_loss": 0.012468743845820427, "actor_loss": -6.638493152618408, "actor_target_entropy": -6.0, "actor_entropy": 4.968363574981689, "alpha_loss": -0.0009097244599834085, "alpha_value": 0.009907500305104905, "duration": 54.90169405937195, "step": 17000}
{"episode_reward": 48.710882939376376, "episode": 69.0, "batch_reward": 0.1268827436864376, "critic_loss": 0.5347959411144256, "ae_transition_loss": 0.0074603810999542474, "ae_encoder_loss": 0.01296109526976943, "actor_loss": -6.736229267120361, "actor_target_entropy": -6.0, "actor_entropy": 4.963555938720703, "alpha_loss": -0.00041968485910911114, "alpha_value": 0.010181664172752007, "duration": 54.9230842590332, "step": 17250}
{"episode_reward": 18.75728714265091, "episode": 70.0, "batch_reward": 0.12538602644205094, "critic_loss": 0.6299943964481354, "ae_transition_loss": 0.007971643496304751, "ae_encoder_loss": 0.013545888159424067, "actor_loss": -6.857453693389893, "actor_target_entropy": -6.0, "actor_entropy": 4.9239468727111815, "alpha_loss": -0.0008064739541150629, "alpha_value": 0.010278983067592153, "duration": 54.908196687698364, "step": 17500}
