{"episode_reward": 0.0, "episode": 1.0, "duration": 17.21865701675415, "step": 250}
{"episode_reward": 8.03772363506006, "episode": 2.0, "duration": 0.4537014961242676, "step": 500}
{"episode_reward": 17.16626315104322, "episode": 3.0, "duration": 0.4164619445800781, "step": 750}
{"episode_reward": 12.073072643141685, "episode": 4.0, "duration": 0.430462121963501, "step": 1000}
{"episode_reward": 11.095187414981499, "episode": 5.0, "batch_reward": 0.04862103489838284, "critic_loss": 0.0033568325950463994, "ae_transition_loss": 0.008087283168215487, "ae_encoder_loss": 0.0009268214087844755, "actor_loss": -0.3674522901278692, "actor_target_entropy": -6.0, "actor_entropy": 6.970686361458802, "alpha_loss": 0.06059220439200936, "alpha_value": 0.006371246594401359, "duration": 284.68389081954956, "step": 1250}
{"episode_reward": 18.263651981224488, "episode": 6.0, "batch_reward": 0.054243879094719887, "critic_loss": 0.0020625734133645894, "ae_transition_loss": 0.002040406855288893, "ae_encoder_loss": 0.001326075107557699, "actor_loss": -0.59095103597641, "actor_target_entropy": -6.0, "actor_entropy": 6.380764652252197, "alpha_loss": 0.03019480641186237, "alpha_value": 0.003931316672311019, "duration": 54.5082905292511, "step": 1500}
{"episode_reward": 17.791511616338664, "episode": 7.0, "batch_reward": 0.054238567009568216, "critic_loss": 0.0024321913556195796, "ae_transition_loss": 0.0019498407179489733, "ae_encoder_loss": 0.0012205241587944329, "actor_loss": -0.6584867992401123, "actor_target_entropy": -6.0, "actor_entropy": 6.210114421844483, "alpha_loss": 0.023213052570819855, "alpha_value": 0.003693324376301214, "duration": 54.30068898200989, "step": 1750}
{"episode_reward": 10.160198378372293, "episode": 8.0, "batch_reward": 0.05437165945768356, "critic_loss": 0.0021656124582514168, "ae_transition_loss": 0.0018138771899975836, "ae_encoder_loss": 0.001131023293826729, "actor_loss": -0.7239982171058654, "actor_target_entropy": -6.0, "actor_entropy": 6.3228353385925296, "alpha_loss": 0.02007029078900814, "alpha_value": 0.00350361138574116, "duration": 54.480685234069824, "step": 2000}
{"episode_reward": 13.248689387612222, "episode": 9.0, "batch_reward": 0.05388469204306603, "critic_loss": 0.0022554676001891494, "ae_transition_loss": 0.0016930923899635673, "ae_encoder_loss": 0.0010485180770047008, "actor_loss": -0.7600185794830322, "actor_target_entropy": -6.0, "actor_entropy": 6.412917686462403, "alpha_loss": 0.016806688465178014, "alpha_value": 0.003332367856476618, "duration": 54.445698738098145, "step": 2250}
{"episode_reward": 27.029659511286162, "episode": 10.0, "batch_reward": 0.06020514568686485, "critic_loss": 0.0035333805354312063, "ae_transition_loss": 0.0021291613951325418, "ae_encoder_loss": 0.0019202996334061026, "actor_loss": -0.82977778673172, "actor_target_entropy": -6.0, "actor_entropy": 6.1304436225891115, "alpha_loss": 0.01070932562276721, "alpha_value": 0.0032022464719532627, "duration": 54.44958996772766, "step": 2500}
{"episode_reward": 17.576195559306086, "episode": 11.0, "batch_reward": 0.062438861429691316, "critic_loss": 0.004710203608497977, "ae_transition_loss": 0.0024084181636571883, "ae_encoder_loss": 0.002116040199995041, "actor_loss": -0.8946317734718323, "actor_target_entropy": -6.0, "actor_entropy": 6.196192901611328, "alpha_loss": 0.008797595851123333, "alpha_value": 0.003109437000585867, "duration": 70.2794291973114, "step": 2750}
