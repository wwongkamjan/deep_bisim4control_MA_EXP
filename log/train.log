{"episode_reward": 0.0, "episode": 1.0, "duration": 18.090904712677002, "step": 250}
{"episode_reward": 7.747346039045906, "episode": 2.0, "duration": 0.41123318672180176, "step": 500}
{"episode_reward": 4.427573337125385, "episode": 3.0, "duration": 0.41044044494628906, "step": 750}
{"episode_reward": 5.050386443524035, "episode": 4.0, "duration": 0.4114706516265869, "step": 1000}
{"episode_reward": 17.85499204884596, "episode": 5.0, "batch_reward": 0.03536178776062279, "critic_loss": 0.004667721175317622, "ae_transition_loss": 0.011407863133135077, "ae_encoder_loss": 0.0012516184843764577, "actor_loss": -0.28595240038547837, "actor_target_entropy": -6.0, "actor_entropy": 6.7770096829353275, "alpha_loss": 0.05915716867993364, "alpha_value": 0.006423959629267029, "duration": 284.6893219947815, "step": 1250}
{"episode_reward": 11.850988659268365, "episode": 6.0, "batch_reward": 0.04003966280817985, "critic_loss": 0.0025528515838086603, "ae_transition_loss": 0.0022821153989061715, "ae_encoder_loss": 0.0013568261673208325, "actor_loss": -0.4890207557678223, "actor_target_entropy": -6.0, "actor_entropy": 6.030498741149902, "alpha_loss": 0.026944907911121845, "alpha_value": 0.004013256842899243, "duration": 54.82247519493103, "step": 1500}
{"episode_reward": 18.85607450557994, "episode": 7.0, "batch_reward": 0.046321295589208604, "critic_loss": 0.0028917251573875547, "ae_transition_loss": 0.002086393374949694, "ae_encoder_loss": 0.0013327712349127977, "actor_loss": -0.5613879883289337, "actor_target_entropy": -6.0, "actor_entropy": 5.571984077453613, "alpha_loss": 0.015449337907135487, "alpha_value": 0.003814244927266849, "duration": 54.797956228256226, "step": 1750}
{"episode_reward": 16.086005644395037, "episode": 8.0, "batch_reward": 0.04820807680487633, "critic_loss": 0.0037219100957736374, "ae_transition_loss": 0.002349862980656326, "ae_encoder_loss": 0.0015004075551405549, "actor_loss": -0.6303206877708435, "actor_target_entropy": -6.0, "actor_entropy": 5.424328025817871, "alpha_loss": 0.011365381211042405, "alpha_value": 0.003679804065145618, "duration": 54.806334257125854, "step": 2000}
{"episode_reward": 19.34056777006201, "episode": 9.0, "batch_reward": 0.051294216185808185, "critic_loss": 0.0038837950360029938, "ae_transition_loss": 0.002341589055489749, "ae_encoder_loss": 0.001622575224377215, "actor_loss": -0.7207054891586304, "actor_target_entropy": -6.0, "actor_entropy": 5.123143810272217, "alpha_loss": 0.008307111338712274, "alpha_value": 0.003588506390874889, "duration": 54.79537749290466, "step": 2250}
{"episode_reward": 13.805543927676897, "episode": 10.0, "batch_reward": 0.05240307749807835, "critic_loss": 0.0030448558703064918, "ae_transition_loss": 0.001913194301072508, "ae_encoder_loss": 0.001454965644981712, "actor_loss": -0.7876085023880005, "actor_target_entropy": -6.0, "actor_entropy": 4.895631072998047, "alpha_loss": 0.007290240736678243, "alpha_value": 0.003498338752524866, "duration": 54.819762229919434, "step": 2500}
{"episode_reward": 12.62554722439256, "episode": 11.0, "batch_reward": 0.05083099104464054, "critic_loss": 0.003347829444333911, "ae_transition_loss": 0.0018566886018961668, "ae_encoder_loss": 0.0013202542399521917, "actor_loss": -0.8435659747123718, "actor_target_entropy": -6.0, "actor_entropy": 4.734374652862549, "alpha_loss": 0.007600042812526226, "alpha_value": 0.003419951092209121, "duration": 71.5586416721344, "step": 2750}
