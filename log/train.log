{"episode_reward": 0.0, "episode": 1.0, "duration": 35.73987436294556, "step": 250}
{"episode_reward": 7.747346039045906, "episode": 2.0, "duration": 1.2862913608551025, "step": 500}
{"episode_reward": 4.427573337125385, "episode": 3.0, "duration": 1.3235766887664795, "step": 750}
{"episode_reward": 5.050386443524035, "episode": 4.0, "duration": 1.306842565536499, "step": 1000}
{"episode_reward": 17.85499204884596, "episode": 5.0, "batch_reward": 0.03552490899904524, "critic_loss": 0.005790734099611204, "ae_transition_loss": 0.01142517360159568, "ae_encoder_loss": 0.0012210815705193446, "actor_loss": -0.3562723033203084, "actor_target_entropy": -6.0, "actor_entropy": 6.886896180512642, "alpha_loss": 0.057331722940639776, "alpha_value": 0.006443533753053027, "duration": 281.4551899433136, "step": 1250}
{"episode_reward": 13.006723714081385, "episode": 6.0, "batch_reward": 0.04045959584414959, "critic_loss": 0.006771085893735289, "ae_transition_loss": 0.003815639884211123, "ae_encoder_loss": 0.0014280989670660347, "actor_loss": -0.6154265916347503, "actor_target_entropy": -6.0, "actor_entropy": 6.470072002410888, "alpha_loss": 0.02369938449561596, "alpha_value": 0.004151242525722925, "duration": 54.30298471450806, "step": 1500}
{"episode_reward": 15.357844709714108, "episode": 7.0, "batch_reward": 0.042852067843079565, "critic_loss": 0.0037953293081372974, "ae_transition_loss": 0.002056236572097987, "ae_encoder_loss": 0.0010336602972820402, "actor_loss": -0.7035791811943054, "actor_target_entropy": -6.0, "actor_entropy": 5.847993923187256, "alpha_loss": 0.015127971358597279, "alpha_value": 0.003949108806086926, "duration": 54.32232880592346, "step": 1750}
{"episode_reward": 12.031562223039701, "episode": 8.0, "batch_reward": 0.043376647397875785, "critic_loss": 0.005799728470854461, "ae_transition_loss": 0.002475638973992318, "ae_encoder_loss": 0.0012050120653584598, "actor_loss": -0.8035704321861267, "actor_target_entropy": -6.0, "actor_entropy": 5.209147480010986, "alpha_loss": 0.010843596398830414, "alpha_value": 0.0038170629248203457, "duration": 54.33047676086426, "step": 2000}
{"episode_reward": 11.912369950999583, "episode": 9.0, "batch_reward": 0.04362414336204529, "critic_loss": 0.0058003412755206225, "ae_transition_loss": 0.0019855652609840034, "ae_encoder_loss": 0.0010397996911779045, "actor_loss": -0.9043025460243225, "actor_target_entropy": -6.0, "actor_entropy": 4.947953662872314, "alpha_loss": 0.007233969394117593, "alpha_value": 0.003714619016924806, "duration": 54.37153220176697, "step": 2250}
{"episode_reward": 9.685975061033002, "episode": 10.0, "batch_reward": 0.042917773187160495, "critic_loss": 0.004405299909412861, "ae_transition_loss": 0.0016976066511124373, "ae_encoder_loss": 0.0008206084324046969, "actor_loss": -0.9435085964202881, "actor_target_entropy": -6.0, "actor_entropy": 4.85921097946167, "alpha_loss": 0.005508121596765704, "alpha_value": 0.003644901435106072, "duration": 54.38002562522888, "step": 2500}
