{"episode_reward": 0.0, "episode": 1.0, "duration": 16.858500957489014, "step": 250}
{"episode_reward": 7.747346039045906, "episode": 2.0, "duration": 0.4108893871307373, "step": 500}
{"episode_reward": 4.427573337125385, "episode": 3.0, "duration": 0.4051995277404785, "step": 750}
{"episode_reward": 5.050386443524035, "episode": 4.0, "duration": 0.4040563106536865, "step": 1000}
{"episode_reward": 17.85499204884596, "episode": 5.0, "batch_reward": 0.03548862167698942, "critic_loss": 0.006094947156721534, "ae_transition_loss": 0.012408098391359009, "ae_encoder_loss": 0.0013214329093703262, "actor_loss": -0.40413637451323753, "actor_target_entropy": -6.0, "actor_entropy": 6.80503491362643, "alpha_loss": 0.057884104045280306, "alpha_value": 0.006445860295221156, "duration": 280.9549090862274, "step": 1250}
{"episode_reward": 15.759811356658956, "episode": 6.0, "batch_reward": 0.04091055254638195, "critic_loss": 0.003481128136627376, "ae_transition_loss": 0.0026689698891714216, "ae_encoder_loss": 0.0015592449135147035, "actor_loss": -0.6777990140914917, "actor_target_entropy": -6.0, "actor_entropy": 5.447162261962891, "alpha_loss": 0.026622095957398416, "alpha_value": 0.00407521844512858, "duration": 53.786309242248535, "step": 1500}
{"episode_reward": 12.04032411183365, "episode": 7.0, "batch_reward": 0.04642298847436905, "critic_loss": 0.005315760831348598, "ae_transition_loss": 0.002497444565873593, "ae_encoder_loss": 0.0014993729428388178, "actor_loss": -0.7811027631759644, "actor_target_entropy": -6.0, "actor_entropy": 5.3062332839965825, "alpha_loss": 0.01573898770380765, "alpha_value": 0.0038573421872536064, "duration": 53.829527616500854, "step": 1750}
{"episode_reward": 22.068953788372973, "episode": 8.0, "batch_reward": 0.05024208714067936, "critic_loss": 0.004282061777077615, "ae_transition_loss": 0.002435617635026574, "ae_encoder_loss": 0.001704606840852648, "actor_loss": -0.8943729667663575, "actor_target_entropy": -6.0, "actor_entropy": 5.2494651832580566, "alpha_loss": 0.007881730929017066, "alpha_value": 0.0037482030731585177, "duration": 53.81527233123779, "step": 2000}
{"episode_reward": 15.806792043694141, "episode": 9.0, "batch_reward": 0.050843014866113666, "critic_loss": 0.0042660716511309146, "ae_transition_loss": 0.0021354581615887582, "ae_encoder_loss": 0.0015232476647943259, "actor_loss": -1.0094018435478211, "actor_target_entropy": -6.0, "actor_entropy": 4.842020378112793, "alpha_loss": 0.0075146042313426736, "alpha_value": 0.0036704624792506307, "duration": 53.840312242507935, "step": 2250}
{"episode_reward": 11.420743236345025, "episode": 10.0, "batch_reward": 0.050217603623867034, "critic_loss": 0.0032297697518952192, "ae_transition_loss": 0.0018731812485493719, "ae_encoder_loss": 0.00128347506118007, "actor_loss": -1.0556409549713135, "actor_target_entropy": -6.0, "actor_entropy": 4.304628566741943, "alpha_loss": 0.005412340992596, "alpha_value": 0.003592569059083984, "duration": 53.82484412193298, "step": 2500}
{"episode_reward": 11.200299179351147, "episode": 11.0, "batch_reward": 0.05100846065580845, "critic_loss": 0.005056621977128088, "ae_transition_loss": 0.0022320629567839207, "ae_encoder_loss": 0.0013022224579472095, "actor_loss": -1.103913365840912, "actor_target_entropy": -6.0, "actor_entropy": 4.231449363708496, "alpha_loss": 0.0045463492872659115, "alpha_value": 0.0035354430538187915, "duration": 68.94795274734497, "step": 2750}
{"episode_reward": 24.88793040240161, "episode": 12.0, "batch_reward": 0.05494474473595619, "critic_loss": 0.005235048071481288, "ae_transition_loss": 0.002198641540016979, "ae_encoder_loss": 0.0013946632081642746, "actor_loss": -1.1722344360351562, "actor_target_entropy": -6.0, "actor_entropy": 4.042086402893067, "alpha_loss": 0.003147099310765043, "alpha_value": 0.003486100097001017, "duration": 53.86656188964844, "step": 3000}
