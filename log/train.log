{"episode_reward": 0.0, "episode": 1.0, "duration": 17.114288091659546, "step": 250}
{"episode_reward": 8.03772363506006, "episode": 2.0, "duration": 0.46495604515075684, "step": 500}
{"episode_reward": 17.16626315104322, "episode": 3.0, "duration": 0.45587754249572754, "step": 750}
{"episode_reward": 12.073072643141685, "episode": 4.0, "duration": 0.441417932510376, "step": 1000}
{"episode_reward": 11.095187414981499, "episode": 5.0, "batch_reward": 0.048569690892465406, "critic_loss": 0.003643242527988573, "ae_transition_loss": 0.0084019762798853, "ae_encoder_loss": 0.000985755131671185, "actor_loss": -0.3550576492961748, "actor_target_entropy": -6.0, "actor_entropy": 7.036233525047098, "alpha_loss": 0.060343892886916516, "alpha_value": 0.00636734692341825, "duration": 285.8916931152344, "step": 1250}
{"episode_reward": 16.981599236259704, "episode": 6.0, "batch_reward": 0.05473637044429779, "critic_loss": 0.0025917317075654866, "ae_transition_loss": 0.0020452769151888786, "ae_encoder_loss": 0.0014111673233564942, "actor_loss": -0.5938002748489379, "actor_target_entropy": -6.0, "actor_entropy": 5.972127555847168, "alpha_loss": 0.025971092462539674, "alpha_value": 0.003976018340690105, "duration": 54.886478424072266, "step": 1500}
{"episode_reward": 22.464798980696006, "episode": 7.0, "batch_reward": 0.05920331735908985, "critic_loss": 0.0027842961070127783, "ae_transition_loss": 0.0020235367487184703, "ae_encoder_loss": 0.0017003267388790845, "actor_loss": -0.6838483624458312, "actor_target_entropy": -6.0, "actor_entropy": 5.812721630096435, "alpha_loss": 0.023281317874789237, "alpha_value": 0.0037541030793623188, "duration": 54.63913345336914, "step": 1750}
{"episode_reward": 17.25660052394233, "episode": 8.0, "batch_reward": 0.06638473021984101, "critic_loss": 0.0037850772934034465, "ae_transition_loss": 0.0022729137958958745, "ae_encoder_loss": 0.0021171511467546226, "actor_loss": -0.7759446253776551, "actor_target_entropy": -6.0, "actor_entropy": 5.70201679611206, "alpha_loss": 0.018719518214464187, "alpha_value": 0.0035575282678903084, "duration": 54.99582505226135, "step": 2000}
{"episode_reward": 32.22145822788492, "episode": 9.0, "batch_reward": 0.06946677497029305, "critic_loss": 0.004282772125676274, "ae_transition_loss": 0.0023055115761235357, "ae_encoder_loss": 0.0021215692874975503, "actor_loss": -0.8352314448356628, "actor_target_entropy": -6.0, "actor_entropy": 5.784283771514892, "alpha_loss": 0.015823686592280863, "alpha_value": 0.0033973194879034544, "duration": 54.73978400230408, "step": 2250}
{"episode_reward": 22.40394311880627, "episode": 10.0, "batch_reward": 0.07310483622550965, "critic_loss": 0.004761436020024121, "ae_transition_loss": 0.002412845310755074, "ae_encoder_loss": 0.0021591739864088594, "actor_loss": -0.9185480875968933, "actor_target_entropy": -6.0, "actor_entropy": 5.599926681518554, "alpha_loss": 0.011822010807693005, "alpha_value": 0.0032608197687790195, "duration": 54.89217233657837, "step": 2500}
{"episode_reward": 32.52468653498005, "episode": 11.0, "batch_reward": 0.0783103461265564, "critic_loss": 0.005459181648679078, "ae_transition_loss": 0.002495631248690188, "ae_encoder_loss": 0.002291756653226912, "actor_loss": -1.020787483692169, "actor_target_entropy": -6.0, "actor_entropy": 5.197719787597657, "alpha_loss": 0.004693989871651865, "alpha_value": 0.0031783889455946268, "duration": 71.64351844787598, "step": 2750}
{"episode_reward": 26.847234005163937, "episode": 12.0, "batch_reward": 0.07968756726384163, "critic_loss": 0.0062690793229267005, "ae_transition_loss": 0.0025186818689107893, "ae_encoder_loss": 0.0022588806203566493, "actor_loss": -1.1292014408111573, "actor_target_entropy": -6.0, "actor_entropy": 5.063528697967529, "alpha_loss": 0.0032228234440553932, "alpha_value": 0.0031333228183356735, "duration": 54.93298649787903, "step": 3000}
