{"episode_reward": 0.0, "episode": 1.0, "duration": 17.6933753490448, "step": 250}
{"episode_reward": 8.03772363506006, "episode": 2.0, "duration": 0.43467211723327637, "step": 500}
{"episode_reward": 17.16626315104322, "episode": 3.0, "duration": 0.4300041198730469, "step": 750}
{"episode_reward": 12.073072643141685, "episode": 4.0, "duration": 0.42908191680908203, "step": 1000}
{"episode_reward": 11.095187414981499, "episode": 5.0, "batch_reward": 0.048668531272963014, "critic_loss": 0.00339506970193935, "ae_transition_loss": 0.0083106644571855, "ae_encoder_loss": 0.0009704766684775505, "actor_loss": -0.3514237428333418, "actor_target_entropy": -6.0, "actor_entropy": 6.970225041447161, "alpha_loss": 0.06082126914831773, "alpha_value": 0.006367132506572593, "duration": 281.67758774757385, "step": 1250}
{"episode_reward": 19.10369562397677, "episode": 6.0, "batch_reward": 0.05473454606533051, "critic_loss": 0.0018443032056093215, "ae_transition_loss": 0.0022271178932860495, "ae_encoder_loss": 0.0014617252913303673, "actor_loss": -0.5560122563838958, "actor_target_entropy": -6.0, "actor_entropy": 6.563047546386719, "alpha_loss": 0.033781991943717005, "alpha_value": 0.003907952201591749, "duration": 53.95250844955444, "step": 1500}
{"episode_reward": 16.115282146948893, "episode": 7.0, "batch_reward": 0.05706570532917976, "critic_loss": 0.0024058760688640177, "ae_transition_loss": 0.002016298876143992, "ae_encoder_loss": 0.0015262796096503735, "actor_loss": -0.6254210586547851, "actor_target_entropy": -6.0, "actor_entropy": 6.308112861633301, "alpha_loss": 0.026709057539701462, "alpha_value": 0.003637698313412732, "duration": 53.93154954910278, "step": 1750}
{"episode_reward": 20.50436737041326, "episode": 8.0, "batch_reward": 0.05955597177147865, "critic_loss": 0.0030184812201187014, "ae_transition_loss": 0.002226275287102908, "ae_encoder_loss": 0.0015443939282558858, "actor_loss": -0.7027896375656127, "actor_target_entropy": -6.0, "actor_entropy": 5.931546306610107, "alpha_loss": 0.01786727784574032, "alpha_value": 0.0034431812966397904, "duration": 53.9522340297699, "step": 2000}
{"episode_reward": 16.21000157194325, "episode": 9.0, "batch_reward": 0.05958127242326736, "critic_loss": 0.00275807626824826, "ae_transition_loss": 0.0020285138017497955, "ae_encoder_loss": 0.001400527779245749, "actor_loss": -0.754546630859375, "actor_target_entropy": -6.0, "actor_entropy": 5.801787086486817, "alpha_loss": 0.014175284445285798, "alpha_value": 0.0033040749350283297, "duration": 53.991615772247314, "step": 2250}
{"episode_reward": 15.928794199874527, "episode": 10.0, "batch_reward": 0.062000448346138, "critic_loss": 0.0037473684614524245, "ae_transition_loss": 0.0020166730051860214, "ae_encoder_loss": 0.0014672734942287207, "actor_loss": -0.8254907536506653, "actor_target_entropy": -6.0, "actor_entropy": 5.576295024871826, "alpha_loss": 0.010439446996897458, "alpha_value": 0.0031875705283188898, "duration": 53.94672870635986, "step": 2500}
{"episode_reward": 26.586194591741133, "episode": 11.0, "batch_reward": 0.06565631100535393, "critic_loss": 0.004008006076328456, "ae_transition_loss": 0.002088563963305205, "ae_encoder_loss": 0.0017814926565624773, "actor_loss": -0.9086420331001281, "actor_target_entropy": -6.0, "actor_entropy": 5.258204444885254, "alpha_loss": 0.006855506392195821, "alpha_value": 0.0031065237073751263, "duration": 69.58717560768127, "step": 2750}
{"episode_reward": 19.868732306610408, "episode": 12.0, "batch_reward": 0.06863302344083785, "critic_loss": 0.004901211169548333, "ae_transition_loss": 0.0021932754456065596, "ae_encoder_loss": 0.0020406245472840965, "actor_loss": -0.9877810997962951, "actor_target_entropy": -6.0, "actor_entropy": 5.071319881439209, "alpha_loss": 0.005764999713748694, "alpha_value": 0.00304126174532263, "duration": 53.97742199897766, "step": 3000}
{"episode_reward": 32.73365803232738, "episode": 13.0, "batch_reward": 0.07339676220715045, "critic_loss": 0.006356109427288175, "ae_transition_loss": 0.0026874511148780584, "ae_encoder_loss": 0.002728199677076191, "actor_loss": -1.0600618171691893, "actor_target_entropy": -6.0, "actor_entropy": 4.909913623809815, "alpha_loss": 0.0033337317956611515, "alpha_value": 0.00299127341935497, "duration": 53.98803758621216, "step": 3250}
{"episode_reward": 24.41693943280807, "episode": 14.0, "batch_reward": 0.07816048255562782, "critic_loss": 0.009503027250990271, "ae_transition_loss": 0.003345879878848791, "ae_encoder_loss": 0.00360828207898885, "actor_loss": -1.1614139709472657, "actor_target_entropy": -6.0, "actor_entropy": 4.8489899215698244, "alpha_loss": 0.0027506511309184135, "alpha_value": 0.0029593624662381476, "duration": 53.99272894859314, "step": 3500}
{"episode_reward": 39.43330270154811, "episode": 15.0, "batch_reward": 0.0838935514986515, "critic_loss": 0.010737333534285427, "ae_transition_loss": 0.0038174578519538046, "ae_encoder_loss": 0.004258850613608956, "actor_loss": -1.2651159458160401, "actor_target_entropy": -6.0, "actor_entropy": 4.7119819374084475, "alpha_loss": 0.001029974298493471, "alpha_value": 0.002930898882919073, "duration": 53.954997062683105, "step": 3750}
