{"episode_reward": 0.0, "episode": 1.0, "duration": 19.869313716888428, "step": 250}
{"episode_reward": 7.747346039045906, "episode": 2.0, "duration": 0.42858290672302246, "step": 500}
{"episode_reward": 4.427573337125385, "episode": 3.0, "duration": 0.43163061141967773, "step": 750}
{"episode_reward": 5.050386443524035, "episode": 4.0, "duration": 0.42612600326538086, "step": 1000}
{"episode_reward": 17.85499204884596, "episode": 5.0, "batch_reward": 0.03606612990506655, "critic_loss": 0.005874368866843681, "ae_transition_loss": 0.013212445840059449, "ae_encoder_loss": 0.0011736746974646585, "actor_loss": -0.29446610580264754, "actor_target_entropy": -6.0, "actor_entropy": 6.698695475944845, "alpha_loss": 0.057611878247089764, "alpha_value": 0.0064452657777700146, "duration": 282.06839513778687, "step": 1250}
{"episode_reward": 19.009889254504696, "episode": 6.0, "batch_reward": 0.04330978548526764, "critic_loss": 0.002963487028609961, "ae_transition_loss": 0.0031122515499591827, "ae_encoder_loss": 0.0015322093737777322, "actor_loss": -0.5438100454807282, "actor_target_entropy": -6.0, "actor_entropy": 5.725172145843506, "alpha_loss": 0.02514484116435051, "alpha_value": 0.004083270908194118, "duration": 54.10831093788147, "step": 1500}
{"episode_reward": 11.02025277193485, "episode": 7.0, "batch_reward": 0.04435640046000481, "critic_loss": 0.0036231798315420746, "ae_transition_loss": 0.0029453759863972665, "ae_encoder_loss": 0.0013894048759248107, "actor_loss": -0.5980429220199585, "actor_target_entropy": -6.0, "actor_entropy": 5.256697257995605, "alpha_loss": 0.02174253434687853, "alpha_value": 0.0038531124454628946, "duration": 54.04162383079529, "step": 1750}
{"episode_reward": 19.54050467598132, "episode": 8.0, "batch_reward": 0.04793741641938686, "critic_loss": 0.004043620177544654, "ae_transition_loss": 0.00270406717248261, "ae_encoder_loss": 0.001548724085558206, "actor_loss": -0.6540412192344666, "actor_target_entropy": -6.0, "actor_entropy": 5.084911796569824, "alpha_loss": 0.018358643360435963, "alpha_value": 0.0036537279520176442, "duration": 54.09377861022949, "step": 2000}
{"episode_reward": 10.606742424786908, "episode": 9.0, "batch_reward": 0.048895308434963224, "critic_loss": 0.004963056610897183, "ae_transition_loss": 0.002458029892295599, "ae_encoder_loss": 0.0014503136437851935, "actor_loss": -0.7456464500427246, "actor_target_entropy": -6.0, "actor_entropy": 4.850940670013427, "alpha_loss": 0.013835349470376969, "alpha_value": 0.003495816657813126, "duration": 54.04279565811157, "step": 2250}
{"episode_reward": 16.309218004592918, "episode": 10.0, "batch_reward": 0.051926154136657715, "critic_loss": 0.0038996527744457124, "ae_transition_loss": 0.0020550197083503007, "ae_encoder_loss": 0.0011796433690469711, "actor_loss": -0.813804783821106, "actor_target_entropy": -6.0, "actor_entropy": 4.237229642868042, "alpha_loss": 0.011181919179856777, "alpha_value": 0.003361158502619966, "duration": 54.043766260147095, "step": 2500}
