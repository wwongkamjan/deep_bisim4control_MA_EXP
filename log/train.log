{"episode_reward": 0.0, "episode": 1.0, "duration": 16.62580418586731, "step": 250}
{"episode_reward": 8.03772363506006, "episode": 2.0, "duration": 0.4040684700012207, "step": 500}
{"episode_reward": 17.16626315104322, "episode": 3.0, "duration": 0.40572690963745117, "step": 750}
{"episode_reward": 12.073072643141685, "episode": 4.0, "duration": 0.4026308059692383, "step": 1000}
{"episode_reward": 11.095187414981499, "episode": 5.0, "batch_reward": 0.0487569803543526, "critic_loss": 0.007732101094504867, "ae_transition_loss": 0.013818764768354818, "ae_encoder_loss": 0.0007700258401666005, "actor_loss": -0.22067834972594258, "actor_target_entropy": -6.0, "actor_entropy": 6.778363098453372, "alpha_loss": 0.05634006566357272, "alpha_value": 0.006406707864146721, "duration": 285.55745100975037, "step": 1250}
{"episode_reward": 17.223328911773525, "episode": 6.0, "batch_reward": 0.052734674721956254, "critic_loss": 0.0037636954011395574, "ae_transition_loss": 0.003398262219503522, "ae_encoder_loss": 0.0008734663100913167, "actor_loss": -0.45181990909576414, "actor_target_entropy": -6.0, "actor_entropy": 5.289503601074219, "alpha_loss": 0.02294260795414448, "alpha_value": 0.004033674941998986, "duration": 54.713637351989746, "step": 1500}
{"episode_reward": 13.698252050415942, "episode": 7.0, "batch_reward": 0.05070366206765175, "critic_loss": 0.003605626720003784, "ae_transition_loss": 0.002645005506463349, "ae_encoder_loss": 0.0007586585907265544, "actor_loss": -0.5237861638069152, "actor_target_entropy": -6.0, "actor_entropy": 4.5991550731658934, "alpha_loss": 0.018544227823615075, "alpha_value": 0.0038256270942585505, "duration": 54.74457144737244, "step": 1750}
{"episode_reward": 6.4589698069355626, "episode": 8.0, "batch_reward": 0.04840418241918087, "critic_loss": 0.003798206441570073, "ae_transition_loss": 0.002453732684254646, "ae_encoder_loss": 0.0007209131657145918, "actor_loss": -0.5952670307159423, "actor_target_entropy": -6.0, "actor_entropy": 4.06558172416687, "alpha_loss": 0.012582374036312103, "alpha_value": 0.0036673917963160655, "duration": 54.71441030502319, "step": 2000}
{"episode_reward": 13.139166697385848, "episode": 9.0, "batch_reward": 0.04912722447514534, "critic_loss": 0.005350232839584351, "ae_transition_loss": 0.0022971071540378034, "ae_encoder_loss": 0.0006401996776694432, "actor_loss": -0.6152519636154175, "actor_target_entropy": -6.0, "actor_entropy": 3.849491636276245, "alpha_loss": 0.01012756635248661, "alpha_value": 0.003552068643402481, "duration": 54.72272777557373, "step": 2250}
{"episode_reward": 10.591676989591143, "episode": 10.0, "batch_reward": 0.05033775977790356, "critic_loss": 0.004557149092666805, "ae_transition_loss": 0.001976793744601309, "ae_encoder_loss": 0.0006493732861708849, "actor_loss": -0.6642381238937378, "actor_target_entropy": -6.0, "actor_entropy": 3.4194077548980712, "alpha_loss": 0.007878344371914864, "alpha_value": 0.0034476585695180357, "duration": 54.74415040016174, "step": 2500}
{"episode_reward": 25.04711952609052, "episode": 11.0, "batch_reward": 0.05609618709981441, "critic_loss": 0.005214169418439269, "ae_transition_loss": 0.002101998209953308, "ae_encoder_loss": 0.000861996706109494, "actor_loss": -0.7419857792854309, "actor_target_entropy": -6.0, "actor_entropy": 3.336465200424194, "alpha_loss": 0.0040995208392851056, "alpha_value": 0.003376867054620965, "duration": 69.9504725933075, "step": 2750}
{"episode_reward": 22.612493980954792, "episode": 12.0, "batch_reward": 0.057669845536351204, "critic_loss": 0.004511920120567083, "ae_transition_loss": 0.0019592141271568835, "ae_encoder_loss": 0.0008987174418289215, "actor_loss": -0.814869104385376, "actor_target_entropy": -6.0, "actor_entropy": 3.214708993911743, "alpha_loss": 0.002787521162885241, "alpha_value": 0.0033356261344457266, "duration": 54.69140911102295, "step": 3000}
