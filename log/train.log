{"episode_reward": 0.0, "episode": 1.0, "duration": 197.24783158302307, "step": 1}
{"episode_reward": 0.0, "episode": 2.0, "duration": 0.474806547164917, "step": 251}
{"episode_reward": 0.0, "episode": 3.0, "duration": 0.4708590507507324, "step": 501}
{"episode_reward": 0.0, "episode": 4.0, "duration": 0.46912717819213867, "step": 751}
{"episode_reward": 0.0, "episode": 5.0, "batch_reward": -0.00013671875, "critic_loss": 0.03361030912818387, "ae_transition_loss": 0.007656333148182966, "ae_encoder_loss": 0.008232862691045739, "actor_loss": -0.16124075314863875, "actor_target_entropy": -6.0, "actor_entropy": 6.282394002646208, "alpha_loss": 0.0510009636587929, "alpha_value": 0.006720194651003696, "duration": 62.66265821456909, "step": 1001}
{"episode_reward": 0.0, "episode": 6.0, "batch_reward": -0.0001796875, "critic_loss": 0.03242956680804491, "ae_transition_loss": 3.746996806876268e-05, "ae_encoder_loss": 0.008152786012738942, "actor_loss": -0.2592227393388748, "actor_target_entropy": -6.0, "actor_entropy": 5.374808734893799, "alpha_loss": 0.0271225555986166, "alpha_value": 0.004429401367867955, "duration": 15.287856817245483, "step": 1251}
{"episode_reward": -3.0, "episode": 7.0, "batch_reward": -0.0023125, "critic_loss": 0.03425945156440139, "ae_transition_loss": 3.19920260735671e-05, "ae_encoder_loss": 0.00855707007041201, "actor_loss": -0.28482267355918883, "actor_target_entropy": -6.0, "actor_entropy": 4.884530506134033, "alpha_loss": 0.025138948813080788, "alpha_value": 0.004125921512638009, "duration": 15.283657312393188, "step": 1501}
{"episode_reward": -3.0, "episode": 8.0, "batch_reward": -0.0047421875, "critic_loss": 0.03587177953869104, "ae_transition_loss": 4.278596585572814e-05, "ae_encoder_loss": 0.009019480176270009, "actor_loss": -0.3058043074607849, "actor_target_entropy": -6.0, "actor_entropy": 4.751881202697754, "alpha_loss": 0.022096139535307886, "alpha_value": 0.0038461127291138917, "duration": 15.297075986862183, "step": 1751}
{"episode_reward": -3.0, "episode": 9.0, "batch_reward": -0.005390625, "critic_loss": 0.03516429668630008, "ae_transition_loss": 0.002402659143830533, "ae_encoder_loss": 0.008739920549251504, "actor_loss": -0.3269708913564682, "actor_target_entropy": -6.0, "actor_entropy": 5.203873281478882, "alpha_loss": 0.019942063502967358, "alpha_value": 0.003611893933853098, "duration": 15.306979656219482, "step": 2001}
{"episode_reward": -3.0, "episode": 10.0, "batch_reward": -0.0066640625, "critic_loss": 0.037135511845350265, "ae_transition_loss": 3.4909205125586595e-05, "ae_encoder_loss": 0.009371421675197781, "actor_loss": -0.34680557012557983, "actor_target_entropy": -6.0, "actor_entropy": 5.048046337127685, "alpha_loss": 0.019085655242204666, "alpha_value": 0.0033862943731127167, "duration": 15.289602279663086, "step": 2251}
{"episode_reward": -3.0, "episode": 11.0, "batch_reward": -0.0066171875, "critic_loss": 0.036797069624066356, "ae_transition_loss": 3.3501627141959034e-05, "ae_encoder_loss": 0.009310835097916425, "actor_loss": -0.3647104625701904, "actor_target_entropy": -6.0, "actor_entropy": 4.689226764678955, "alpha_loss": 0.01819687482714653, "alpha_value": 0.0031770670846634594, "duration": 15.291926622390747, "step": 2501}
{"episode_reward": -3.0, "episode": 12.0, "batch_reward": -0.0065546875, "critic_loss": 0.03579900795966387, "ae_transition_loss": 2.8965655706997496e-05, "ae_encoder_loss": 0.009031489571556448, "actor_loss": -0.3781021866798401, "actor_target_entropy": -6.0, "actor_entropy": 4.752023170471191, "alpha_loss": 0.016937441095709802, "alpha_value": 0.0029770059544959233, "duration": 15.291895151138306, "step": 2751}
{"episode_reward": -3.0, "episode": 13.0, "batch_reward": -0.007875, "critic_loss": 0.03874821320176124, "ae_transition_loss": 1.726920817964128e-05, "ae_encoder_loss": 0.00974824132118374, "actor_loss": -0.38763150477409364, "actor_target_entropy": -6.0, "actor_entropy": 5.067935089111328, "alpha_loss": 0.017008266523480416, "alpha_value": 0.0027885292974471404, "duration": 15.289963006973267, "step": 3001}
{"episode_reward": -3.0, "episode": 14.0, "batch_reward": -0.007984375, "critic_loss": 0.036787383392453195, "ae_transition_loss": 1.8709536258029403e-05, "ae_encoder_loss": 0.009312609947286546, "actor_loss": -0.3940179038047791, "actor_target_entropy": -6.0, "actor_entropy": 4.9125398254394534, "alpha_loss": 0.01446407937258482, "alpha_value": 0.0026105960834217554, "duration": 15.271825313568115, "step": 3251}
{"episode_reward": -3.0, "episode": 15.0, "batch_reward": -0.0084375, "critic_loss": 0.03787839425355196, "ae_transition_loss": 2.5765016263903816e-05, "ae_encoder_loss": 0.009587996458634734, "actor_loss": -0.4032429242134094, "actor_target_entropy": -6.0, "actor_entropy": 4.655808641433715, "alpha_loss": 0.012287564225494861, "alpha_value": 0.00246639265706881, "duration": 15.280599355697632, "step": 3501}
{"episode_reward": -3.0, "episode": 16.0, "batch_reward": -0.008046875, "critic_loss": 0.037089260645210745, "ae_transition_loss": 4.96063040336594e-05, "ae_encoder_loss": 0.009390723435208201, "actor_loss": -0.40586062502861026, "actor_target_entropy": -6.0, "actor_entropy": 4.90402725982666, "alpha_loss": 0.012256398722529411, "alpha_value": 0.0023312908884175164, "duration": 15.285011291503906, "step": 3751}
{"episode_reward": -3.0, "episode": 17.0, "batch_reward": -0.00921875, "critic_loss": 0.03743692537024617, "ae_transition_loss": 3.554628458368825e-05, "ae_encoder_loss": 0.009529386610724031, "actor_loss": -0.40744368147850035, "actor_target_entropy": -6.0, "actor_entropy": 4.709464458465576, "alpha_loss": 0.011272320620715618, "alpha_value": 0.0022040966320037826, "duration": 15.272019863128662, "step": 4001}
{"episode_reward": -3.0, "episode": 18.0, "batch_reward": -0.0079609375, "critic_loss": 0.03696194814145565, "ae_transition_loss": 8.581969729857519e-05, "ae_encoder_loss": 0.009296964585781097, "actor_loss": -0.41108766341209413, "actor_target_entropy": -6.0, "actor_entropy": 4.7528698539733885, "alpha_loss": 0.009838533490896225, "alpha_value": 0.002085472501249122, "duration": 15.30324387550354, "step": 4251}
{"episode_reward": -3.0, "episode": 19.0, "batch_reward": -0.009046875, "critic_loss": 0.036804345715790984, "ae_transition_loss": 9.293511130090337e-05, "ae_encoder_loss": 0.009373123155906797, "actor_loss": -0.41739497661590574, "actor_target_entropy": -6.0, "actor_entropy": 4.975504264831543, "alpha_loss": 0.007999940279871226, "alpha_value": 0.0019874075166516404, "duration": 15.31469464302063, "step": 4501}
{"episode_reward": -3.0, "episode": 20.0, "batch_reward": -0.00903125, "critic_loss": 0.03821261569112539, "ae_transition_loss": 9.661595249781385e-05, "ae_encoder_loss": 0.009677811565808951, "actor_loss": -0.4233774616718292, "actor_target_entropy": -6.0, "actor_entropy": 4.62084356880188, "alpha_loss": 0.0071924513094127175, "alpha_value": 0.0019041709745623834, "duration": 15.296670198440552, "step": 4751}
{"episode_reward": -3.0, "episode": 21.0, "batch_reward": -0.009890625, "critic_loss": 0.03812743063271046, "ae_transition_loss": 0.00011554842893383466, "ae_encoder_loss": 0.009630939717404544, "actor_loss": -0.42734942841529844, "actor_target_entropy": -6.0, "actor_entropy": 4.727101631164551, "alpha_loss": 0.0071623634919524195, "alpha_value": 0.001823045476805788, "duration": 211.56880497932434, "step": 5001}
{"episode_reward": -3.0, "episode": 22.0, "batch_reward": -0.0100390625, "critic_loss": 0.03833722010999918, "ae_transition_loss": 0.00012860063403786624, "ae_encoder_loss": 0.009632894190959633, "actor_loss": -0.4287215402126312, "actor_target_entropy": -6.0, "actor_entropy": 4.921207042694092, "alpha_loss": 0.0072648411840200424, "alpha_value": 0.0017421656219936182, "duration": 15.269009351730347, "step": 5251}
{"episode_reward": -3.0, "episode": 23.0, "batch_reward": -0.009828125, "critic_loss": 0.037504111785441635, "ae_transition_loss": 7.400326055358164e-05, "ae_encoder_loss": 0.009509819814935327, "actor_loss": -0.432118723154068, "actor_target_entropy": -6.0, "actor_entropy": 4.853131366729737, "alpha_loss": 0.007505043424665928, "alpha_value": 0.0016557092070995142, "duration": 15.288968801498413, "step": 5501}
{"episode_reward": -3.0, "episode": 24.0, "batch_reward": -0.009828125, "critic_loss": 0.039281845077872274, "ae_transition_loss": 0.00021110720519209282, "ae_encoder_loss": 0.009939570867456496, "actor_loss": -0.42920228695869445, "actor_target_entropy": -6.0, "actor_entropy": 5.26675651550293, "alpha_loss": 0.007409415744245053, "alpha_value": 0.0015699688802812892, "duration": 15.275851488113403, "step": 5751}
{"episode_reward": -3.0, "episode": 25.0, "batch_reward": -0.0097578125, "critic_loss": 0.03709458784386516, "ae_transition_loss": 0.0001511675511719659, "ae_encoder_loss": 0.009390701600350439, "actor_loss": -0.43090402841567993, "actor_target_entropy": -6.0, "actor_entropy": 5.258260757446289, "alpha_loss": 0.006749930016696453, "alpha_value": 0.0014879238176901744, "duration": 15.267651319503784, "step": 6001}
{"episode_reward": -3.0, "episode": 26.0, "batch_reward": -0.0099140625, "critic_loss": 0.03828579580783844, "ae_transition_loss": 0.00010505738689971622, "ae_encoder_loss": 0.009675767128355802, "actor_loss": -0.4286290922164917, "actor_target_entropy": -6.0, "actor_entropy": 5.178598445892334, "alpha_loss": 0.006568391069769859, "alpha_value": 0.0014115007738406762, "duration": 15.316447734832764, "step": 6251}
{"episode_reward": -3.0, "episode": 27.0, "batch_reward": -0.0094453125, "critic_loss": 0.03898187108337879, "ae_transition_loss": 0.0002397583999991184, "ae_encoder_loss": 0.009816971720196306, "actor_loss": -0.4308414349555969, "actor_target_entropy": -6.0, "actor_entropy": 5.13307693862915, "alpha_loss": 0.006322749212384224, "alpha_value": 0.0013376137900233548, "duration": 15.287148714065552, "step": 6501}
{"episode_reward": -3.0, "episode": 28.0, "batch_reward": -0.01003125, "critic_loss": 0.03801516800373793, "ae_transition_loss": 0.0018544604581256863, "ae_encoder_loss": 0.009607407514937221, "actor_loss": -0.4280592918395996, "actor_target_entropy": -6.0, "actor_entropy": 5.358662712097168, "alpha_loss": 0.005088062226772308, "alpha_value": 0.0012671989589681171, "duration": 15.292890071868896, "step": 6751}
{"episode_reward": -3.0, "episode": 29.0, "batch_reward": -0.01, "critic_loss": 0.03711620048433542, "ae_transition_loss": 0.0007933382097398863, "ae_encoder_loss": 0.009393298758193851, "actor_loss": -0.4253518466949463, "actor_target_entropy": -6.0, "actor_entropy": 5.292761486053466, "alpha_loss": 0.004314047554507852, "alpha_value": 0.0012160689796570391, "duration": 15.273054361343384, "step": 7001}
{"episode_reward": -3.0, "episode": 30.0, "batch_reward": -0.0108671875, "critic_loss": 0.03851601856201887, "ae_transition_loss": 0.0013401500042527915, "ae_encoder_loss": 0.009771653286181391, "actor_loss": -0.42154048442840575, "actor_target_entropy": -6.0, "actor_entropy": 5.333922721862793, "alpha_loss": 0.003834208300337195, "alpha_value": 0.0011681011873670684, "duration": 15.264086723327637, "step": 7251}
{"episode_reward": -3.0, "episode": 31.0, "batch_reward": -0.010421875, "critic_loss": 0.03966895593702793, "ae_transition_loss": 0.0023884425800060854, "ae_encoder_loss": 0.010030994835309684, "actor_loss": -0.4229361357688904, "actor_target_entropy": -6.0, "actor_entropy": 5.644026756286621, "alpha_loss": 0.0035592463994398712, "alpha_value": 0.001124257920361247, "duration": 15.269692420959473, "step": 7501}
{"episode_reward": -3.0, "episode": 32.0, "batch_reward": -0.0094140625, "critic_loss": 0.03815549731627107, "ae_transition_loss": 0.004489883796311915, "ae_encoder_loss": 0.009733528198674322, "actor_loss": -0.4206892352104187, "actor_target_entropy": -6.0, "actor_entropy": 5.599207592010498, "alpha_loss": 0.003532737439498305, "alpha_value": 0.001081954724239644, "duration": 15.304911851882935, "step": 7751}
{"episode_reward": -3.0, "episode": 33.0, "batch_reward": -0.010015625, "critic_loss": 0.03653733046725392, "ae_transition_loss": 0.006892289081122726, "ae_encoder_loss": 0.01022067922540009, "actor_loss": -0.39733682775497436, "actor_target_entropy": -6.0, "actor_entropy": 6.037842891693115, "alpha_loss": 0.0054823926314711575, "alpha_value": 0.001028151574686422, "duration": 15.27395224571228, "step": 8001}
{"episode_reward": -3.0, "episode": 34.0, "batch_reward": -0.010625, "critic_loss": 0.03689062479883432, "ae_transition_loss": 0.0036681009600870312, "ae_encoder_loss": 0.01016009021922946, "actor_loss": -0.36248881673812866, "actor_target_entropy": -6.0, "actor_entropy": 6.2037547416687016, "alpha_loss": 0.006043536204844714, "alpha_value": 0.0009558195609698475, "duration": 15.27636981010437, "step": 8251}
{"episode_reward": -3.0, "episode": 35.0, "batch_reward": -0.0107734375, "critic_loss": 0.03538463916629553, "ae_transition_loss": 0.0032253394972067326, "ae_encoder_loss": 0.010005499672144652, "actor_loss": -0.3059841480255127, "actor_target_entropy": -6.0, "actor_entropy": 5.889003810882568, "alpha_loss": 0.005245867419987917, "alpha_value": 0.0008896144788848524, "duration": 15.284836292266846, "step": 8501}
{"episode_reward": -3.0, "episode": 36.0, "batch_reward": -0.00996875, "critic_loss": 0.03546374748274684, "ae_transition_loss": 0.002970404438790865, "ae_encoder_loss": 0.01011800063494593, "actor_loss": -0.2818440990447998, "actor_target_entropy": -6.0, "actor_entropy": 5.349653028488159, "alpha_loss": 0.0046165856439620255, "alpha_value": 0.0008329421126349791, "duration": 15.298893928527832, "step": 8751}
{"episode_reward": -3.0, "episode": 37.0, "batch_reward": -0.00996875, "critic_loss": 0.03374710286781192, "ae_transition_loss": 0.00469504476687871, "ae_encoder_loss": 0.010188671190291643, "actor_loss": -0.27513283920288084, "actor_target_entropy": -6.0, "actor_entropy": 5.540748226165771, "alpha_loss": 0.0046460002642124895, "alpha_value": 0.0007800069836592535, "duration": 15.304823637008667, "step": 9001}
{"episode_reward": -3.0, "episode": 38.0, "batch_reward": -0.0107578125, "critic_loss": 0.03489008516073227, "ae_transition_loss": 0.002988080505980179, "ae_encoder_loss": 0.009957993348129093, "actor_loss": -0.28018663597106935, "actor_target_entropy": -6.0, "actor_entropy": 5.466186771392822, "alpha_loss": 0.0036240633632987737, "alpha_value": 0.0007335030192530132, "duration": 15.278013229370117, "step": 9251}
{"episode_reward": -1.0, "episode": 39.0, "batch_reward": -0.011, "critic_loss": 0.03557450381293893, "ae_transition_loss": 0.003224461660254747, "ae_encoder_loss": 0.009818580355495215, "actor_loss": -0.27992576313018797, "actor_target_entropy": -6.0, "actor_entropy": 5.4982756271362305, "alpha_loss": 0.0029357299208641052, "alpha_value": 0.0006974485580965638, "duration": 15.281532764434814, "step": 9501}
{"episode_reward": -3.0, "episode": 40.0, "batch_reward": -0.0111171875, "critic_loss": 0.03639653842896223, "ae_transition_loss": 0.003838675333885476, "ae_encoder_loss": 0.01047211774252355, "actor_loss": -0.27349470841884616, "actor_target_entropy": -6.0, "actor_entropy": 5.504447822570801, "alpha_loss": 0.0029772609528154136, "alpha_value": 0.0006657495991799369, "duration": 15.296868801116943, "step": 9751}
{"episode_reward": -3.0, "episode": 41.0, "batch_reward": -0.01009375, "critic_loss": 0.03559081124141812, "ae_transition_loss": 0.003143142016953789, "ae_encoder_loss": 0.009979000585153698, "actor_loss": -0.270564971446991, "actor_target_entropy": -6.0, "actor_entropy": 5.566126735687256, "alpha_loss": 0.0031930653844028712, "alpha_value": 0.0006310019794667809, "duration": 213.7028148174286, "step": 10001}
{"episode_reward": -3.0, "episode": 42.0, "batch_reward": -0.010140625, "critic_loss": 0.037031050253659485, "ae_transition_loss": 0.0036321127956034614, "ae_encoder_loss": 0.01019701927807182, "actor_loss": -0.2645420912504196, "actor_target_entropy": -6.0, "actor_entropy": 5.1841497631073, "alpha_loss": 0.0032771636061370373, "alpha_value": 0.0005957557510070474, "duration": 15.369574308395386, "step": 10251}
{"episode_reward": -3.0, "episode": 43.0, "batch_reward": -0.010203125, "critic_loss": 0.03605551946349442, "ae_transition_loss": 0.0036804231874411927, "ae_encoder_loss": 0.010082198325544595, "actor_loss": -0.2618051073551178, "actor_target_entropy": -6.0, "actor_entropy": 5.558567665100098, "alpha_loss": 0.0030170838516205547, "alpha_value": 0.0005612909389598248, "duration": 15.289166927337646, "step": 10501}
{"episode_reward": -3.0, "episode": 44.0, "batch_reward": -0.0102265625, "critic_loss": 0.036441350422799584, "ae_transition_loss": 0.0044965800296049565, "ae_encoder_loss": 0.010492317063733936, "actor_loss": -0.2549035311937332, "actor_target_entropy": -6.0, "actor_entropy": 5.485091878890991, "alpha_loss": 0.0029600222557783127, "alpha_value": 0.0005294182896132145, "duration": 15.317626953125, "step": 10751}
{"episode_reward": -2.0, "episode": 45.0, "batch_reward": -0.0109296875, "critic_loss": 0.03833141437545419, "ae_transition_loss": 0.004565003338153474, "ae_encoder_loss": 0.010672813631594181, "actor_loss": -0.2527355737686157, "actor_target_entropy": -6.0, "actor_entropy": 5.454769050598144, "alpha_loss": 0.002499460596591234, "alpha_value": 0.0004993658158075615, "duration": 15.2875497341156, "step": 11001}
{"episode_reward": -3.0, "episode": 46.0, "batch_reward": -0.010515625, "critic_loss": 0.038867163948714735, "ae_transition_loss": 0.00698817806574516, "ae_encoder_loss": 0.011616013650782407, "actor_loss": -0.2464848573207855, "actor_target_entropy": -6.0, "actor_entropy": 5.628202265515924, "alpha_loss": 0.003057495567947626, "alpha_value": 0.0004705763311865417, "duration": 15.286713123321533, "step": 11251}
{"episode_reward": -3.0, "episode": 47.0, "batch_reward": -0.0107421875, "critic_loss": 0.03952446848154068, "ae_transition_loss": 0.0035594988176599145, "ae_encoder_loss": 0.010383247050456703, "actor_loss": -0.2453927844762802, "actor_target_entropy": -6.0, "actor_entropy": 6.767827110290527, "alpha_loss": 0.002941754976287484, "alpha_value": 0.00043850842767141905, "duration": 15.282444953918457, "step": 11501}
{"episode_reward": -3.0, "episode": 48.0, "batch_reward": -0.0108359375, "critic_loss": 0.039080369904637334, "ae_transition_loss": 0.0030861401214497164, "ae_encoder_loss": 0.010190054043196142, "actor_loss": -0.2411977550983429, "actor_target_entropy": -6.0, "actor_entropy": 6.687376594543457, "alpha_loss": 0.0024641480445861817, "alpha_value": 0.0004109879422948477, "duration": 15.285896301269531, "step": 11751}
{"episode_reward": -3.0, "episode": 49.0, "batch_reward": -0.0100234375, "critic_loss": 0.03814048129692674, "ae_transition_loss": 0.004454326513921842, "ae_encoder_loss": 0.01022445415891707, "actor_loss": -0.23735598170757294, "actor_target_entropy": -6.0, "actor_entropy": 6.012821220397949, "alpha_loss": 0.0019860554868355394, "alpha_value": 0.00038834475725229716, "duration": 15.296419620513916, "step": 12001}
{"episode_reward": -3.0, "episode": 50.0, "batch_reward": -0.0110859375, "critic_loss": 0.039482034746557475, "ae_transition_loss": 0.004128548929700628, "ae_encoder_loss": 0.0105905846869573, "actor_loss": -0.2286109366416931, "actor_target_entropy": -6.0, "actor_entropy": 5.911060626983643, "alpha_loss": 0.002100438442081213, "alpha_value": 0.00036808463269437754, "duration": 15.296161651611328, "step": 12251}
{"episode_reward": -3.0, "episode": 51.0, "batch_reward": -0.0106640625, "critic_loss": 0.041090631332248446, "ae_transition_loss": 0.0039011614583432676, "ae_encoder_loss": 0.010419726862572134, "actor_loss": -0.22602606678009032, "actor_target_entropy": -6.0, "actor_entropy": 5.6579011573791504, "alpha_loss": 0.0016133488416671753, "alpha_value": 0.0003495824267228974, "duration": 15.28646206855774, "step": 12501}
{"episode_reward": -3.0, "episode": 52.0, "batch_reward": -0.010453125, "critic_loss": 0.03824598796665669, "ae_transition_loss": 0.0036935089428443463, "ae_encoder_loss": 0.009259539727121591, "actor_loss": -0.2180778316259384, "actor_target_entropy": -6.0, "actor_entropy": 6.1270315551757815, "alpha_loss": 0.0019326108898967504, "alpha_value": 0.00033252620693736246, "duration": 15.286534309387207, "step": 12751}
{"episode_reward": -1.0, "episode": 53.0, "batch_reward": -0.0105546875, "critic_loss": 0.0397845633327961, "ae_transition_loss": 0.0081570331947878, "ae_encoder_loss": 0.011005396811291576, "actor_loss": -0.21390923166275025, "actor_target_entropy": -6.0, "actor_entropy": 4.91335708579421, "alpha_loss": 0.001520423993933946, "alpha_value": 0.00031442166665493666, "duration": 15.298677206039429, "step": 13001}
{"episode_reward": -3.0, "episode": 54.0, "batch_reward": -0.0102890625, "critic_loss": 0.03846908912807703, "ae_transition_loss": 0.004852044307161123, "ae_encoder_loss": 0.00969101476110518, "actor_loss": -0.2095834822654724, "actor_target_entropy": -6.0, "actor_entropy": 5.718638523101807, "alpha_loss": 0.0013454444971866906, "alpha_value": 0.000300391683400852, "duration": 15.325425386428833, "step": 13251}
{"episode_reward": -3.0, "episode": 55.0, "batch_reward": -0.0103515625, "critic_loss": 0.03969160378724337, "ae_transition_loss": 0.00857829200103879, "ae_encoder_loss": 0.010054080123081803, "actor_loss": -0.20756558299064637, "actor_target_entropy": -6.0, "actor_entropy": 5.114716549873352, "alpha_loss": 0.0013635641888249666, "alpha_value": 0.00028618384295995484, "duration": 15.306473970413208, "step": 13501}
{"episode_reward": -2.0, "episode": 56.0, "batch_reward": -0.0106484375, "critic_loss": 0.03892595221765805, "ae_transition_loss": 0.015299092169152572, "ae_encoder_loss": 0.010168258840218186, "actor_loss": -0.2037108544111252, "actor_target_entropy": -6.0, "actor_entropy": 5.1183854804039, "alpha_loss": 0.0013162156689795665, "alpha_value": 0.00027266234656308575, "duration": 15.292402267456055, "step": 13751}
{"episode_reward": -3.0, "episode": 57.0, "batch_reward": -0.0111484375, "critic_loss": 0.039968334585428235, "ae_transition_loss": 0.003778137132525444, "ae_encoder_loss": 0.00951298223529011, "actor_loss": -0.20316019010543823, "actor_target_entropy": -6.0, "actor_entropy": 6.014816017150879, "alpha_loss": 0.001014593047555536, "alpha_value": 0.0002614200068370717, "duration": 15.401345491409302, "step": 14001}
{"episode_reward": -3.0, "episode": 58.0, "batch_reward": -0.010140625, "critic_loss": 0.039504022508859635, "ae_transition_loss": 0.006406204213388264, "ae_encoder_loss": 0.009512796766124666, "actor_loss": -0.198300119638443, "actor_target_entropy": -6.0, "actor_entropy": 4.842773884773254, "alpha_loss": 0.0007804991635348415, "alpha_value": 0.00025138784462656866, "duration": 15.346400022506714, "step": 14251}
{"episode_reward": -2.0, "episode": 59.0, "batch_reward": -0.01125, "critic_loss": 0.04451032142341137, "ae_transition_loss": 0.024839842775370925, "ae_encoder_loss": 0.011496980893425643, "actor_loss": -0.1894229652285576, "actor_target_entropy": -6.0, "actor_entropy": 5.515916673183441, "alpha_loss": 0.0008978379223844968, "alpha_value": 0.00024220993606994816, "duration": 15.31170129776001, "step": 14501}
{"episode_reward": 0.0, "episode": 60.0, "batch_reward": -0.0099765625, "critic_loss": 0.03970562616363168, "ae_transition_loss": 0.012865737530519254, "ae_encoder_loss": 0.010494936624541878, "actor_loss": -0.19225263488292693, "actor_target_entropy": -6.0, "actor_entropy": 4.607566558241844, "alpha_loss": 0.0008251774289237801, "alpha_value": 0.00023389294473121346, "duration": 15.288890600204468, "step": 14751}
{"episode_reward": -3.0, "episode": 61.0, "batch_reward": -0.010078125, "critic_loss": 0.03805269826576114, "ae_transition_loss": 0.0029235239550471307, "ae_encoder_loss": 0.009869752062484622, "actor_loss": -0.2094856597185135, "actor_target_entropy": -6.0, "actor_entropy": 4.948066097259521, "alpha_loss": 0.00027400510373990985, "alpha_value": 0.000227365313098531, "duration": 212.41270637512207, "step": 15001}
{"episode_reward": -3.0, "episode": 62.0, "batch_reward": -0.0107578125, "critic_loss": 0.04079344685748219, "ae_transition_loss": 0.003284438723931089, "ae_encoder_loss": 0.009901042700745166, "actor_loss": -0.20869350326061248, "actor_target_entropy": -6.0, "actor_entropy": 4.8447706985473635, "alpha_loss": 7.844828675297322e-05, "alpha_value": 0.000225428455375785, "duration": 15.280570030212402, "step": 15251}
{"episode_reward": -3.0, "episode": 63.0, "batch_reward": -0.010234375, "critic_loss": 0.03935044874623418, "ae_transition_loss": 0.003585230674012564, "ae_encoder_loss": 0.009332483890932053, "actor_loss": -0.20487447369098663, "actor_target_entropy": -6.0, "actor_entropy": 5.26566697883606, "alpha_loss": 0.00014509004339197419, "alpha_value": 0.0002245643420789241, "duration": 15.290085077285767, "step": 15501}
{"episode_reward": -3.0, "episode": 64.0, "batch_reward": -0.0103125, "critic_loss": 0.03895690824650228, "ae_transition_loss": 0.003922358578303829, "ae_encoder_loss": 0.009166376436129213, "actor_loss": -0.20714003455638885, "actor_target_entropy": -6.0, "actor_entropy": 5.129487480163574, "alpha_loss": 0.0001877839183289325, "alpha_value": 0.00022165428992779442, "duration": 15.267515182495117, "step": 15751}
{"episode_reward": -3.0, "episode": 65.0, "batch_reward": -0.011265625, "critic_loss": 0.039491452496498824, "ae_transition_loss": 0.0032689774965401737, "ae_encoder_loss": 0.009362012055702508, "actor_loss": -0.20178103959560395, "actor_target_entropy": -6.0, "actor_entropy": 5.104564903259277, "alpha_loss": 0.00023201189641986275, "alpha_value": 0.00021933934277263122, "duration": 15.280822515487671, "step": 16001}
{"episode_reward": -3.0, "episode": 66.0, "batch_reward": -0.010734375, "critic_loss": 0.03942075852304697, "ae_transition_loss": 0.004330795343499631, "ae_encoder_loss": 0.009452578883618116, "actor_loss": -0.19640879786014556, "actor_target_entropy": -6.0, "actor_entropy": 5.326492156982422, "alpha_loss": 0.00012859548685810297, "alpha_value": 0.00021654193443045098, "duration": 15.292070627212524, "step": 16251}
{"episode_reward": -3.0, "episode": 67.0, "batch_reward": -0.009375, "critic_loss": 0.03868699996173382, "ae_transition_loss": 0.01806249590869993, "ae_encoder_loss": 0.009991526299156249, "actor_loss": -0.18995028030872346, "actor_target_entropy": -6.0, "actor_entropy": 5.297599169969558, "alpha_loss": 0.00045863277555326933, "alpha_value": 0.00021308686787649877, "duration": 15.317739486694336, "step": 16501}
{"episode_reward": -3.0, "episode": 68.0, "batch_reward": -0.01025, "critic_loss": 0.040215970680117605, "ae_transition_loss": 0.003474869450321421, "ae_encoder_loss": 0.009177466383203865, "actor_loss": -0.18647280049324036, "actor_target_entropy": -6.0, "actor_entropy": 5.432460542678833, "alpha_loss": 0.00046642738196533176, "alpha_value": 0.00020550251145188642, "duration": 15.28902006149292, "step": 16751}
{"episode_reward": -3.0, "episode": 69.0, "batch_reward": -0.010703125, "critic_loss": 0.040973986841738225, "ae_transition_loss": 0.007124905528035015, "ae_encoder_loss": 0.009763254782184958, "actor_loss": -0.18573535430431365, "actor_target_entropy": -6.0, "actor_entropy": 5.356156869888306, "alpha_loss": 0.0004259605026745703, "alpha_value": 0.00019822327849806467, "duration": 15.294409036636353, "step": 17001}
{"episode_reward": 0.0, "episode": 70.0, "batch_reward": -0.0105, "critic_loss": 0.042791749201714996, "ae_transition_loss": 0.01621842899732292, "ae_encoder_loss": 0.010488424375653267, "actor_loss": -0.1814658622145653, "actor_target_entropy": -6.0, "actor_entropy": 5.465559072494507, "alpha_loss": 0.00045190270444436467, "alpha_value": 0.00019134242960111832, "duration": 15.307806491851807, "step": 17251}
{"episode_reward": -2.0, "episode": 71.0, "batch_reward": -0.0094453125, "critic_loss": 0.044297850608825684, "ae_transition_loss": 0.004574298122432083, "ae_encoder_loss": 0.010350259392522276, "actor_loss": -0.18442775082588195, "actor_target_entropy": -6.0, "actor_entropy": 5.3310855781883, "alpha_loss": 3.194271989923436e-05, "alpha_value": 0.0001887450382500313, "duration": 15.328420877456665, "step": 17501}
{"episode_reward": -3.0, "episode": 72.0, "batch_reward": -0.01078125, "critic_loss": 0.041440115634351966, "ae_transition_loss": 0.0038542679351521655, "ae_encoder_loss": 0.009708794976118953, "actor_loss": -0.19101277506351472, "actor_target_entropy": -6.0, "actor_entropy": 5.146104690551758, "alpha_loss": 0.0004078957053134218, "alpha_value": 0.00018351056650079566, "duration": 15.304108381271362, "step": 17751}
{"episode_reward": -3.0, "episode": 73.0, "batch_reward": -0.0099921875, "critic_loss": 0.03984898908436298, "ae_transition_loss": 0.0037127268069889396, "ae_encoder_loss": 0.009314987506717444, "actor_loss": -0.18821485567092897, "actor_target_entropy": -6.0, "actor_entropy": 5.357521068572998, "alpha_loss": 0.00038126886729151013, "alpha_value": 0.0001772826387722289, "duration": 15.296476125717163, "step": 18001}
{"episode_reward": -3.0, "episode": 74.0, "batch_reward": -0.0106171875, "critic_loss": 0.04119772740453482, "ae_transition_loss": 0.004115181318949908, "ae_encoder_loss": 0.009178637294098735, "actor_loss": -0.18770909810066222, "actor_target_entropy": -6.0, "actor_entropy": 5.374038311004639, "alpha_loss": 0.0005834375368431211, "alpha_value": 0.00016870008522972574, "duration": 15.292357921600342, "step": 18251}
{"episode_reward": -3.0, "episode": 75.0, "batch_reward": -0.0110390625, "critic_loss": 0.04595645979046822, "ae_transition_loss": 0.01969707663310692, "ae_encoder_loss": 0.010488588832318783, "actor_loss": -0.17591834038496018, "actor_target_entropy": -6.0, "actor_entropy": 4.82205653333664, "alpha_loss": 0.0003749835971393622, "alpha_value": 0.0001598622695563035, "duration": 15.304347515106201, "step": 18501}
{"episode_reward": -3.0, "episode": 76.0, "batch_reward": -0.0098046875, "critic_loss": 0.04559830818325281, "ae_transition_loss": 0.0033386170788435266, "ae_encoder_loss": 0.009533619116991758, "actor_loss": -0.19587391579151153, "actor_target_entropy": -6.0, "actor_entropy": 5.088036804199219, "alpha_loss": -5.92571474298893e-05, "alpha_value": 0.0001575158766287411, "duration": 15.29737114906311, "step": 18751}
{"episode_reward": -3.0, "episode": 77.0, "batch_reward": -0.010703125, "critic_loss": 0.043322968263179065, "ae_transition_loss": 0.004892231843201444, "ae_encoder_loss": 0.009704591696150601, "actor_loss": -0.1953342043161392, "actor_target_entropy": -6.0, "actor_entropy": 5.610654903411866, "alpha_loss": 0.00017032105845282786, "alpha_value": 0.0001559239852008235, "duration": 15.293675422668457, "step": 19001}
{"episode_reward": -3.0, "episode": 78.0, "batch_reward": -0.0101796875, "critic_loss": 0.043279896952211856, "ae_transition_loss": 0.004047563435044139, "ae_encoder_loss": 0.009151000129058957, "actor_loss": -0.19734839344024657, "actor_target_entropy": -6.0, "actor_entropy": 5.693470943450928, "alpha_loss": 0.0001131616541169933, "alpha_value": 0.00015333270143075235, "duration": 15.306248903274536, "step": 19251}
{"episode_reward": -3.0, "episode": 79.0, "batch_reward": -0.0106015625, "critic_loss": 0.04627827540785074, "ae_transition_loss": 0.010439351524459198, "ae_encoder_loss": 0.009929348487406968, "actor_loss": -0.2093020006418228, "actor_target_entropy": -6.0, "actor_entropy": 4.831170834541321, "alpha_loss": -0.00011273273383994819, "alpha_value": 0.00015310889076478634, "duration": 15.299322843551636, "step": 19501}
{"episode_reward": -3.0, "episode": 80.0, "batch_reward": -0.0101171875, "critic_loss": 0.04493315298855305, "ae_transition_loss": 0.003826305897673592, "ae_encoder_loss": 0.00927438509091735, "actor_loss": -0.21285430788993837, "actor_target_entropy": -6.0, "actor_entropy": 5.741453351974488, "alpha_loss": 6.418482698063598e-05, "alpha_value": 0.0001544008861958864, "duration": 15.290317058563232, "step": 19751}
{"episode_reward": -3.0, "episode": 81.0, "batch_reward": -0.0108828125, "critic_loss": 0.04662589324265719, "ae_transition_loss": 0.004083355525974185, "ae_encoder_loss": 0.009595648832153528, "actor_loss": -0.21652162086963653, "actor_target_entropy": -6.0, "actor_entropy": 5.203651058197021, "alpha_loss": -5.456701758521376e-05, "alpha_value": 0.00015308746569603715, "duration": 214.37987875938416, "step": 20001}
{"episode_reward": -3.0, "episode": 82.0, "batch_reward": -0.01021875, "critic_loss": 0.046542226031422614, "ae_transition_loss": 0.006510580282192677, "ae_encoder_loss": 0.00967900034878403, "actor_loss": -0.21335523426532746, "actor_target_entropy": -6.0, "actor_entropy": 5.211801420211792, "alpha_loss": 1.8311399340745992e-05, "alpha_value": 0.00015412225869856203, "duration": 15.366698026657104, "step": 20251}
{"episode_reward": -3.0, "episode": 83.0, "batch_reward": -0.0106484375, "critic_loss": 0.0500146751254797, "ae_transition_loss": 0.021162023664452134, "ae_encoder_loss": 0.01104185472568497, "actor_loss": -0.24079126477241516, "actor_target_entropy": -6.0, "actor_entropy": 4.2822102607488635, "alpha_loss": -0.00029433842182334047, "alpha_value": 0.00015732665086160428, "duration": 15.285242080688477, "step": 20501}
{"episode_reward": -3.0, "episode": 84.0, "batch_reward": -0.0104921875, "critic_loss": 0.04914851768314838, "ae_transition_loss": 0.005412965716444887, "ae_encoder_loss": 0.009900274187326431, "actor_loss": -0.2998569024801254, "actor_target_entropy": -6.0, "actor_entropy": 4.942692066192627, "alpha_loss": -0.00017458718970010523, "alpha_value": 0.00016531309759689047, "duration": 15.285539627075195, "step": 20751}
{"episode_reward": -3.0, "episode": 85.0, "batch_reward": -0.010296875, "critic_loss": 0.059036958910524844, "ae_transition_loss": 0.016640381059085486, "ae_encoder_loss": 0.010599521423690021, "actor_loss": -0.304547544002533, "actor_target_entropy": -6.0, "actor_entropy": 4.75341415143013, "alpha_loss": -0.0002874362744332757, "alpha_value": 0.0001727822932261977, "duration": 15.291504621505737, "step": 21001}
{"episode_reward": -3.0, "episode": 86.0, "batch_reward": -0.0104296875, "critic_loss": 0.05421553246676922, "ae_transition_loss": 0.003681440137792379, "ae_encoder_loss": 0.009931074514053761, "actor_loss": -0.36621753716468813, "actor_target_entropy": -6.0, "actor_entropy": 5.136640651702881, "alpha_loss": -9.063562652590917e-05, "alpha_value": 0.00017870713107696564, "duration": 15.290543794631958, "step": 21251}
{"episode_reward": -3.0, "episode": 87.0, "batch_reward": -0.01084375, "critic_loss": 0.05967563641816378, "ae_transition_loss": 0.00585558675788343, "ae_encoder_loss": 0.01030738065391779, "actor_loss": -0.39572656440734866, "actor_target_entropy": -6.0, "actor_entropy": 5.541062961578369, "alpha_loss": -0.00011779325303359656, "alpha_value": 0.00018146939819804595, "duration": 15.272292375564575, "step": 21501}
{"episode_reward": -3.0, "episode": 88.0, "batch_reward": -0.0109296875, "critic_loss": 0.07113025578111411, "ae_transition_loss": 0.01872936181910336, "ae_encoder_loss": 0.011968700014054775, "actor_loss": -0.4380458519458771, "actor_target_entropy": -6.0, "actor_entropy": 5.109088996887207, "alpha_loss": -0.0005375761988398154, "alpha_value": 0.0001931194436458279, "duration": 15.304500579833984, "step": 21751}
{"episode_reward": -3.0, "episode": 89.0, "batch_reward": -0.0110234375, "critic_loss": 0.06670355626195669, "ae_transition_loss": 0.009210475662490353, "ae_encoder_loss": 0.010965039587579668, "actor_loss": -0.4708267669677734, "actor_target_entropy": -6.0, "actor_entropy": 4.824507930934429, "alpha_loss": -8.118124175234698e-05, "alpha_value": 0.00020423366996106214, "duration": 15.316461086273193, "step": 22001}
{"episode_reward": -3.0, "episode": 90.0, "batch_reward": -0.0106484375, "critic_loss": 0.0737603108137846, "ae_transition_loss": 0.014094599836040288, "ae_encoder_loss": 0.010406581595540046, "actor_loss": -0.5501328902244568, "actor_target_entropy": -6.0, "actor_entropy": 4.578559572696686, "alpha_loss": -9.87724570732098e-05, "alpha_value": 0.00021460897422275182, "duration": 15.273946285247803, "step": 22251}
{"episode_reward": -3.0, "episode": 91.0, "batch_reward": -0.0111484375, "critic_loss": 0.06972242630273104, "ae_transition_loss": 0.0029738548065070063, "ae_encoder_loss": 0.009732450713403522, "actor_loss": -0.5572051362991333, "actor_target_entropy": -6.0, "actor_entropy": 5.865665161132813, "alpha_loss": 0.0001308134044520557, "alpha_value": 0.00021049710718398928, "duration": 15.281939506530762, "step": 22501}
{"episode_reward": -3.0, "episode": 92.0, "batch_reward": -0.01128125, "critic_loss": 0.07504338791221381, "ae_transition_loss": 0.0036495168609544633, "ae_encoder_loss": 0.009882198408246041, "actor_loss": -0.5669152584075928, "actor_target_entropy": -6.0, "actor_entropy": 5.842209289550781, "alpha_loss": 4.952335695998045e-05, "alpha_value": 0.00020572781093239437, "duration": 15.29621434211731, "step": 22751}
{"episode_reward": -3.0, "episode": 93.0, "batch_reward": -0.0098125, "critic_loss": 0.07373056532442569, "ae_transition_loss": 0.003493356094462797, "ae_encoder_loss": 0.009071255354210734, "actor_loss": -0.5891877586841583, "actor_target_entropy": -6.0, "actor_entropy": 5.667647838592529, "alpha_loss": 7.51394348917529e-05, "alpha_value": 0.00020323893815854866, "duration": 15.299139499664307, "step": 23001}
{"episode_reward": -3.0, "episode": 94.0, "batch_reward": -0.0104375, "critic_loss": 0.083129877358675, "ae_transition_loss": 0.003752628036774695, "ae_encoder_loss": 0.009508524162694812, "actor_loss": -0.5975057451725007, "actor_target_entropy": -6.0, "actor_entropy": 5.672934230804444, "alpha_loss": -9.676522159861634e-05, "alpha_value": 0.00020214596902227725, "duration": 15.285680294036865, "step": 23251}
{"episode_reward": -3.0, "episode": 95.0, "batch_reward": -0.0104921875, "critic_loss": 0.08571825988590717, "ae_transition_loss": 0.004910183503758162, "ae_encoder_loss": 0.009577014970593154, "actor_loss": -0.59889142370224, "actor_target_entropy": -6.0, "actor_entropy": 5.778835189819336, "alpha_loss": -0.00017341228076838888, "alpha_value": 0.00020917282339921216, "duration": 15.285365104675293, "step": 23501}
{"episode_reward": -3.0, "episode": 96.0, "batch_reward": -0.010515625, "critic_loss": 0.11185674715042114, "ae_transition_loss": 0.020575110527686773, "ae_encoder_loss": 0.011033921731635928, "actor_loss": -0.5972196147441864, "actor_target_entropy": -6.0, "actor_entropy": 5.34882745718956, "alpha_loss": -0.0012992045592181967, "alpha_value": 0.00024090773724891662, "duration": 15.295993089675903, "step": 23751}
{"episode_reward": -2.0, "episode": 97.0, "batch_reward": -0.0110234375, "critic_loss": 0.09796906936913728, "ae_transition_loss": 0.0034840877554379404, "ae_encoder_loss": 0.00997034855838865, "actor_loss": -0.6808568329811097, "actor_target_entropy": -6.0, "actor_entropy": 5.658522674322128, "alpha_loss": 0.0003652886413910892, "alpha_value": 0.0002761076367438198, "duration": 15.3075852394104, "step": 24001}
{"episode_reward": -3.0, "episode": 98.0, "batch_reward": -0.0108359375, "critic_loss": 0.11727459728717804, "ae_transition_loss": 0.015474366788053886, "ae_encoder_loss": 0.010419443821534514, "actor_loss": -0.7049471817016602, "actor_target_entropy": -6.0, "actor_entropy": 5.240219237983227, "alpha_loss": -0.00041833446896634994, "alpha_value": 0.00027184304842154365, "duration": 15.281386375427246, "step": 24251}
{"episode_reward": 0.0, "episode": 99.0, "batch_reward": -0.011765625, "critic_loss": 0.11239694477617741, "ae_transition_loss": 0.003349044933449477, "ae_encoder_loss": 0.009835999213159084, "actor_loss": -0.7470185341835022, "actor_target_entropy": -6.0, "actor_entropy": 5.370207027435303, "alpha_loss": 0.0003261692744999891, "alpha_value": 0.00027509743315965026, "duration": 15.312740325927734, "step": 24501}
{"episode_reward": -3.0, "episode": 100.0, "batch_reward": -0.0107890625, "critic_loss": 0.11649575304239988, "ae_transition_loss": 0.0076916961688548325, "ae_encoder_loss": 0.01001114909350872, "actor_loss": -0.7588933219909668, "actor_target_entropy": -6.0, "actor_entropy": 5.3928920211791995, "alpha_loss": 0.00022834481004974804, "alpha_value": 0.0002617751649495339, "duration": 15.2828369140625, "step": 24751}
{"episode_reward": -3.0, "episode": 101.0, "batch_reward": -0.011109375, "critic_loss": 0.1700960746705532, "ae_transition_loss": 0.02324661356350407, "ae_encoder_loss": 0.01055275457818061, "actor_loss": -0.8664354286193847, "actor_target_entropy": -6.0, "actor_entropy": 3.739614377781749, "alpha_loss": -0.0011238658234069589, "alpha_value": 0.00028375269123548385, "duration": 212.24298357963562, "step": 25001}
{"episode_reward": -3.0, "episode": 102.0, "batch_reward": -0.010625, "critic_loss": 0.16564752580225467, "ae_transition_loss": 0.002970327081391588, "ae_encoder_loss": 0.009522096110507847, "actor_loss": -0.9756113028526306, "actor_target_entropy": -6.0, "actor_entropy": 5.851656646728515, "alpha_loss": 2.2080894486862233e-05, "alpha_value": 0.00030106292361022096, "duration": 15.294785022735596, "step": 25251}
{"episode_reward": -3.0, "episode": 103.0, "batch_reward": -0.0097578125, "critic_loss": 0.17161353842914104, "ae_transition_loss": 0.003295605265069753, "ae_encoder_loss": 0.009453609395772219, "actor_loss": -1.045679352760315, "actor_target_entropy": -6.0, "actor_entropy": 5.932263679504395, "alpha_loss": 6.487986080173869e-05, "alpha_value": 0.0002988195999784974, "duration": 15.279752969741821, "step": 25501}
{"episode_reward": -3.0, "episode": 104.0, "batch_reward": -0.0101328125, "critic_loss": 0.17873467427492143, "ae_transition_loss": 0.0033486039726994933, "ae_encoder_loss": 0.009406902575865388, "actor_loss": -1.0686746916770935, "actor_target_entropy": -6.0, "actor_entropy": 5.896237007141114, "alpha_loss": 0.00012139043721253984, "alpha_value": 0.00029511456437400296, "duration": 15.274933338165283, "step": 25751}
{"episode_reward": -3.0, "episode": 105.0, "batch_reward": -0.011484375, "critic_loss": 0.19466680152714252, "ae_transition_loss": 0.0039793384186923506, "ae_encoder_loss": 0.010030224799644202, "actor_loss": -1.0855681405067443, "actor_target_entropy": -6.0, "actor_entropy": 5.885151931762695, "alpha_loss": 3.1772445916431025e-05, "alpha_value": 0.0002898284197413169, "duration": 15.286319255828857, "step": 26001}
{"episode_reward": -3.0, "episode": 106.0, "batch_reward": -0.0104921875, "critic_loss": 0.19790602698922158, "ae_transition_loss": 0.004189637629315257, "ae_encoder_loss": 0.009618518310599029, "actor_loss": -1.1102572073936463, "actor_target_entropy": -6.0, "actor_entropy": 5.918102596282959, "alpha_loss": 0.0001276134935906157, "alpha_value": 0.0002876926748438969, "duration": 15.308715581893921, "step": 26251}
{"episode_reward": -3.0, "episode": 107.0, "batch_reward": -0.0108359375, "critic_loss": 0.19795232966542245, "ae_transition_loss": 0.005146137727424502, "ae_encoder_loss": 0.009829447168856859, "actor_loss": -1.1188296103477477, "actor_target_entropy": -6.0, "actor_entropy": 5.787794784545898, "alpha_loss": 0.0001290369722119067, "alpha_value": 0.0002802017682028384, "duration": 15.284154891967773, "step": 26501}
{"episode_reward": -3.0, "episode": 108.0, "batch_reward": -0.0112265625, "critic_loss": 0.2123063061386347, "ae_transition_loss": 0.009411662011872978, "ae_encoder_loss": 0.010710721013136209, "actor_loss": -1.143962100982666, "actor_target_entropy": -6.0, "actor_entropy": 4.812084106445313, "alpha_loss": -0.00027124413450655994, "alpha_value": 0.00028416165416507736, "duration": 15.290232181549072, "step": 26751}
{"episode_reward": -1.0, "episode": 109.0, "batch_reward": -0.010359375, "critic_loss": 0.23150440657138824, "ae_transition_loss": 0.00690123118693009, "ae_encoder_loss": 0.010061416489537805, "actor_loss": -1.2077266292572022, "actor_target_entropy": -6.0, "actor_entropy": 5.353026393890381, "alpha_loss": 0.0001447120759403333, "alpha_value": 0.000290002754524786, "duration": 15.294069051742554, "step": 27001}
{"episode_reward": 0.0, "episode": 110.0, "batch_reward": -0.010875, "critic_loss": 0.23730182000994682, "ae_transition_loss": 0.005032443938311189, "ae_encoder_loss": 0.009937420631758868, "actor_loss": -1.2540282278060912, "actor_target_entropy": -6.0, "actor_entropy": 5.251985039710998, "alpha_loss": 0.00010792114258219953, "alpha_value": 0.00028066245039014905, "duration": 15.301123857498169, "step": 27251}
{"episode_reward": -1.0, "episode": 111.0, "batch_reward": -0.0109453125, "critic_loss": 0.3518951129466295, "ae_transition_loss": 0.011636240054154768, "ae_encoder_loss": 0.010933824161067605, "actor_loss": -1.2770799021720887, "actor_target_entropy": -6.0, "actor_entropy": 5.163205406188965, "alpha_loss": -0.00010061825595039409, "alpha_value": 0.00027238369316053544, "duration": 15.284178972244263, "step": 27501}
{"episode_reward": -2.0, "episode": 112.0, "batch_reward": -0.0106953125, "critic_loss": 0.24795707762241365, "ae_transition_loss": 0.003782126280479133, "ae_encoder_loss": 0.009826850448735058, "actor_loss": -1.3477651290893555, "actor_target_entropy": -6.0, "actor_entropy": 5.2775550365448, "alpha_loss": 7.615380488277878e-05, "alpha_value": 0.00028161751894353247, "duration": 15.288381099700928, "step": 27751}
{"episode_reward": -3.0, "episode": 113.0, "batch_reward": -0.0113359375, "critic_loss": 0.26484947311878204, "ae_transition_loss": 0.0043240690734237435, "ae_encoder_loss": 0.010082190607674419, "actor_loss": -1.3883513031005859, "actor_target_entropy": -6.0, "actor_entropy": 4.9457082767486575, "alpha_loss": 7.210112802567892e-06, "alpha_value": 0.00027634812413576774, "duration": 15.305376529693604, "step": 28001}
{"episode_reward": -3.0, "episode": 114.0, "batch_reward": -0.0106796875, "critic_loss": 0.2663680274486542, "ae_transition_loss": 0.005257604965474456, "ae_encoder_loss": 0.010036327531561256, "actor_loss": -1.4237619323730468, "actor_target_entropy": -6.0, "actor_entropy": 4.6250818214416505, "alpha_loss": -0.0002727260654064594, "alpha_value": 0.0002838938147413246, "duration": 15.282110691070557, "step": 28251}
{"episode_reward": -3.0, "episode": 115.0, "batch_reward": -0.01021875, "critic_loss": 0.5778613144159317, "ae_transition_loss": 0.013620269564911722, "ae_encoder_loss": 0.011124843334779142, "actor_loss": -1.4495296554565429, "actor_target_entropy": -6.0, "actor_entropy": 3.366881803512573, "alpha_loss": -0.0019649923876859246, "alpha_value": 0.0003313555069855929, "duration": 15.287951469421387, "step": 28501}
{"episode_reward": 0.0, "episode": 116.0, "batch_reward": -0.010296875, "critic_loss": 0.31442611616849897, "ae_transition_loss": 0.008173629622673616, "ae_encoder_loss": 0.010485157947987317, "actor_loss": -1.515069637298584, "actor_target_entropy": -6.0, "actor_entropy": 4.672504648208618, "alpha_loss": -0.00014716086503176485, "alpha_value": 0.00041264950875013356, "duration": 15.309777975082397, "step": 28751}
{"episode_reward": -3.0, "episode": 117.0, "batch_reward": -0.01025, "critic_loss": 0.3136604231894016, "ae_transition_loss": 0.003175684817833826, "ae_encoder_loss": 0.009488751254975796, "actor_loss": -1.599723443031311, "actor_target_entropy": -6.0, "actor_entropy": 5.332538669586182, "alpha_loss": 0.00048497046509874054, "alpha_value": 0.00040250014768071886, "duration": 15.320289611816406, "step": 29001}
{"episode_reward": -3.0, "episode": 118.0, "batch_reward": -0.00990625, "critic_loss": 0.34480977164208887, "ae_transition_loss": 0.004112178765237331, "ae_encoder_loss": 0.00930532135348767, "actor_loss": -1.6138527431488037, "actor_target_entropy": -6.0, "actor_entropy": 4.980911191940308, "alpha_loss": 0.0001766010558931157, "alpha_value": 0.00038462989543236654, "duration": 15.276288986206055, "step": 29251}
{"episode_reward": -3.0, "episode": 119.0, "batch_reward": -0.010546875, "critic_loss": 0.430717493057251, "ae_transition_loss": 0.0143417541582603, "ae_encoder_loss": 0.010524423574097455, "actor_loss": -1.6761668615341188, "actor_target_entropy": -6.0, "actor_entropy": 5.468154582977295, "alpha_loss": -0.0008606872782256687, "alpha_value": 0.0004025305915106719, "duration": 15.305301666259766, "step": 29501}
{"episode_reward": 0.0, "episode": 120.0, "batch_reward": -0.0106015625, "critic_loss": 0.43035873913764955, "ae_transition_loss": 0.003254231815459207, "ae_encoder_loss": 0.010117855242453516, "actor_loss": -1.8050424394607545, "actor_target_entropy": -6.0, "actor_entropy": 5.44396120262146, "alpha_loss": 0.00036536338840960526, "alpha_value": 0.0004136315139243911, "duration": 15.38572382926941, "step": 29751}
{"episode_reward": -3.0, "episode": 121.0, "batch_reward": -0.01015625, "critic_loss": 0.48154448890686036, "ae_transition_loss": 0.0034007243326632307, "ae_encoder_loss": 0.009828478871379047, "actor_loss": -1.8767663488388062, "actor_target_entropy": -6.0, "actor_entropy": 4.896804244995117, "alpha_loss": 7.375310786301271e-05, "alpha_value": 0.0004020451430218691, "duration": 213.79987740516663, "step": 30001}
{"episode_reward": -3.0, "episode": 122.0, "batch_reward": -0.0106015625, "critic_loss": 0.47748984348773954, "ae_transition_loss": 0.003474104994442314, "ae_encoder_loss": 0.009738409794867038, "actor_loss": -1.922069459915161, "actor_target_entropy": -6.0, "actor_entropy": 4.9722072238922115, "alpha_loss": -0.00018353054598264863, "alpha_value": 0.0004056579341655567, "duration": 15.366135120391846, "step": 30251}
{"episode_reward": -3.0, "episode": 123.0, "batch_reward": -0.01065625, "critic_loss": 0.47756643749773503, "ae_transition_loss": 0.0032260390873998404, "ae_encoder_loss": 0.00962557293381542, "actor_loss": -1.9543228712081908, "actor_target_entropy": -6.0, "actor_entropy": 5.319397819519043, "alpha_loss": 0.00011775903354282491, "alpha_value": 0.00041318229813994004, "duration": 15.298245668411255, "step": 30501}
{"episode_reward": -3.0, "episode": 124.0, "batch_reward": -0.0101484375, "critic_loss": 0.4646108892261982, "ae_transition_loss": 0.0036558583485893905, "ae_encoder_loss": 0.009809914212673902, "actor_loss": -1.9850316219329833, "actor_target_entropy": -6.0, "actor_entropy": 5.176413845062256, "alpha_loss": 6.156532469321974e-05, "alpha_value": 0.0004045038394483579, "duration": 15.314134120941162, "step": 30751}
{"episode_reward": -3.0, "episode": 125.0, "batch_reward": -0.0107578125, "critic_loss": 0.4786775682270527, "ae_transition_loss": 0.003179546699859202, "ae_encoder_loss": 0.009597336519509553, "actor_loss": -1.9864918279647827, "actor_target_entropy": -6.0, "actor_entropy": 4.688660699844361, "alpha_loss": -0.0003997749910849961, "alpha_value": 0.00041943330890966844, "duration": 15.299437284469604, "step": 31001}
{"episode_reward": -3.0, "episode": 126.0, "batch_reward": -0.0103203125, "critic_loss": 0.4965551070272923, "ae_transition_loss": 0.00398816786322277, "ae_encoder_loss": 0.00971210846118629, "actor_loss": -2.0190176296234132, "actor_target_entropy": -6.0, "actor_entropy": 4.934433017730713, "alpha_loss": 0.00012291361001553013, "alpha_value": 0.0004217023381157241, "duration": 15.284513235092163, "step": 31251}
{"episode_reward": -3.0, "episode": 127.0, "batch_reward": -0.0108984375, "critic_loss": 0.5086350972354412, "ae_transition_loss": 0.0035770247823093087, "ae_encoder_loss": 0.009984070512466133, "actor_loss": -2.0432191066741945, "actor_target_entropy": -6.0, "actor_entropy": 5.250129152297974, "alpha_loss": 0.00036575118290784305, "alpha_value": 0.00041493918033840737, "duration": 15.271951675415039, "step": 31501}
{"episode_reward": -3.0, "episode": 128.0, "batch_reward": -0.0102265625, "critic_loss": 0.6506221281588077, "ae_transition_loss": 0.012195806019939482, "ae_encoder_loss": 0.010770138453692197, "actor_loss": -2.0690574007034304, "actor_target_entropy": -6.0, "actor_entropy": 5.22051333618164, "alpha_loss": -0.001375130172687932, "alpha_value": 0.00042522719702410235, "duration": 15.285969257354736, "step": 31751}
{"episode_reward": 0.0, "episode": 129.0, "batch_reward": -0.0109296875, "critic_loss": 0.5385957321822643, "ae_transition_loss": 0.003163511720253155, "ae_encoder_loss": 0.009928768767975271, "actor_loss": -2.232444083213806, "actor_target_entropy": -6.0, "actor_entropy": 5.181557460784912, "alpha_loss": 0.0006197872363700299, "alpha_value": 0.0004619804714756823, "duration": 15.286695957183838, "step": 32001}
{"episode_reward": -3.0, "episode": 130.0, "batch_reward": -0.0105546875, "critic_loss": 0.5816242052912712, "ae_transition_loss": 0.003497268367325887, "ae_encoder_loss": 0.009962612398900091, "actor_loss": -2.244866374015808, "actor_target_entropy": -6.0, "actor_entropy": 5.436255832672119, "alpha_loss": 0.0005503162191016599, "alpha_value": 0.0004223995321246897, "duration": 15.29350233078003, "step": 32251}
{"episode_reward": -3.0, "episode": 131.0, "batch_reward": -0.0107734375, "critic_loss": 0.5870410215556622, "ae_transition_loss": 0.0038087961151031776, "ae_encoder_loss": 0.009736860760487616, "actor_loss": -2.291518610954285, "actor_target_entropy": -6.0, "actor_entropy": 5.297162574768066, "alpha_loss": 0.00041173084442561956, "alpha_value": 0.000397486174160799, "duration": 15.299968957901001, "step": 32501}
{"episode_reward": -3.0, "episode": 132.0, "batch_reward": -0.0107734375, "critic_loss": 0.7604105278253556, "ae_transition_loss": 0.009289119599387049, "ae_encoder_loss": 0.010381447739899158, "actor_loss": -2.292127636909485, "actor_target_entropy": -6.0, "actor_entropy": 4.734950222015381, "alpha_loss": -0.0005008931086631492, "alpha_value": 0.0003845987080175939, "duration": 15.297459602355957, "step": 32751}
{"episode_reward": -1.0, "episode": 133.0, "batch_reward": -0.0111640625, "critic_loss": 0.6660976782441139, "ae_transition_loss": 0.0033569463049061598, "ae_encoder_loss": 0.009978888416662812, "actor_loss": -2.3472058839797976, "actor_target_entropy": -6.0, "actor_entropy": 5.325942691802979, "alpha_loss": 0.00033564073260640724, "alpha_value": 0.0004137991927856546, "duration": 15.281841516494751, "step": 33001}
{"episode_reward": 0.0, "episode": 134.0, "batch_reward": -0.009625, "critic_loss": 0.7800662515461445, "ae_transition_loss": 0.006902461242862046, "ae_encoder_loss": 0.01026233605388552, "actor_loss": -2.3364253120422362, "actor_target_entropy": -6.0, "actor_entropy": 4.274810135602951, "alpha_loss": 0.0004106259315740317, "alpha_value": 0.00037429325567253397, "duration": 15.317102193832397, "step": 33251}
{"episode_reward": -1.0, "episode": 135.0, "batch_reward": -0.0097890625, "critic_loss": 0.707892392128706, "ae_transition_loss": 0.0029868552098050714, "ae_encoder_loss": 0.0096750157661736, "actor_loss": -2.4286793422698976, "actor_target_entropy": -6.0, "actor_entropy": 5.588437069892883, "alpha_loss": -0.0004768503666709876, "alpha_value": 0.00039403060906718034, "duration": 15.294678449630737, "step": 33501}
{"episode_reward": -3.0, "episode": 136.0, "batch_reward": -0.0100625, "critic_loss": 1.1062840210795402, "ae_transition_loss": 0.022815693612676113, "ae_encoder_loss": 0.011504842172376812, "actor_loss": -2.3953869428634644, "actor_target_entropy": -6.0, "actor_entropy": 4.169126113891601, "alpha_loss": -0.00031092804262880237, "alpha_value": 0.00039508843335649997, "duration": 15.299357891082764, "step": 33751}
{"episode_reward": 0.0, "episode": 137.0, "batch_reward": -0.010609375, "critic_loss": 0.8069079947471619, "ae_transition_loss": 0.002373480524402112, "ae_encoder_loss": 0.009973545140586794, "actor_loss": -2.0555584468841555, "actor_target_entropy": -6.0, "actor_entropy": 3.8886520004272462, "alpha_loss": -0.00013852953136665746, "alpha_value": 0.0004086952895717205, "duration": 15.293320178985596, "step": 34001}
{"episode_reward": -3.0, "episode": 138.0, "batch_reward": -0.0112578125, "critic_loss": 0.8644785054922104, "ae_transition_loss": 0.0027214574015233667, "ae_encoder_loss": 0.010054879036732018, "actor_loss": -2.2097695083618163, "actor_target_entropy": -6.0, "actor_entropy": 4.816313413619995, "alpha_loss": -0.0006251473812444601, "alpha_value": 0.0004347596563359929, "duration": 15.30606722831726, "step": 34251}
{"episode_reward": 0.0, "episode": 139.0, "batch_reward": -0.009796875, "critic_loss": 0.8587575714588165, "ae_transition_loss": 0.0031531563703902066, "ae_encoder_loss": 0.01015793691109866, "actor_loss": -2.2767056636810303, "actor_target_entropy": -6.0, "actor_entropy": 4.903898057937622, "alpha_loss": -0.00023823584386263973, "alpha_value": 0.00046015388943516374, "duration": 15.311779975891113, "step": 34501}
{"episode_reward": 0.0, "episode": 140.0, "batch_reward": -0.01053125, "critic_loss": 0.8232819149792194, "ae_transition_loss": 0.0030521699103992434, "ae_encoder_loss": 0.00994754388811998, "actor_loss": -2.290238582611084, "actor_target_entropy": -6.0, "actor_entropy": 4.779256645202636, "alpha_loss": 1.6171040188055486e-06, "alpha_value": 0.0004706055098817056, "duration": 15.282341718673706, "step": 34751}
{"episode_reward": -1.0, "episode": 141.0, "batch_reward": -0.0096953125, "critic_loss": 0.7639199969172478, "ae_transition_loss": 0.002991928957402706, "ae_encoder_loss": 0.009384594733826817, "actor_loss": -2.3566673765182493, "actor_target_entropy": -6.0, "actor_entropy": 4.584088829040527, "alpha_loss": 0.0003669438073702622, "alpha_value": 0.0004600338298215993, "duration": 44.26632857322693, "step": 35001}
{"episode_reward": 0.0, "episode": 142.0, "batch_reward": -0.009859375, "critic_loss": 0.7260806015431881, "ae_transition_loss": 0.0035283647174946964, "ae_encoder_loss": 0.010132370108738542, "actor_loss": -2.3735156755447386, "actor_target_entropy": -6.0, "actor_entropy": 4.805780706405639, "alpha_loss": 0.0002354479317436926, "alpha_value": 0.00043561288938316205, "duration": 15.336264610290527, "step": 35251}
{"episode_reward": 0.0, "episode": 143.0, "batch_reward": -0.010046875, "critic_loss": 0.739230690062046, "ae_transition_loss": 0.0038871873465832324, "ae_encoder_loss": 0.009965144589543343, "actor_loss": -2.397760232925415, "actor_target_entropy": -6.0, "actor_entropy": 4.387607870101928, "alpha_loss": 0.00011189551922143437, "alpha_value": 0.00042390228092629897, "duration": 15.293647766113281, "step": 35501}
{"episode_reward": 0.0, "episode": 144.0, "batch_reward": -0.0097265625, "critic_loss": 0.7202826745510101, "ae_transition_loss": 0.0037871158188208937, "ae_encoder_loss": 0.010046026206575335, "actor_loss": -2.4050935916900635, "actor_target_entropy": -6.0, "actor_entropy": 4.799469421386719, "alpha_loss": 3.755392201128416e-05, "alpha_value": 0.00043223724729295004, "duration": 15.298921585083008, "step": 35751}
{"episode_reward": 0.0, "episode": 145.0, "batch_reward": -0.0103671875, "critic_loss": 0.6741208561360836, "ae_transition_loss": 0.0033584427472087555, "ae_encoder_loss": 0.00990100165642798, "actor_loss": -2.4283630352020262, "actor_target_entropy": -6.0, "actor_entropy": 3.780707944869995, "alpha_loss": 0.0005936491431493778, "alpha_value": 0.0004013062050272009, "duration": 15.320905923843384, "step": 36001}
{"episode_reward": 0.0, "episode": 146.0, "batch_reward": -0.0083203125, "critic_loss": 0.6945312905907631, "ae_transition_loss": 0.0046990014535840605, "ae_encoder_loss": 0.009815961953252555, "actor_loss": -2.4533434009552, "actor_target_entropy": -6.0, "actor_entropy": 3.8343259162902834, "alpha_loss": -1.6937899534241298e-05, "alpha_value": 0.0003837205568454946, "duration": 15.287999629974365, "step": 36251}
{"episode_reward": 0.0, "episode": 147.0, "batch_reward": -0.010125, "critic_loss": 0.7665142412781716, "ae_transition_loss": 0.006104014271404595, "ae_encoder_loss": 0.010541680269874633, "actor_loss": -2.4981223945617677, "actor_target_entropy": -6.0, "actor_entropy": 3.795626431465149, "alpha_loss": 0.00011887364293215796, "alpha_value": 0.0003730720964150766, "duration": 15.290602922439575, "step": 36501}
{"episode_reward": 0.0, "episode": 148.0, "batch_reward": -0.0095078125, "critic_loss": 2.073908310294151, "ae_transition_loss": 0.028441906568827106, "ae_encoder_loss": 0.011639151041395962, "actor_loss": -2.5584499034881594, "actor_target_entropy": -6.0, "actor_entropy": 2.706693394780159, "alpha_loss": -0.005194478566729231, "alpha_value": 0.0004418421188140817, "duration": 15.289785146713257, "step": 36751}
{"episode_reward": -1.0, "episode": 149.0, "batch_reward": -0.00990625, "critic_loss": 0.8950566124320031, "ae_transition_loss": 0.0020305053319316355, "ae_encoder_loss": 0.009941058236174286, "actor_loss": -3.3249972953796387, "actor_target_entropy": -6.0, "actor_entropy": 2.1269277967512608, "alpha_loss": -0.0015300900614238343, "alpha_value": 0.0005483908613194909, "duration": 15.287303447723389, "step": 37001}
{"episode_reward": 0.0, "episode": 150.0, "batch_reward": -0.0088984375, "critic_loss": 1.0731155040562153, "ae_transition_loss": 0.014640356191899627, "ae_encoder_loss": 0.011375827284529806, "actor_loss": -3.2209198513031008, "actor_target_entropy": -6.0, "actor_entropy": 4.276630887985229, "alpha_loss": -0.0006901909247389995, "alpha_value": 0.0005676651226703092, "duration": 15.304090738296509, "step": 37251}
{"episode_reward": -1.0, "episode": 151.0, "batch_reward": -0.0098125, "critic_loss": 2.542465253829956, "ae_transition_loss": 0.006154337569838389, "ae_encoder_loss": 0.011794385234825313, "actor_loss": -4.359508676528931, "actor_target_entropy": -6.0, "actor_entropy": 5.93187343120575, "alpha_loss": -0.0032905479706532787, "alpha_value": 0.0006430944415522315, "duration": 15.283335447311401, "step": 37501}
{"episode_reward": -1.0, "episode": 152.0, "batch_reward": -0.0093984375, "critic_loss": 3.542178516507149, "ae_transition_loss": 0.06314282786357217, "ae_encoder_loss": 0.013235585168004035, "actor_loss": -5.3615627822875975, "actor_target_entropy": -6.0, "actor_entropy": 4.687052965536713, "alpha_loss": -0.0036674458257621155, "alpha_value": 0.0007346824969706299, "duration": 15.323686361312866, "step": 37751}
{"episode_reward": 0.0, "episode": 153.0, "batch_reward": -0.009328125, "critic_loss": 3.5080410302877425, "ae_transition_loss": 0.08541426908690482, "ae_encoder_loss": 0.01393046149238944, "actor_loss": -6.247809364318847, "actor_target_entropy": -6.0, "actor_entropy": 4.252891993880272, "alpha_loss": -0.003746827904426027, "alpha_value": 0.0008324835364445165, "duration": 15.308237791061401, "step": 38001}
{"episode_reward": 0.0, "episode": 154.0, "batch_reward": -0.0096796875, "critic_loss": 8.121382532596588, "ae_transition_loss": 0.012782158955931664, "ae_encoder_loss": 0.01103883245959878, "actor_loss": -7.188346996307373, "actor_target_entropy": -6.0, "actor_entropy": 5.079778924942016, "alpha_loss": -0.006082144725369289, "alpha_value": 0.001012722402711229, "duration": 15.293626546859741, "step": 38251}
{"episode_reward": 0.0, "episode": 155.0, "batch_reward": -0.009640625, "critic_loss": 3.401762317419052, "ae_transition_loss": 0.002616718602133915, "ae_encoder_loss": 0.010736552838236093, "actor_loss": -6.2044839134216305, "actor_target_entropy": -6.0, "actor_entropy": 5.428756608963012, "alpha_loss": -0.0006106489572557621, "alpha_value": 0.0011086963149914088, "duration": 15.291285276412964, "step": 38501}
{"episode_reward": 0.0, "episode": 156.0, "batch_reward": -0.0092890625, "critic_loss": 3.4217706100344656, "ae_transition_loss": 0.0032453290246194227, "ae_encoder_loss": 0.010928441504016518, "actor_loss": -6.207120170593262, "actor_target_entropy": -6.0, "actor_entropy": 6.286259552001953, "alpha_loss": 0.0005835664219921454, "alpha_value": 0.0011146131072824164, "duration": 15.316022634506226, "step": 38751}
{"episode_reward": 0.0, "episode": 157.0, "batch_reward": -0.0099375, "critic_loss": 2.7740474811792373, "ae_transition_loss": 0.003919546637218446, "ae_encoder_loss": 0.011050122338347138, "actor_loss": -6.156740013122558, "actor_target_entropy": -6.0, "actor_entropy": 6.187746223449707, "alpha_loss": 0.000896820276597282, "alpha_value": 0.0010859683601677657, "duration": 15.295565605163574, "step": 39001}
{"episode_reward": 0.0, "episode": 158.0, "batch_reward": -0.008921875, "critic_loss": 2.4673328644633292, "ae_transition_loss": 0.003817748641129583, "ae_encoder_loss": 0.010023465808480979, "actor_loss": -6.113883152008056, "actor_target_entropy": -6.0, "actor_entropy": 5.230308950424194, "alpha_loss": 0.00015782670385669918, "alpha_value": 0.0010695244638268167, "duration": 15.288931846618652, "step": 39251}
{"episode_reward": 0.0, "episode": 159.0, "batch_reward": -0.0093671875, "critic_loss": 2.240771468222141, "ae_transition_loss": 0.004068313260213472, "ae_encoder_loss": 0.010718607021495699, "actor_loss": -6.290058822631836, "actor_target_entropy": -6.0, "actor_entropy": 5.040499797821045, "alpha_loss": -0.001022326464211801, "alpha_value": 0.0010917974182222106, "duration": 15.300869226455688, "step": 39501}
{"episode_reward": 0.0, "episode": 160.0, "batch_reward": -0.00871875, "critic_loss": 2.249712116420269, "ae_transition_loss": 0.003940719971084036, "ae_encoder_loss": 0.010389487921260297, "actor_loss": -6.3864082336425785, "actor_target_entropy": -6.0, "actor_entropy": 5.763720825195312, "alpha_loss": 0.0006233903009560891, "alpha_value": 0.0010882408438248165, "duration": 15.309564113616943, "step": 39751}
{"episode_reward": 0.0, "episode": 161.0, "batch_reward": -0.0096328125, "critic_loss": 2.2007126591801645, "ae_transition_loss": 0.004041622158139944, "ae_encoder_loss": 0.010638962345197798, "actor_loss": -6.501608303070069, "actor_target_entropy": -6.0, "actor_entropy": 5.6035228385925295, "alpha_loss": -0.00026218164121382867, "alpha_value": 0.0010814595646070992, "duration": 45.639015674591064, "step": 40001}
{"episode_reward": 0.0, "episode": 162.0, "batch_reward": -0.008546875, "critic_loss": 2.3802928194999695, "ae_transition_loss": 0.008588878384791315, "ae_encoder_loss": 0.011191572635434568, "actor_loss": -6.63458642578125, "actor_target_entropy": -6.0, "actor_entropy": 5.087189315795898, "alpha_loss": -0.001492555238190107, "alpha_value": 0.0011108287278455914, "duration": 15.361441135406494, "step": 40251}
{"episode_reward": 0.0, "episode": 163.0, "batch_reward": -0.009109375, "critic_loss": 2.3943057607412337, "ae_transition_loss": 0.010263339466648176, "ae_encoder_loss": 0.011325607932172715, "actor_loss": -6.815511493682862, "actor_target_entropy": -6.0, "actor_entropy": 5.240677555084228, "alpha_loss": -0.0023701446175109597, "alpha_value": 0.0012012751138796827, "duration": 15.278020143508911, "step": 40501}
{"episode_reward": 0.0, "episode": 164.0, "batch_reward": -0.0083359375, "critic_loss": 2.48504862177372, "ae_transition_loss": 0.008741880326997489, "ae_encoder_loss": 0.01103132096491754, "actor_loss": -6.821686214447022, "actor_target_entropy": -6.0, "actor_entropy": 5.319771869659424, "alpha_loss": -0.0008599236367153935, "alpha_value": 0.0013039129070198108, "duration": 15.280296802520752, "step": 40751}
{"episode_reward": 0.0, "episode": 165.0, "batch_reward": -0.009484375, "critic_loss": 2.411294979453087, "ae_transition_loss": 0.004019510125042871, "ae_encoder_loss": 0.010575981829315424, "actor_loss": -6.990619682312012, "actor_target_entropy": -6.0, "actor_entropy": 5.2092092399597165, "alpha_loss": -0.002017239595297724, "alpha_value": 0.00137608591600381, "duration": 15.308780670166016, "step": 41001}
{"episode_reward": 0.0, "episode": 166.0, "batch_reward": -0.0089765625, "critic_loss": 2.382610215842724, "ae_transition_loss": 0.004021960536018014, "ae_encoder_loss": 0.010282587080262601, "actor_loss": -7.1913186492919925, "actor_target_entropy": -6.0, "actor_entropy": 4.8980505065917965, "alpha_loss": -0.0023135446729138495, "alpha_value": 0.0015246578156064, "duration": 15.292446374893188, "step": 41251}
{"episode_reward": 0.0, "episode": 167.0, "batch_reward": -0.008890625, "critic_loss": 2.3601714110970495, "ae_transition_loss": 0.004986393564380706, "ae_encoder_loss": 0.01060089966468513, "actor_loss": -7.301692325592041, "actor_target_entropy": -6.0, "actor_entropy": 4.918844039916992, "alpha_loss": -0.0022023845626972616, "alpha_value": 0.0016823283004529714, "duration": 15.286612272262573, "step": 41501}
{"episode_reward": 0.0, "episode": 168.0, "batch_reward": -0.00940625, "critic_loss": 2.434293897330761, "ae_transition_loss": 0.006222067308146507, "ae_encoder_loss": 0.010948516423813998, "actor_loss": -7.407416069030762, "actor_target_entropy": -6.0, "actor_entropy": 5.141067756652832, "alpha_loss": -0.0014037223785999232, "alpha_value": 0.0018356663970796668, "duration": 15.294436693191528, "step": 41751}
{"episode_reward": 0.0, "episode": 169.0, "batch_reward": -0.008359375, "critic_loss": 3.3294759677648544, "ae_transition_loss": 0.040060460884124044, "ae_encoder_loss": 0.01432440401148051, "actor_loss": -7.810164817810058, "actor_target_entropy": -6.0, "actor_entropy": 5.515443870544433, "alpha_loss": -0.0016298646369832567, "alpha_value": 0.0019449033348323446, "duration": 15.30727767944336, "step": 42001}
{"episode_reward": -1.0, "episode": 170.0, "batch_reward": -0.008171875, "critic_loss": 4.137550871372223, "ae_transition_loss": 0.003045413132989779, "ae_encoder_loss": 0.011542735503986478, "actor_loss": -9.216978534698486, "actor_target_entropy": -6.0, "actor_entropy": 4.876693553924561, "alpha_loss": 0.002472857654327527, "alpha_value": 0.0019451831840078247, "duration": 15.28327989578247, "step": 42251}
{"episode_reward": 0.0, "episode": 171.0, "batch_reward": -0.0084453125, "critic_loss": 4.479724727988243, "ae_transition_loss": 0.0034646488341968508, "ae_encoder_loss": 0.010799511686898768, "actor_loss": -9.851062538146973, "actor_target_entropy": -6.0, "actor_entropy": 4.40753127861023, "alpha_loss": 0.0015919826158788055, "alpha_value": 0.0017777374596991782, "duration": 15.314923763275146, "step": 42501}
{"episode_reward": -3.0, "episode": 172.0, "batch_reward": -0.0081171875, "critic_loss": 4.171797922730446, "ae_transition_loss": 0.003553511775797233, "ae_encoder_loss": 0.010696493498980999, "actor_loss": -9.953680648803712, "actor_target_entropy": -6.0, "actor_entropy": 4.102650548934936, "alpha_loss": 0.001251653264509514, "alpha_value": 0.001672719070381409, "duration": 15.308127880096436, "step": 42751}
{"episode_reward": -3.0, "episode": 173.0, "batch_reward": -0.0081953125, "critic_loss": 3.753719704031944, "ae_transition_loss": 0.0035309946064371617, "ae_encoder_loss": 0.010750325360335409, "actor_loss": -10.040916259765625, "actor_target_entropy": -6.0, "actor_entropy": 4.184682991027832, "alpha_loss": -0.00035332705627661197, "alpha_value": 0.0016216852461248402, "duration": 15.285128831863403, "step": 43001}
{"episode_reward": -3.0, "episode": 174.0, "batch_reward": -0.0086640625, "critic_loss": 3.995092902302742, "ae_transition_loss": 0.0037812449992634355, "ae_encoder_loss": 0.0107074683168903, "actor_loss": -10.130308197021485, "actor_target_entropy": -6.0, "actor_entropy": 3.764794916152954, "alpha_loss": -0.00028256942745065315, "alpha_value": 0.001661201747420678, "duration": 15.289131879806519, "step": 43251}
{"episode_reward": -3.0, "episode": 175.0, "batch_reward": -0.0088359375, "critic_loss": 4.30022241973877, "ae_transition_loss": 0.003945458625443279, "ae_encoder_loss": 0.010440354541875422, "actor_loss": -10.098612213134766, "actor_target_entropy": -6.0, "actor_entropy": 3.6806828708648682, "alpha_loss": -0.0007059256479842588, "alpha_value": 0.0016960679096232726, "duration": 15.286750555038452, "step": 43501}
{"episode_reward": -3.0, "episode": 176.0, "batch_reward": -0.0081015625, "critic_loss": 4.0336169888973235, "ae_transition_loss": 0.0037164908363483845, "ae_encoder_loss": 0.010524401156231761, "actor_loss": -10.23093472290039, "actor_target_entropy": -6.0, "actor_entropy": 3.734542137145996, "alpha_loss": -0.00034517395321745424, "alpha_value": 0.0017423164199168654, "duration": 15.298451900482178, "step": 43751}
{"episode_reward": -3.0, "episode": 177.0, "batch_reward": -0.0084921875, "critic_loss": 3.7403869109153747, "ae_transition_loss": 0.0035738082968164234, "ae_encoder_loss": 0.010434922493062913, "actor_loss": -10.157415092468261, "actor_target_entropy": -6.0, "actor_entropy": 3.811224048614502, "alpha_loss": 2.5691750808618963e-05, "alpha_value": 0.0017708625427886994, "duration": 15.280641555786133, "step": 44001}
{"episode_reward": -3.0, "episode": 178.0, "batch_reward": -0.00853125, "critic_loss": 4.062908894658088, "ae_transition_loss": 0.00385024595039431, "ae_encoder_loss": 0.010694153701886535, "actor_loss": -10.211621711730958, "actor_target_entropy": -6.0, "actor_entropy": 3.7660691871643066, "alpha_loss": -0.00015675084583926946, "alpha_value": 0.0017762044551705835, "duration": 15.297465562820435, "step": 44251}
{"episode_reward": -3.0, "episode": 179.0, "batch_reward": -0.0077109375, "critic_loss": 4.539352494180203, "ae_transition_loss": 0.004043914580484852, "ae_encoder_loss": 0.010557160811498762, "actor_loss": -10.241356315612792, "actor_target_entropy": -6.0, "actor_entropy": 3.7187364749908447, "alpha_loss": -0.00035741846414748577, "alpha_value": 0.0017994858948983362, "duration": 15.296716928482056, "step": 44501}
{"episode_reward": -3.0, "episode": 180.0, "batch_reward": -0.007828125, "critic_loss": 4.07822663128376, "ae_transition_loss": 0.0036928836521692575, "ae_encoder_loss": 0.010295078324154019, "actor_loss": -10.221850395202637, "actor_target_entropy": -6.0, "actor_entropy": 3.8026471004486084, "alpha_loss": -0.00031336836866103113, "alpha_value": 0.0018368894687986847, "duration": 15.302947759628296, "step": 44751}
{"episode_reward": -3.0, "episode": 181.0, "batch_reward": -0.009046875, "critic_loss": 3.813989234209061, "ae_transition_loss": 0.003533087072893977, "ae_encoder_loss": 0.010469600447453558, "actor_loss": -10.37173698425293, "actor_target_entropy": -6.0, "actor_entropy": 3.7130418624877928, "alpha_loss": -0.00045886712172068655, "alpha_value": 0.0018900959607154794, "duration": 212.43181490898132, "step": 45001}
{"episode_reward": -3.0, "episode": 182.0, "batch_reward": -0.0087421875, "critic_loss": 4.043424254477024, "ae_transition_loss": 0.003802586699370295, "ae_encoder_loss": 0.010399297282099724, "actor_loss": -10.381680679321288, "actor_target_entropy": -6.0, "actor_entropy": 4.109030548095703, "alpha_loss": -0.00024126837804215029, "alpha_value": 0.0019287070211150702, "duration": 15.274402141571045, "step": 45251}
{"episode_reward": -3.0, "episode": 183.0, "batch_reward": -0.0080703125, "critic_loss": 4.028856232106685, "ae_transition_loss": 0.003795688146725297, "ae_encoder_loss": 0.010286758203059436, "actor_loss": -10.532868644714355, "actor_target_entropy": -6.0, "actor_entropy": 4.136560863494873, "alpha_loss": -0.00045757992257131264, "alpha_value": 0.0019787165327505587, "duration": 15.29332423210144, "step": 45501}
{"episode_reward": 0.0, "episode": 184.0, "batch_reward": -0.0079765625, "critic_loss": 4.117995034694672, "ae_transition_loss": 0.0057760919784195725, "ae_encoder_loss": 0.010618296943604947, "actor_loss": -10.470845085144044, "actor_target_entropy": -6.0, "actor_entropy": 4.458339681625366, "alpha_loss": -6.884062848985196e-05, "alpha_value": 0.002042807223165695, "duration": 15.304294347763062, "step": 45751}
{"episode_reward": 0.0, "episode": 185.0, "batch_reward": -0.008734375, "critic_loss": 4.183117465257645, "ae_transition_loss": 0.007825969995465129, "ae_encoder_loss": 0.01149860950000584, "actor_loss": -10.660721000671387, "actor_target_entropy": -6.0, "actor_entropy": 4.907579093933106, "alpha_loss": -0.0006349093846511096, "alpha_value": 0.0020978357269515573, "duration": 15.282763957977295, "step": 46001}
{"episode_reward": -1.0, "episode": 186.0, "batch_reward": -0.0088671875, "critic_loss": 4.08889283978939, "ae_transition_loss": 0.003906535569112748, "ae_encoder_loss": 0.010070969583466649, "actor_loss": -10.871392967224121, "actor_target_entropy": -6.0, "actor_entropy": 4.311805778503418, "alpha_loss": -0.00024449144199024887, "alpha_value": 0.0021433868092220116, "duration": 15.295166969299316, "step": 46251}
{"episode_reward": 0.0, "episode": 187.0, "batch_reward": -0.0085078125, "critic_loss": 4.827976599931717, "ae_transition_loss": 0.00891032377211377, "ae_encoder_loss": 0.011285250714980067, "actor_loss": -10.935491340637206, "actor_target_entropy": -6.0, "actor_entropy": 4.319164136886597, "alpha_loss": -0.0010040125054074452, "alpha_value": 0.002249474160103741, "duration": 15.310699701309204, "step": 46501}
{"episode_reward": -1.0, "episode": 188.0, "batch_reward": -0.0093046875, "critic_loss": 3.9870550179481508, "ae_transition_loss": 0.00400526595453266, "ae_encoder_loss": 0.010378523694351315, "actor_loss": -11.317040672302246, "actor_target_entropy": -6.0, "actor_entropy": 4.515870374679565, "alpha_loss": -0.0007935798073885963, "alpha_value": 0.002438023403932367, "duration": 15.301239013671875, "step": 46751}
{"episode_reward": 0.0, "episode": 189.0, "batch_reward": -0.009140625, "critic_loss": 4.012065576672554, "ae_transition_loss": 0.0032775117036653685, "ae_encoder_loss": 0.010393659070134164, "actor_loss": -11.375594367980957, "actor_target_entropy": -6.0, "actor_entropy": 4.410158023834229, "alpha_loss": -0.0003461603336618282, "alpha_value": 0.0025160961045259554, "duration": 15.324427366256714, "step": 47001}
{"episode_reward": -3.0, "episode": 190.0, "batch_reward": -0.0080859375, "critic_loss": 4.111435659766197, "ae_transition_loss": 0.00374890490132384, "ae_encoder_loss": 0.010382876364514232, "actor_loss": -11.434874313354491, "actor_target_entropy": -6.0, "actor_entropy": 4.498490562438965, "alpha_loss": 1.4927642303518952e-05, "alpha_value": 0.002537112590066974, "duration": 15.299877643585205, "step": 47251}
{"episode_reward": 0.0, "episode": 191.0, "batch_reward": -0.008515625, "critic_loss": 6.858690529227257, "ae_transition_loss": 0.03678654727176763, "ae_encoder_loss": 0.01364855275861919, "actor_loss": -11.815514717102051, "actor_target_entropy": -6.0, "actor_entropy": 4.296452802658081, "alpha_loss": -0.0008435723282746039, "alpha_value": 0.002567672207753583, "duration": 15.425246477127075, "step": 47501}
{"episode_reward": 0.0, "episode": 192.0, "batch_reward": -0.0082890625, "critic_loss": 4.967611105442047, "ae_transition_loss": 0.0025932995043694974, "ae_encoder_loss": 0.010773536921478808, "actor_loss": -12.190953826904297, "actor_target_entropy": -6.0, "actor_entropy": 4.6862804985046385, "alpha_loss": 1.186502689961344e-05, "alpha_value": 0.0026105571176165145, "duration": 15.292176246643066, "step": 47751}
{"episode_reward": -3.0, "episode": 193.0, "batch_reward": -0.0083828125, "critic_loss": 4.458351530313492, "ae_transition_loss": 0.0026977387544466183, "ae_encoder_loss": 0.01073554022796452, "actor_loss": -12.169782615661621, "actor_target_entropy": -6.0, "actor_entropy": 4.352052499771118, "alpha_loss": 0.0008340920154005289, "alpha_value": 0.002744739075955066, "duration": 15.279715061187744, "step": 48001}
{"episode_reward": -3.0, "episode": 194.0, "batch_reward": -0.00871875, "critic_loss": 4.670158945441246, "ae_transition_loss": 0.002865341382450424, "ae_encoder_loss": 0.010522008333355188, "actor_loss": -12.518746192932129, "actor_target_entropy": -6.0, "actor_entropy": 4.612209836959839, "alpha_loss": 0.0012350641072844154, "alpha_value": 0.002480274084955462, "duration": 15.292880535125732, "step": 48251}
{"episode_reward": -3.0, "episode": 195.0, "batch_reward": -0.0086953125, "critic_loss": 4.91668278336525, "ae_transition_loss": 0.0031911811337340624, "ae_encoder_loss": 0.010547819865867496, "actor_loss": -12.422082244873048, "actor_target_entropy": -6.0, "actor_entropy": 4.532440305709839, "alpha_loss": 0.0022532784526702017, "alpha_value": 0.002301116383473405, "duration": 15.287994384765625, "step": 48501}
{"episode_reward": -3.0, "episode": 196.0, "batch_reward": -0.008, "critic_loss": 4.6463379294872285, "ae_transition_loss": 0.00332839668658562, "ae_encoder_loss": 0.010191039356403054, "actor_loss": -12.555006889343261, "actor_target_entropy": -6.0, "actor_entropy": 4.225712827682495, "alpha_loss": 0.0011310446946299635, "alpha_value": 0.002106777668861798, "duration": 15.294821739196777, "step": 48751}
{"episode_reward": -3.0, "episode": 197.0, "batch_reward": -0.008015625, "critic_loss": 4.39677652478218, "ae_transition_loss": 0.0032840011299122124, "ae_encoder_loss": 0.010347232996486128, "actor_loss": -12.618603950500487, "actor_target_entropy": -6.0, "actor_entropy": 3.5832429304122924, "alpha_loss": 0.0006449926177738235, "alpha_value": 0.00200644206449131, "duration": 15.300204277038574, "step": 49001}
{"episode_reward": -3.0, "episode": 198.0, "batch_reward": -0.008796875, "critic_loss": 4.492217163085938, "ae_transition_loss": 0.003457949605770409, "ae_encoder_loss": 0.010366528394632042, "actor_loss": -12.54606990814209, "actor_target_entropy": -6.0, "actor_entropy": 3.79787406539917, "alpha_loss": 0.00016009361925534905, "alpha_value": 0.0019594375949206055, "duration": 15.305178880691528, "step": 49251}
{"episode_reward": -3.0, "episode": 199.0, "batch_reward": -0.00796875, "critic_loss": 4.389121584534645, "ae_transition_loss": 0.003608909906761255, "ae_encoder_loss": 0.01036612879578024, "actor_loss": -12.647128829956054, "actor_target_entropy": -6.0, "actor_entropy": 3.7924025325775146, "alpha_loss": -0.0006810848219320178, "alpha_value": 0.0019844664038352745, "duration": 15.300833225250244, "step": 49501}
{"episode_reward": -3.0, "episode": 200.0, "batch_reward": -0.008734375, "critic_loss": 4.503701327443123, "ae_transition_loss": 0.003593730213469826, "ae_encoder_loss": 0.010182455323636532, "actor_loss": -12.597784416198731, "actor_target_entropy": -6.0, "actor_entropy": 4.0042847118377685, "alpha_loss": -0.0005084864753880538, "alpha_value": 0.002027101898671394, "duration": 15.291113376617432, "step": 49751}
{"episode_reward": -3.0, "episode": 201.0, "batch_reward": -0.0081484375, "critic_loss": 4.255117625117302, "ae_transition_loss": 0.003474588707787916, "ae_encoder_loss": 0.010503404418937863, "actor_loss": -12.589579139709473, "actor_target_entropy": -6.0, "actor_entropy": 4.016034683227539, "alpha_loss": -0.001172175585408695, "alpha_value": 0.002173283485081603, "duration": 213.66694903373718, "step": 50001}
{"episode_reward": -3.0, "episode": 202.0, "batch_reward": -0.0088671875, "critic_loss": 4.500723151803017, "ae_transition_loss": 0.003706025338266045, "ae_encoder_loss": 0.010482356653548778, "actor_loss": -12.628202713012696, "actor_target_entropy": -6.0, "actor_entropy": 3.836650457382202, "alpha_loss": -0.0010077303485595622, "alpha_value": 0.0023200485058184864, "duration": 15.316126108169556, "step": 50251}
{"episode_reward": 0.0, "episode": 203.0, "batch_reward": -0.0087265625, "critic_loss": 4.184806245684624, "ae_transition_loss": 0.003517112071393058, "ae_encoder_loss": 0.010510584981180727, "actor_loss": -12.724665336608886, "actor_target_entropy": -6.0, "actor_entropy": 4.130185338973999, "alpha_loss": -0.0005790834677754901, "alpha_value": 0.0024725535058259768, "duration": 15.273803234100342, "step": 50501}
{"episode_reward": -3.0, "episode": 204.0, "batch_reward": -0.0089453125, "critic_loss": 4.0947602088451385, "ae_transition_loss": 0.003599159484845586, "ae_encoder_loss": 0.010629191491752864, "actor_loss": -12.698587356567383, "actor_target_entropy": -6.0, "actor_entropy": 4.282558971405029, "alpha_loss": -0.00017778403725242242, "alpha_value": 0.0025101680021954328, "duration": 15.283285140991211, "step": 50751}
{"episode_reward": -2.0, "episode": 205.0, "batch_reward": -0.00765625, "critic_loss": 4.6838350628614425, "ae_transition_loss": 0.003886951779248193, "ae_encoder_loss": 0.010203003991395235, "actor_loss": -12.817823211669921, "actor_target_entropy": -6.0, "actor_entropy": 4.239351793289185, "alpha_loss": 0.00025384006986860187, "alpha_value": 0.0024620209532423116, "duration": 15.285999536514282, "step": 51001}
{"episode_reward": -3.0, "episode": 206.0, "batch_reward": -0.0078828125, "critic_loss": 4.1766800484657285, "ae_transition_loss": 0.00355233607662376, "ae_encoder_loss": 0.010332568247336894, "actor_loss": -12.786361000061035, "actor_target_entropy": -6.0, "actor_entropy": 4.242032253265381, "alpha_loss": -0.0004003886773716658, "alpha_value": 0.0025319668843204984, "duration": 15.316852807998657, "step": 51251}
{"episode_reward": 0.0, "episode": 207.0, "batch_reward": -0.0084765625, "critic_loss": 3.972567848443985, "ae_transition_loss": 0.0036626222503837198, "ae_encoder_loss": 0.010459531559608876, "actor_loss": -12.830051666259765, "actor_target_entropy": -6.0, "actor_entropy": 4.204886987686157, "alpha_loss": 0.00047736586164683103, "alpha_value": 0.0025224555239360034, "duration": 15.310357332229614, "step": 51501}
{"episode_reward": -3.0, "episode": 208.0, "batch_reward": -0.0084453125, "critic_loss": 4.274093275427818, "ae_transition_loss": 0.003833563030348159, "ae_encoder_loss": 0.010378191275522112, "actor_loss": -12.848951301574708, "actor_target_entropy": -6.0, "actor_entropy": 4.363406187057495, "alpha_loss": -0.00013800580910174176, "alpha_value": 0.0024477701156598976, "duration": 15.28585958480835, "step": 51751}
{"episode_reward": -3.0, "episode": 209.0, "batch_reward": -0.008625, "critic_loss": 4.914730577111245, "ae_transition_loss": 0.004880577820586041, "ae_encoder_loss": 0.010826183384284377, "actor_loss": -12.905428329467773, "actor_target_entropy": -6.0, "actor_entropy": 4.392596164703369, "alpha_loss": -0.0006121700523653999, "alpha_value": 0.0025400057211096426, "duration": 15.28934121131897, "step": 52001}
{"episode_reward": -3.0, "episode": 210.0, "batch_reward": -0.0081640625, "critic_loss": 3.9692043046951295, "ae_transition_loss": 0.00370376835973002, "ae_encoder_loss": 0.01041288632247597, "actor_loss": -12.760867729187012, "actor_target_entropy": -6.0, "actor_entropy": 4.122228885650634, "alpha_loss": 0.000583578796708025, "alpha_value": 0.002563161458689447, "duration": 15.29202914237976, "step": 52251}
{"episode_reward": -3.0, "episode": 211.0, "batch_reward": -0.00890625, "critic_loss": 4.139187180638313, "ae_transition_loss": 0.0035781556583242492, "ae_encoder_loss": 0.010266327408142388, "actor_loss": -12.58261695098877, "actor_target_entropy": -6.0, "actor_entropy": 4.15003482055664, "alpha_loss": 0.0003295519666862674, "alpha_value": 0.002441329221442886, "duration": 15.293229103088379, "step": 52501}
{"episode_reward": -3.0, "episode": 212.0, "batch_reward": -0.008390625, "critic_loss": 3.8565681201219557, "ae_transition_loss": 0.003475674912566319, "ae_encoder_loss": 0.010369331397116184, "actor_loss": -12.587747215270996, "actor_target_entropy": -6.0, "actor_entropy": 4.319024280548096, "alpha_loss": -0.0002828805371536873, "alpha_value": 0.0024839941578929377, "duration": 15.285274505615234, "step": 52751}
{"episode_reward": -3.0, "episode": 213.0, "batch_reward": -0.008375, "critic_loss": 3.89453593313694, "ae_transition_loss": 0.0036906739349942654, "ae_encoder_loss": 0.01080411571636796, "actor_loss": -12.638815559387208, "actor_target_entropy": -6.0, "actor_entropy": 4.152006652832031, "alpha_loss": -0.0005229388299048878, "alpha_value": 0.0025330539262655304, "duration": 15.300119638442993, "step": 53001}
{"episode_reward": -3.0, "episode": 214.0, "batch_reward": -0.0086171875, "critic_loss": 3.9685883662700654, "ae_transition_loss": 0.0038630946069024502, "ae_encoder_loss": 0.01055480556935072, "actor_loss": -12.612190628051758, "actor_target_entropy": -6.0, "actor_entropy": 4.249799015045166, "alpha_loss": -0.0006276108366437257, "alpha_value": 0.0026514436439803567, "duration": 15.290355443954468, "step": 53251}
{"episode_reward": -1.0, "episode": 215.0, "batch_reward": -0.0083046875, "critic_loss": 3.4651737213134766, "ae_transition_loss": 0.0032980241572950035, "ae_encoder_loss": 0.010498074001632631, "actor_loss": -12.702134384155274, "actor_target_entropy": -6.0, "actor_entropy": 3.7080676136016844, "alpha_loss": -0.0017274044233490714, "alpha_value": 0.0029121060679341386, "duration": 15.286569356918335, "step": 53501}
{"episode_reward": 0.0, "episode": 216.0, "batch_reward": -0.00909375, "critic_loss": 3.4394276992678643, "ae_transition_loss": 0.0032086404431611298, "ae_encoder_loss": 0.010297091921791434, "actor_loss": -12.794691497802734, "actor_target_entropy": -6.0, "actor_entropy": 3.934898389816284, "alpha_loss": 0.0011546333865262567, "alpha_value": 0.0030806628440511784, "duration": 15.304188013076782, "step": 53751}
{"episode_reward": 0.0, "episode": 217.0, "batch_reward": -0.0077890625, "critic_loss": 3.297320971548557, "ae_transition_loss": 0.003293614329188131, "ae_encoder_loss": 0.010254557682201267, "actor_loss": -12.833568466186524, "actor_target_entropy": -6.0, "actor_entropy": 4.00318828201294, "alpha_loss": 0.001121574099175632, "alpha_value": 0.002777565885788594, "duration": 15.30372929573059, "step": 54001}
{"episode_reward": -3.0, "episode": 218.0, "batch_reward": -0.00775, "critic_loss": 3.3056463054418566, "ae_transition_loss": 0.0033187312048394233, "ae_encoder_loss": 0.010129236897453665, "actor_loss": -12.800639114379884, "actor_target_entropy": -6.0, "actor_entropy": 3.916300220489502, "alpha_loss": -3.6964245839044455e-05, "alpha_value": 0.0026412161711523306, "duration": 15.277511358261108, "step": 54251}
{"episode_reward": 0.0, "episode": 219.0, "batch_reward": -0.00796875, "critic_loss": 3.4440815898776056, "ae_transition_loss": 0.0035630275774747133, "ae_encoder_loss": 0.010691739574074745, "actor_loss": -12.881749298095704, "actor_target_entropy": -6.0, "actor_entropy": 4.007658842086792, "alpha_loss": -4.5804391615092755e-05, "alpha_value": 0.002672899690152716, "duration": 15.288749933242798, "step": 54501}
{"episode_reward": -3.0, "episode": 220.0, "batch_reward": -0.0081640625, "critic_loss": 3.340247098267078, "ae_transition_loss": 0.0036965225317981093, "ae_encoder_loss": 0.010315561693161727, "actor_loss": -12.82056908416748, "actor_target_entropy": -6.0, "actor_entropy": 4.069314659118652, "alpha_loss": 2.535807905951515e-05, "alpha_value": 0.0026649736461161954, "duration": 15.288005828857422, "step": 54751}
{"episode_reward": -3.0, "episode": 221.0, "batch_reward": -0.0078984375, "critic_loss": 3.26538702917099, "ae_transition_loss": 0.0036926466266158967, "ae_encoder_loss": 0.010842089620418847, "actor_loss": -12.894941680908204, "actor_target_entropy": -6.0, "actor_entropy": 4.179434293746948, "alpha_loss": 5.004915245808661e-05, "alpha_value": 0.002660006735012303, "duration": 212.24154949188232, "step": 55001}
{"episode_reward": -3.0, "episode": 222.0, "batch_reward": -0.009078125, "critic_loss": 3.25967256039381, "ae_transition_loss": 0.003803617542842403, "ae_encoder_loss": 0.010698996325954795, "actor_loss": -12.916898475646972, "actor_target_entropy": -6.0, "actor_entropy": 4.215322696685791, "alpha_loss": -0.000587245091679506, "alpha_value": 0.002682988047988171, "duration": 15.310867309570312, "step": 55251}
{"episode_reward": -3.0, "episode": 223.0, "batch_reward": -0.00825, "critic_loss": 3.130715106844902, "ae_transition_loss": 0.003884825197281316, "ae_encoder_loss": 0.01057966430298984, "actor_loss": -12.89247836303711, "actor_target_entropy": -6.0, "actor_entropy": 4.226966688156128, "alpha_loss": -0.00014980937517248093, "alpha_value": 0.002788988934023936, "duration": 15.325460433959961, "step": 55501}
{"episode_reward": -3.0, "episode": 224.0, "batch_reward": -0.0080859375, "critic_loss": 3.203519756436348, "ae_transition_loss": 0.004086137494537979, "ae_encoder_loss": 0.010445892409421504, "actor_loss": -12.899766624450683, "actor_target_entropy": -6.0, "actor_entropy": 4.493472702026367, "alpha_loss": 0.00017612093046773226, "alpha_value": 0.0028320285533389856, "duration": 15.27877402305603, "step": 55751}
{"episode_reward": -3.0, "episode": 225.0, "batch_reward": -0.0085234375, "critic_loss": 2.899417788207531, "ae_transition_loss": 0.003995390619384125, "ae_encoder_loss": 0.010742798684164882, "actor_loss": -12.866394203186035, "actor_target_entropy": -6.0, "actor_entropy": 4.405331470489502, "alpha_loss": -2.4963247531559318e-05, "alpha_value": 0.0028043559615376027, "duration": 15.291984796524048, "step": 56001}
{"episode_reward": -3.0, "episode": 226.0, "batch_reward": -0.0085, "critic_loss": 3.0726032654047013, "ae_transition_loss": 0.004340215555857867, "ae_encoder_loss": 0.01088358718715608, "actor_loss": -12.857757209777832, "actor_target_entropy": -6.0, "actor_entropy": 4.407399276733399, "alpha_loss": -0.0002786027303081937, "alpha_value": 0.002789260475762511, "duration": 15.315680742263794, "step": 56251}
{"episode_reward": -3.0, "episode": 227.0, "batch_reward": -0.0083984375, "critic_loss": 2.8804435613155364, "ae_transition_loss": 0.004149669184815139, "ae_encoder_loss": 0.010525869105011226, "actor_loss": -12.925336700439454, "actor_target_entropy": -6.0, "actor_entropy": 4.532428812026978, "alpha_loss": -0.0007368792827473953, "alpha_value": 0.0029515524639134507, "duration": 15.318938970565796, "step": 56501}
{"episode_reward": -3.0, "episode": 228.0, "batch_reward": -0.0084453125, "critic_loss": 2.74764941740036, "ae_transition_loss": 0.004483060927363112, "ae_encoder_loss": 0.010608548354357481, "actor_loss": -12.918146705627441, "actor_target_entropy": -6.0, "actor_entropy": 4.538262237548828, "alpha_loss": 1.8156349542550744e-05, "alpha_value": 0.003056681531368805, "duration": 15.319504499435425, "step": 56751}
{"episode_reward": -3.0, "episode": 229.0, "batch_reward": -0.008765625, "critic_loss": 3.0765237364172937, "ae_transition_loss": 0.005406456792261451, "ae_encoder_loss": 0.011225832006894053, "actor_loss": -12.942204513549804, "actor_target_entropy": -6.0, "actor_entropy": 4.597745811462402, "alpha_loss": 0.0006139194443821908, "alpha_value": 0.00302297692416218, "duration": 15.346576452255249, "step": 57001}
{"episode_reward": -2.0, "episode": 230.0, "batch_reward": -0.008390625, "critic_loss": 3.0000867753624916, "ae_transition_loss": 0.005371849157847464, "ae_encoder_loss": 0.010693023637868464, "actor_loss": -13.042058708190918, "actor_target_entropy": -6.0, "actor_entropy": 4.53466978263855, "alpha_loss": 0.0009032706287689507, "alpha_value": 0.0028014479979058767, "duration": 15.339349746704102, "step": 57251}
{"episode_reward": 0.0, "episode": 231.0, "batch_reward": -0.0082890625, "critic_loss": 2.533004944562912, "ae_transition_loss": 0.004655081349890679, "ae_encoder_loss": 0.010388250740244985, "actor_loss": -13.19187712097168, "actor_target_entropy": -6.0, "actor_entropy": 4.055560110092163, "alpha_loss": -0.00015265318437013775, "alpha_value": 0.0026222728275392074, "duration": 15.333943367004395, "step": 57501}
{"episode_reward": 0.0, "episode": 232.0, "batch_reward": -0.0081328125, "critic_loss": 2.6801116167902945, "ae_transition_loss": 0.005149859051220119, "ae_encoder_loss": 0.010406138057820499, "actor_loss": -13.267825202941895, "actor_target_entropy": -6.0, "actor_entropy": 4.116269607543945, "alpha_loss": 8.551623008679598e-05, "alpha_value": 0.002623641809078648, "duration": 15.356146097183228, "step": 57751}
{"episode_reward": -1.0, "episode": 233.0, "batch_reward": -0.008375, "critic_loss": 4.175321806550026, "ae_transition_loss": 0.022940606831572948, "ae_encoder_loss": 0.013938564620912076, "actor_loss": -13.224312614440917, "actor_target_entropy": -6.0, "actor_entropy": 4.697846256256104, "alpha_loss": -0.0015636145470198245, "alpha_value": 0.0027364631255260077, "duration": 15.35770559310913, "step": 58001}
{"episode_reward": -3.0, "episode": 234.0, "batch_reward": -0.0079609375, "critic_loss": 6.984768023967743, "ae_transition_loss": 0.03123923109145835, "ae_encoder_loss": 0.012899645818397403, "actor_loss": -14.652169143676758, "actor_target_entropy": -6.0, "actor_entropy": 4.178378131866455, "alpha_loss": -0.01109526556590572, "alpha_value": 0.003671495589403458, "duration": 15.366525411605835, "step": 58251}
{"episode_reward": -3.0, "episode": 235.0, "batch_reward": -0.0082578125, "critic_loss": 5.36047577881813, "ae_transition_loss": 0.0026097732209600507, "ae_encoder_loss": 0.010978991023264825, "actor_loss": -16.44469827270508, "actor_target_entropy": -6.0, "actor_entropy": 3.9820490188598634, "alpha_loss": -0.006234339216258377, "alpha_value": 0.004516713896821281, "duration": 15.350068092346191, "step": 58501}
{"episode_reward": -3.0, "episode": 236.0, "batch_reward": -0.008671875, "critic_loss": 5.407754451751709, "ae_transition_loss": 0.0032461745422333477, "ae_encoder_loss": 0.01147308619460091, "actor_loss": -16.732693008422853, "actor_target_entropy": -6.0, "actor_entropy": 4.076417638778686, "alpha_loss": -0.011680016987025739, "alpha_value": 0.005379982655898733, "duration": 15.344744205474854, "step": 58751}
{"episode_reward": -3.0, "episode": 237.0, "batch_reward": -0.008828125, "critic_loss": 5.270519860506058, "ae_transition_loss": 0.003590129993390292, "ae_encoder_loss": 0.011726203326135875, "actor_loss": -17.007201393127442, "actor_target_entropy": -6.0, "actor_entropy": 4.411102319717407, "alpha_loss": -0.0077735724044032395, "alpha_value": 0.0063984250136495825, "duration": 15.349362850189209, "step": 59001}
{"episode_reward": -3.0, "episode": 238.0, "batch_reward": -0.0080625, "critic_loss": 5.365852187633514, "ae_transition_loss": 0.0037946197350975128, "ae_encoder_loss": 0.010726904975250364, "actor_loss": -17.02158567047119, "actor_target_entropy": -6.0, "actor_entropy": 4.5415311012268065, "alpha_loss": -0.0049681201013736425, "alpha_value": 0.007279315708079058, "duration": 15.354740381240845, "step": 59251}
{"episode_reward": -3.0, "episode": 239.0, "batch_reward": -0.0078125, "critic_loss": 5.903996399641037, "ae_transition_loss": 0.013488150698598475, "ae_encoder_loss": 0.012479773679748177, "actor_loss": -17.043654525756835, "actor_target_entropy": -6.0, "actor_entropy": 5.013055274963379, "alpha_loss": 0.006328214194742031, "alpha_value": 0.007426395675924084, "duration": 15.376607894897461, "step": 59501}
{"episode_reward": -3.0, "episode": 240.0, "batch_reward": -0.008546875, "critic_loss": 13.388968546390533, "ae_transition_loss": 0.030984075861051678, "ae_encoder_loss": 0.017332401154562832, "actor_loss": -17.709441932678224, "actor_target_entropy": -6.0, "actor_entropy": 5.745717786788941, "alpha_loss": 0.009275070681236685, "alpha_value": 0.006498958457662113, "duration": 15.343969821929932, "step": 59751}
{"episode_reward": 0.0, "episode": 241.0, "batch_reward": -0.0084140625, "critic_loss": 15.868378002166748, "ae_transition_loss": 0.03100298597384244, "ae_encoder_loss": 0.0159666102360934, "actor_loss": -18.73090754699707, "actor_target_entropy": -6.0, "actor_entropy": 4.88719455909729, "alpha_loss": -0.00012577107455581427, "alpha_value": 0.006136584644109386, "duration": 46.06023859977722, "step": 60001}
{"episode_reward": 0.0, "episode": 242.0, "batch_reward": -0.007765625, "critic_loss": 13.89311562347412, "ae_transition_loss": 0.0052798618841916325, "ae_encoder_loss": 0.012181101256981491, "actor_loss": -19.05242610168457, "actor_target_entropy": -6.0, "actor_entropy": 5.347679592132568, "alpha_loss": 0.003052286458900198, "alpha_value": 0.005921034159854149, "duration": 15.42607045173645, "step": 60251}
{"episode_reward": -3.0, "episode": 243.0, "batch_reward": -0.008140625, "critic_loss": 13.330758086681366, "ae_transition_loss": 0.008000746891833842, "ae_encoder_loss": 0.01223052314016968, "actor_loss": -18.720842239379884, "actor_target_entropy": -6.0, "actor_entropy": 5.302646839141846, "alpha_loss": -0.001993163581704721, "alpha_value": 0.005758983826969362, "duration": 15.649052619934082, "step": 60501}
{"episode_reward": -2.0, "episode": 244.0, "batch_reward": -0.0091015625, "critic_loss": 14.511724217414855, "ae_transition_loss": 0.010120120817795395, "ae_encoder_loss": 0.012660679185763002, "actor_loss": -20.676172439575197, "actor_target_entropy": -6.0, "actor_entropy": 5.421566009521484, "alpha_loss": -0.003113852278096601, "alpha_value": 0.005992746020949665, "duration": 15.68879508972168, "step": 60751}
{"episode_reward": -1.0, "episode": 245.0, "batch_reward": -0.0086796875, "critic_loss": 18.00900619983673, "ae_transition_loss": 0.020868923198897393, "ae_encoder_loss": 0.01411833250336349, "actor_loss": -20.288948944091796, "actor_target_entropy": -6.0, "actor_entropy": 5.258374227523804, "alpha_loss": -0.0010207036149222403, "alpha_value": 0.006317068603853892, "duration": 15.633081197738647, "step": 61001}
{"episode_reward": -1.0, "episode": 246.0, "batch_reward": -0.008515625, "critic_loss": 24.128608104705812, "ae_transition_loss": 0.019962018185295166, "ae_encoder_loss": 0.01406835098285228, "actor_loss": -20.88108236694336, "actor_target_entropy": -6.0, "actor_entropy": 4.6323337020874025, "alpha_loss": -0.008481921182014048, "alpha_value": 0.00663387700834649, "duration": 15.669147729873657, "step": 61251}
{"episode_reward": -3.0, "episode": 247.0, "batch_reward": -0.0085859375, "critic_loss": 22.658285469055176, "ae_transition_loss": 0.0039831288959831, "ae_encoder_loss": 0.0113548783371225, "actor_loss": -25.100219467163086, "actor_target_entropy": -6.0, "actor_entropy": 4.298566244125366, "alpha_loss": -0.007841276459861548, "alpha_value": 0.007416028118455492, "duration": 15.729158639907837, "step": 61501}
{"episode_reward": -3.0, "episode": 248.0, "batch_reward": -0.008734375, "critic_loss": 20.89078464984894, "ae_transition_loss": 0.004256105317268521, "ae_encoder_loss": 0.01105252540204674, "actor_loss": -26.742311614990236, "actor_target_entropy": -6.0, "actor_entropy": 4.3593817234039305, "alpha_loss": -0.006821493496070616, "alpha_value": 0.00836363700680509, "duration": 15.627387285232544, "step": 61751}
{"episode_reward": -3.0, "episode": 249.0, "batch_reward": -0.009015625, "critic_loss": 23.09415888595581, "ae_transition_loss": 0.005557469211518764, "ae_encoder_loss": 0.011602511666715144, "actor_loss": -26.8359288482666, "actor_target_entropy": -6.0, "actor_entropy": 4.564056427001953, "alpha_loss": -0.005139879372669384, "alpha_value": 0.009008377591251329, "duration": 15.662224531173706, "step": 62001}
{"episode_reward": -3.0, "episode": 250.0, "batch_reward": -0.0087890625, "critic_loss": 19.467986005306244, "ae_transition_loss": 0.005133081332780421, "ae_encoder_loss": 0.011088270092383027, "actor_loss": -26.884770584106445, "actor_target_entropy": -6.0, "actor_entropy": 4.618989824295044, "alpha_loss": -0.0057868802533485, "alpha_value": 0.00976521395570694, "duration": 15.39784550666809, "step": 62251}
{"episode_reward": -3.0, "episode": 251.0, "batch_reward": -0.0088828125, "critic_loss": 18.12090296936035, "ae_transition_loss": 0.006140454861335456, "ae_encoder_loss": 0.011238812990486622, "actor_loss": -27.08386227416992, "actor_target_entropy": -6.0, "actor_entropy": 4.956146879196167, "alpha_loss": 0.005653110960498452, "alpha_value": 0.010064090708953618, "duration": 15.386999607086182, "step": 62501}
{"episode_reward": -2.0, "episode": 252.0, "batch_reward": -0.008671875, "critic_loss": 20.659914546012878, "ae_transition_loss": 0.006090804645325989, "ae_encoder_loss": 0.01092524504289031, "actor_loss": -27.012832412719728, "actor_target_entropy": -6.0, "actor_entropy": 5.115805727005005, "alpha_loss": 0.01756611065613106, "alpha_value": 0.008594638841805754, "duration": 15.403441429138184, "step": 62751}
{"episode_reward": 0.0, "episode": 253.0, "batch_reward": -0.008515625, "critic_loss": 18.665660284042357, "ae_transition_loss": 0.004514465799322352, "ae_encoder_loss": 0.010721695304848254, "actor_loss": -27.343428298950194, "actor_target_entropy": -6.0, "actor_entropy": 4.564037714004517, "alpha_loss": 0.011861567410174757, "alpha_value": 0.0070919365970761265, "duration": 15.395339250564575, "step": 63001}
{"episode_reward": 0.0, "episode": 254.0, "batch_reward": -0.008734375, "critic_loss": 18.775744242668154, "ae_transition_loss": 0.005249548960942775, "ae_encoder_loss": 0.011087354244664312, "actor_loss": -27.681499084472655, "actor_target_entropy": -6.0, "actor_entropy": 4.274187866210937, "alpha_loss": 0.0015718362815678119, "alpha_value": 0.006531256305703327, "duration": 15.411417007446289, "step": 63251}
{"episode_reward": 0.0, "episode": 255.0, "batch_reward": -0.0089375, "critic_loss": 19.43386541748047, "ae_transition_loss": 0.005139441956300289, "ae_encoder_loss": 0.010931712194345891, "actor_loss": -27.660062240600585, "actor_target_entropy": -6.0, "actor_entropy": 4.219846670150757, "alpha_loss": 0.0015025893221609294, "alpha_value": 0.006507919949538729, "duration": 15.411259651184082, "step": 63501}
{"episode_reward": 0.0, "episode": 256.0, "batch_reward": -0.007453125, "critic_loss": 20.138086251735686, "ae_transition_loss": 0.009782923424150795, "ae_encoder_loss": 0.01162215598858893, "actor_loss": -27.6049178314209, "actor_target_entropy": -6.0, "actor_entropy": 5.018783357620239, "alpha_loss": 0.002561694343574345, "alpha_value": 0.006204016668255235, "duration": 15.54891300201416, "step": 63751}
{"episode_reward": 0.0, "episode": 257.0, "batch_reward": -0.0080390625, "critic_loss": 16.64314141845703, "ae_transition_loss": 0.004265778467990458, "ae_encoder_loss": 0.010461420506238937, "actor_loss": -27.888320922851562, "actor_target_entropy": -6.0, "actor_entropy": 4.262539922714233, "alpha_loss": -0.003420985735487193, "alpha_value": 0.00620092593586494, "duration": 15.590436220169067, "step": 64001}
{"episode_reward": 0.0, "episode": 258.0, "batch_reward": -0.008625, "critic_loss": 17.674929606437683, "ae_transition_loss": 0.005342967183794826, "ae_encoder_loss": 0.011178098233416676, "actor_loss": -27.889731658935546, "actor_target_entropy": -6.0, "actor_entropy": 4.790753303527832, "alpha_loss": -0.0093023548014462, "alpha_value": 0.0068560723948147915, "duration": 15.472589492797852, "step": 64251}
{"episode_reward": 0.0, "episode": 259.0, "batch_reward": -0.008140625, "critic_loss": 16.4290915017128, "ae_transition_loss": 0.004955952882766724, "ae_encoder_loss": 0.010585164463147521, "actor_loss": -28.15200131225586, "actor_target_entropy": -6.0, "actor_entropy": 5.005950962066651, "alpha_loss": -0.000652401392813772, "alpha_value": 0.007375997697202691, "duration": 15.541871070861816, "step": 64501}
{"episode_reward": 0.0, "episode": 260.0, "batch_reward": -0.0086484375, "critic_loss": 15.950127558708191, "ae_transition_loss": 0.004840975568629801, "ae_encoder_loss": 0.011027475874871016, "actor_loss": -28.270413650512694, "actor_target_entropy": -6.0, "actor_entropy": 4.812023788452149, "alpha_loss": -0.007685688158730045, "alpha_value": 0.007586520766846492, "duration": 15.609307527542114, "step": 64751}
{"episode_reward": 0.0, "episode": 261.0, "batch_reward": -0.0085625, "critic_loss": 33.00306064987183, "ae_transition_loss": 0.02937629844341427, "ae_encoder_loss": 0.015126098860055209, "actor_loss": -28.267101379394532, "actor_target_entropy": -6.0, "actor_entropy": 5.073888303756714, "alpha_loss": -0.012208149980986491, "alpha_value": 0.008825909789092042, "duration": 213.05873227119446, "step": 65001}
{"episode_reward": 0.0, "episode": 262.0, "batch_reward": -0.0082734375, "critic_loss": 20.580316071510314, "ae_transition_loss": 0.006113810548558831, "ae_encoder_loss": 0.01156683311983943, "actor_loss": -28.657218215942382, "actor_target_entropy": -6.0, "actor_entropy": 4.8301687316894535, "alpha_loss": 0.00884172193100676, "alpha_value": 0.009024818004259335, "duration": 15.429670095443726, "step": 65251}
{"episode_reward": 0.0, "episode": 263.0, "batch_reward": -0.008390625, "critic_loss": 75.93728309631348, "ae_transition_loss": 0.0780449650157243, "ae_encoder_loss": 0.02025873751938343, "actor_loss": -28.03608204650879, "actor_target_entropy": -6.0, "actor_entropy": 5.332623559951783, "alpha_loss": -0.023836753210518508, "alpha_value": 0.008945808265227114, "duration": 15.546725034713745, "step": 65501}
{"episode_reward": 0.0, "episode": 264.0, "batch_reward": -0.0085546875, "critic_loss": 52.139928186416626, "ae_transition_loss": 0.042796620016917586, "ae_encoder_loss": 0.013556994150392711, "actor_loss": -31.860748046875, "actor_target_entropy": -6.0, "actor_entropy": 3.942467945098877, "alpha_loss": -0.004538359087426215, "alpha_value": 0.010285160813711414, "duration": 15.56230878829956, "step": 65751}
{"episode_reward": 0.0, "episode": 265.0, "batch_reward": -0.0082265625, "critic_loss": 31.027640244483948, "ae_transition_loss": 0.003878361803945154, "ae_encoder_loss": 0.01133347111567855, "actor_loss": -32.58575048828125, "actor_target_entropy": -6.0, "actor_entropy": 4.849491302490234, "alpha_loss": -0.039585669078864155, "alpha_value": 0.011731050212273378, "duration": 15.476354360580444, "step": 66001}
{"episode_reward": -1.0, "episode": 266.0, "batch_reward": -0.00821875, "critic_loss": 28.51416214942932, "ae_transition_loss": 0.0058016677438281474, "ae_encoder_loss": 0.011411665875464678, "actor_loss": -32.55290240478516, "actor_target_entropy": -6.0, "actor_entropy": 5.634936626434326, "alpha_loss": 0.007804478238336742, "alpha_value": 0.01267907950131625, "duration": 15.615097522735596, "step": 66251}
{"episode_reward": -1.0, "episode": 267.0, "batch_reward": -0.00784375, "critic_loss": 88.79878319358826, "ae_transition_loss": 0.02458086759969592, "ae_encoder_loss": 0.01613937424682081, "actor_loss": -37.73510832214355, "actor_target_entropy": -6.0, "actor_entropy": 5.149767343521118, "alpha_loss": -0.006653682418167591, "alpha_value": 0.01268328618900779, "duration": 15.605292797088623, "step": 66501}
{"episode_reward": 0.0, "episode": 268.0, "batch_reward": -0.007984375, "critic_loss": 85.01222433471679, "ae_transition_loss": 0.005815633567981422, "ae_encoder_loss": 0.012392401285469532, "actor_loss": -64.62221545410156, "actor_target_entropy": -6.0, "actor_entropy": 5.69155114364624, "alpha_loss": -0.005591173530556262, "alpha_value": 0.01308019728080294, "duration": 16.134833574295044, "step": 66751}
{"episode_reward": 0.0, "episode": 269.0, "batch_reward": -0.0081640625, "critic_loss": 74.31801020812988, "ae_transition_loss": 0.004999701399821788, "ae_encoder_loss": 0.011952823592349887, "actor_loss": -71.2512700805664, "actor_target_entropy": -6.0, "actor_entropy": 5.268353725433349, "alpha_loss": -0.016860397931188344, "alpha_value": 0.01373435222310134, "duration": 15.47400450706482, "step": 67001}
{"episode_reward": 0.0, "episode": 270.0, "batch_reward": -0.0077265625, "critic_loss": 69.47785178375244, "ae_transition_loss": 0.005193540536798537, "ae_encoder_loss": 0.011927167866379023, "actor_loss": -74.06189611816406, "actor_target_entropy": -6.0, "actor_entropy": 5.098841194152832, "alpha_loss": -0.027621184907853605, "alpha_value": 0.015797178714060046, "duration": 15.804972887039185, "step": 67251}
{"episode_reward": -3.0, "episode": 271.0, "batch_reward": -0.00721875, "critic_loss": 71.64371792602539, "ae_transition_loss": 0.006098908951506019, "ae_encoder_loss": 0.011347814604640008, "actor_loss": -74.48125219726562, "actor_target_entropy": -6.0, "actor_entropy": 4.790328269958496, "alpha_loss": -0.02747528952732682, "alpha_value": 0.018175373422145577, "duration": 15.601393222808838, "step": 67501}
{"episode_reward": -3.0, "episode": 272.0, "batch_reward": -0.0082265625, "critic_loss": 80.07242560577393, "ae_transition_loss": 0.010251469617709518, "ae_encoder_loss": 0.011995250734500588, "actor_loss": -74.20205249023438, "actor_target_entropy": -6.0, "actor_entropy": 4.964192043304443, "alpha_loss": -0.0048468074202537535, "alpha_value": 0.02021535641833657, "duration": 15.503357648849487, "step": 67751}
{"episode_reward": -3.0, "episode": 273.0, "batch_reward": -0.00815625, "critic_loss": 109.77625998687743, "ae_transition_loss": 0.013372492264024913, "ae_encoder_loss": 0.013467409368604421, "actor_loss": -74.61222570800781, "actor_target_entropy": -6.0, "actor_entropy": 4.9355966663360595, "alpha_loss": -0.004359741058200597, "alpha_value": 0.020195494828317898, "duration": 15.621896028518677, "step": 68001}
{"episode_reward": -3.0, "episode": 274.0, "batch_reward": -0.0074140625, "critic_loss": 116.82961772155761, "ae_transition_loss": 0.015877054909244178, "ae_encoder_loss": 0.013330917578190564, "actor_loss": -82.14504266357422, "actor_target_entropy": -6.0, "actor_entropy": 5.286105430603027, "alpha_loss": -0.009288397676311433, "alpha_value": 0.020614775051580148, "duration": 15.62759518623352, "step": 68251}
{"episode_reward": 0.0, "episode": 275.0, "batch_reward": -0.0078515625, "critic_loss": 161.9513032684326, "ae_transition_loss": 0.026037878578528764, "ae_encoder_loss": 0.015824769331142307, "actor_loss": -96.22728497314453, "actor_target_entropy": -6.0, "actor_entropy": 4.622251583099366, "alpha_loss": -0.036260165486484766, "alpha_value": 0.022620334934189787, "duration": 15.521482706069946, "step": 68501}
{"episode_reward": -3.0, "episode": 276.0, "batch_reward": -0.0076015625, "critic_loss": 118.38425074768067, "ae_transition_loss": 0.0062655746107921, "ae_encoder_loss": 0.011885631216689944, "actor_loss": -97.01405389404297, "actor_target_entropy": -6.0, "actor_entropy": 4.793115348815918, "alpha_loss": -0.05594077749550343, "alpha_value": 0.028114523819008065, "duration": 15.597904682159424, "step": 68751}
{"episode_reward": -3.0, "episode": 277.0, "batch_reward": -0.0096328125, "critic_loss": 92.56438059997559, "ae_transition_loss": 0.00473284340556711, "ae_encoder_loss": 0.011758490219712257, "actor_loss": -97.25421362304688, "actor_target_entropy": -6.0, "actor_entropy": 4.633706401824951, "alpha_loss": -0.020833330892026426, "alpha_value": 0.032530946598687216, "duration": 15.551856279373169, "step": 69001}
{"episode_reward": -3.0, "episode": 278.0, "batch_reward": -0.007859375, "critic_loss": 83.83915873718261, "ae_transition_loss": 0.004837036184966564, "ae_encoder_loss": 0.011164786466397346, "actor_loss": -99.28121838378907, "actor_target_entropy": -6.0, "actor_entropy": 4.593320243835449, "alpha_loss": -0.0011784852524287998, "alpha_value": 0.034110767590316, "duration": 15.604029417037964, "step": 69251}
{"episode_reward": -3.0, "episode": 279.0, "batch_reward": -0.0075703125, "critic_loss": 76.92779066467286, "ae_transition_loss": 0.004857301335781813, "ae_encoder_loss": 0.010823920311406254, "actor_loss": -101.30220562744141, "actor_target_entropy": -6.0, "actor_entropy": 4.439325912475586, "alpha_loss": 0.003337732503656298, "alpha_value": 0.0339907167028318, "duration": 15.536160230636597, "step": 69501}
{"episode_reward": -3.0, "episode": 280.0, "batch_reward": -0.0082265625, "critic_loss": 84.56935884857178, "ae_transition_loss": 0.0056559487665072085, "ae_encoder_loss": 0.011074400899931789, "actor_loss": -102.8368755493164, "actor_target_entropy": -6.0, "actor_entropy": 4.712540729522705, "alpha_loss": 0.015388036554679274, "alpha_value": 0.03302500380131102, "duration": 15.692763090133667, "step": 69751}
{"episode_reward": -3.0, "episode": 281.0, "batch_reward": -0.0090859375, "critic_loss": 138.2832234954834, "ae_transition_loss": 0.021908836307004093, "ae_encoder_loss": 0.013902406411245466, "actor_loss": -102.44838006591797, "actor_target_entropy": -6.0, "actor_entropy": 4.860804588317871, "alpha_loss": 0.021318897414021196, "alpha_value": 0.03060540883544596, "duration": 214.3566837310791, "step": 70001}
{"episode_reward": -2.0, "episode": 282.0, "batch_reward": -0.008171875, "critic_loss": 134.23086171722412, "ae_transition_loss": 0.011110110851936042, "ae_encoder_loss": 0.012036579301580787, "actor_loss": -107.2679234008789, "actor_target_entropy": -6.0, "actor_entropy": 4.944228769302368, "alpha_loss": -0.04497520897909999, "alpha_value": 0.0310538949283763, "duration": 15.702964782714844, "step": 70251}
{"episode_reward": -3.0, "episode": 283.0, "batch_reward": -0.007546875, "critic_loss": 71.66730102539063, "ae_transition_loss": 0.004876827522180974, "ae_encoder_loss": 0.010614614164456726, "actor_loss": -107.67084497070313, "actor_target_entropy": -6.0, "actor_entropy": 5.0427578239440916, "alpha_loss": -0.0006006639795377851, "alpha_value": 0.0339946584364424, "duration": 15.487668752670288, "step": 70501}
{"episode_reward": -3.0, "episode": 284.0, "batch_reward": -0.00803125, "critic_loss": 73.18712461090088, "ae_transition_loss": 0.005603477851487696, "ae_encoder_loss": 0.011079867531079798, "actor_loss": -107.53503015136718, "actor_target_entropy": -6.0, "actor_entropy": 4.6718411521911625, "alpha_loss": -0.0045137886684387925, "alpha_value": 0.03357517171049514, "duration": 15.679811477661133, "step": 70751}
{"episode_reward": -3.0, "episode": 285.0, "batch_reward": -0.0085859375, "critic_loss": 259.5528466796875, "ae_transition_loss": 0.036793797673657536, "ae_encoder_loss": 0.015139265893027186, "actor_loss": -112.53825708007813, "actor_target_entropy": -6.0, "actor_entropy": 4.920642393112183, "alpha_loss": 0.02315690787881613, "alpha_value": 0.03205263760052049, "duration": 15.606901168823242, "step": 71001}
{"episode_reward": 0.0, "episode": 286.0, "batch_reward": -0.0075546875, "critic_loss": 228.91406814575194, "ae_transition_loss": 0.01937999473512173, "ae_encoder_loss": 0.014095775041729212, "actor_loss": -139.90783618164062, "actor_target_entropy": -6.0, "actor_entropy": 4.858672912597656, "alpha_loss": -0.048745376706123354, "alpha_value": 0.03481276476526302, "duration": 15.563929796218872, "step": 71251}
{"episode_reward": -3.0, "episode": 287.0, "batch_reward": -0.008296875, "critic_loss": 143.64724920654297, "ae_transition_loss": 0.0048513114396482706, "ae_encoder_loss": 0.011596687013283372, "actor_loss": -141.27170275878908, "actor_target_entropy": -6.0, "actor_entropy": 4.204308931350708, "alpha_loss": -0.008062756820581853, "alpha_value": 0.03617715766359432, "duration": 15.54125189781189, "step": 71501}
{"episode_reward": -3.0, "episode": 288.0, "batch_reward": -0.0080625, "critic_loss": 125.67299969482421, "ae_transition_loss": 0.004878507796674967, "ae_encoder_loss": 0.01134684394672513, "actor_loss": -139.35142590332032, "actor_target_entropy": -6.0, "actor_entropy": 4.118757646560669, "alpha_loss": -0.012949097132775932, "alpha_value": 0.03710993810439753, "duration": 15.576929330825806, "step": 71751}
{"episode_reward": -3.0, "episode": 289.0, "batch_reward": -0.008296875, "critic_loss": 118.62322520446777, "ae_transition_loss": 0.004968055935576558, "ae_encoder_loss": 0.011309225181117653, "actor_loss": -138.55123645019532, "actor_target_entropy": -6.0, "actor_entropy": 4.437614318847657, "alpha_loss": -0.010024330751039088, "alpha_value": 0.0382545707772675, "duration": 15.669260501861572, "step": 72001}
{"episode_reward": -3.0, "episode": 290.0, "batch_reward": -0.008390625, "critic_loss": 90.99342643737793, "ae_transition_loss": 0.0048433695435523985, "ae_encoder_loss": 0.0110204094145447, "actor_loss": -136.7307149658203, "actor_target_entropy": -6.0, "actor_entropy": 4.355519153594971, "alpha_loss": 0.001476209401153028, "alpha_value": 0.038987533673741546, "duration": 15.520731210708618, "step": 72251}
{"episode_reward": -3.0, "episode": 291.0, "batch_reward": -0.008296875, "critic_loss": 80.82628799438477, "ae_transition_loss": 0.004639493718277663, "ae_encoder_loss": 0.010904688093811274, "actor_loss": -136.05118103027343, "actor_target_entropy": -6.0, "actor_entropy": 4.210772884368897, "alpha_loss": 0.007562761371023953, "alpha_value": 0.038388882607111055, "duration": 15.694859504699707, "step": 72501}
{"episode_reward": -3.0, "episode": 292.0, "batch_reward": -0.0080859375, "critic_loss": 70.18926386260986, "ae_transition_loss": 0.0046734395595267415, "ae_encoder_loss": 0.010707429943606257, "actor_loss": -135.94325231933593, "actor_target_entropy": -6.0, "actor_entropy": 4.092100505828857, "alpha_loss": 0.01842591680213809, "alpha_value": 0.03690989670526701, "duration": 15.619436979293823, "step": 72751}
{"episode_reward": -3.0, "episode": 293.0, "batch_reward": -0.008671875, "critic_loss": 68.07514052581787, "ae_transition_loss": 0.004359235537238419, "ae_encoder_loss": 0.01063137586414814, "actor_loss": -135.33555383300782, "actor_target_entropy": -6.0, "actor_entropy": 3.9012719554901123, "alpha_loss": 0.015395168624818325, "alpha_value": 0.03476042225845612, "duration": 15.568527460098267, "step": 73001}
{"episode_reward": -3.0, "episode": 294.0, "batch_reward": -0.0086015625, "critic_loss": 65.71457317352295, "ae_transition_loss": 0.004547425141092389, "ae_encoder_loss": 0.010506392724812031, "actor_loss": -135.04776586914062, "actor_target_entropy": -6.0, "actor_entropy": 3.7705668392181395, "alpha_loss": 0.008425657172687352, "alpha_value": 0.03342247205822349, "duration": 15.49708366394043, "step": 73251}
{"episode_reward": -3.0, "episode": 295.0, "batch_reward": -0.0081171875, "critic_loss": 59.810532196044925, "ae_transition_loss": 0.004429434140212834, "ae_encoder_loss": 0.01058418435882777, "actor_loss": -134.43663232421875, "actor_target_entropy": -6.0, "actor_entropy": 3.660618362426758, "alpha_loss": 0.0141336604161188, "alpha_value": 0.031830364015826604, "duration": 15.646941900253296, "step": 73501}
{"episode_reward": -3.0, "episode": 296.0, "batch_reward": -0.0085625, "critic_loss": 59.53857874298096, "ae_transition_loss": 0.004559539821464569, "ae_encoder_loss": 0.010534598402678966, "actor_loss": -134.09604138183593, "actor_target_entropy": -6.0, "actor_entropy": 3.733481050491333, "alpha_loss": 0.010843109314329922, "alpha_value": 0.030305691608007477, "duration": 15.691253662109375, "step": 73751}
{"episode_reward": -3.0, "episode": 297.0, "batch_reward": -0.0089296875, "critic_loss": 54.946924667358395, "ae_transition_loss": 0.004522959313355386, "ae_encoder_loss": 0.010463890127837657, "actor_loss": -133.93633715820312, "actor_target_entropy": -6.0, "actor_entropy": 3.742105360031128, "alpha_loss": -0.0025061406386084856, "alpha_value": 0.029585244138744242, "duration": 15.565189361572266, "step": 74001}
{"episode_reward": -3.0, "episode": 298.0, "batch_reward": -0.0079453125, "critic_loss": 53.20850309753418, "ae_transition_loss": 0.0044443729752674695, "ae_encoder_loss": 0.010003210673108697, "actor_loss": -134.07849035644531, "actor_target_entropy": -6.0, "actor_entropy": 3.8767690048217776, "alpha_loss": -0.002015842266846448, "alpha_value": 0.030272725288487998, "duration": 15.639832258224487, "step": 74251}
{"episode_reward": 0.0, "episode": 299.0, "batch_reward": -0.0087109375, "critic_loss": 54.58313515472412, "ae_transition_loss": 0.0044479616163298485, "ae_encoder_loss": 0.01034861895814538, "actor_loss": -133.43367224121093, "actor_target_entropy": -6.0, "actor_entropy": 3.9811883640289305, "alpha_loss": -0.0010022300211712718, "alpha_value": 0.030008900947136256, "duration": 15.558819055557251, "step": 74501}
{"episode_reward": -3.0, "episode": 300.0, "batch_reward": -0.008453125, "critic_loss": 51.16500897216797, "ae_transition_loss": 0.004437555956654251, "ae_encoder_loss": 0.010260974279604852, "actor_loss": -133.58992626953125, "actor_target_entropy": -6.0, "actor_entropy": 3.87936342048645, "alpha_loss": -0.013725186032243073, "alpha_value": 0.031225979340493745, "duration": 15.690574645996094, "step": 74751}
{"episode_reward": -3.0, "episode": 301.0, "batch_reward": -0.0087734375, "critic_loss": 50.00594151306152, "ae_transition_loss": 0.004460555237252265, "ae_encoder_loss": 0.010303460241295397, "actor_loss": -133.29521240234374, "actor_target_entropy": -6.0, "actor_entropy": 4.054216255187988, "alpha_loss": -0.0039004638167098164, "alpha_value": 0.03291902476260242, "duration": 212.92241835594177, "step": 75001}
{"episode_reward": -3.0, "episode": 302.0, "batch_reward": -0.0078515625, "critic_loss": 48.292587326049805, "ae_transition_loss": 0.004551630034577102, "ae_encoder_loss": 0.009835054552182556, "actor_loss": -133.22707604980468, "actor_target_entropy": -6.0, "actor_entropy": 4.040381921768189, "alpha_loss": 0.00033860633708536627, "alpha_value": 0.03321516682301212, "duration": 15.303366661071777, "step": 75251}
{"episode_reward": -3.0, "episode": 303.0, "batch_reward": -0.0089453125, "critic_loss": 47.02215621566773, "ae_transition_loss": 0.004642154034227133, "ae_encoder_loss": 0.01021293520461768, "actor_loss": -133.32087731933595, "actor_target_entropy": -6.0, "actor_entropy": 4.2041135578155515, "alpha_loss": 0.0052077775280922655, "alpha_value": 0.03276054274653086, "duration": 15.421337127685547, "step": 75501}
{"episode_reward": -3.0, "episode": 304.0, "batch_reward": -0.0082421875, "critic_loss": 47.37813875579834, "ae_transition_loss": 0.004663255556952208, "ae_encoder_loss": 0.010193735860288143, "actor_loss": -133.24982006835938, "actor_target_entropy": -6.0, "actor_entropy": 4.033659465789795, "alpha_loss": 0.00284746852144599, "alpha_value": 0.031713015745156226, "duration": 15.52614951133728, "step": 75751}
{"episode_reward": -3.0, "episode": 305.0, "batch_reward": -0.0083515625, "critic_loss": 43.140327919006346, "ae_transition_loss": 0.004508513474371284, "ae_encoder_loss": 0.009765115665271879, "actor_loss": -133.35381164550782, "actor_target_entropy": -6.0, "actor_entropy": 4.034997631072998, "alpha_loss": -0.004510640021413564, "alpha_value": 0.03161983814146049, "duration": 15.319135665893555, "step": 76001}
{"episode_reward": -3.0, "episode": 306.0, "batch_reward": -0.0079765625, "critic_loss": 41.72387925720215, "ae_transition_loss": 0.004356922793202102, "ae_encoder_loss": 0.009692368950694799, "actor_loss": -133.16286145019532, "actor_target_entropy": -6.0, "actor_entropy": 4.093056957244873, "alpha_loss": 0.0008206267897039652, "alpha_value": 0.03219847408870529, "duration": 15.430007457733154, "step": 76251}
{"episode_reward": -3.0, "episode": 307.0, "batch_reward": -0.008078125, "critic_loss": 37.69347655487061, "ae_transition_loss": 0.004651367811951786, "ae_encoder_loss": 0.010135424324311316, "actor_loss": -133.03529772949219, "actor_target_entropy": -6.0, "actor_entropy": 4.094698276519775, "alpha_loss": 0.0006094051655381918, "alpha_value": 0.03232221464121719, "duration": 15.3849618434906, "step": 76501}
{"episode_reward": -3.0, "episode": 308.0, "batch_reward": -0.008515625, "critic_loss": 37.12440830612183, "ae_transition_loss": 0.004509049563203007, "ae_encoder_loss": 0.010133465350605548, "actor_loss": -133.26748461914062, "actor_target_entropy": -6.0, "actor_entropy": 4.083023504257202, "alpha_loss": 0.000162688328884542, "alpha_value": 0.03234310643523697, "duration": 15.506362438201904, "step": 76751}
{"episode_reward": -3.0, "episode": 309.0, "batch_reward": -0.0084140625, "critic_loss": 33.93556332397461, "ae_transition_loss": 0.004592653374187648, "ae_encoder_loss": 0.010118888033553958, "actor_loss": -133.05260827636718, "actor_target_entropy": -6.0, "actor_entropy": 4.058238836288452, "alpha_loss": 0.0005432123779319227, "alpha_value": 0.03242153404779494, "duration": 15.317301511764526, "step": 77001}
{"episode_reward": -3.0, "episode": 310.0, "batch_reward": -0.00903125, "critic_loss": 34.40812747192383, "ae_transition_loss": 0.004488152116537094, "ae_encoder_loss": 0.009903610759414733, "actor_loss": -133.10390234375, "actor_target_entropy": -6.0, "actor_entropy": 4.032172548294067, "alpha_loss": -0.005790216501336545, "alpha_value": 0.03290470970565137, "duration": 15.467056035995483, "step": 77251}
{"episode_reward": -3.0, "episode": 311.0, "batch_reward": -0.0085, "critic_loss": 33.04469672393799, "ae_transition_loss": 0.004463714240118861, "ae_encoder_loss": 0.010041207432746887, "actor_loss": -133.3115264892578, "actor_target_entropy": -6.0, "actor_entropy": 3.981894515991211, "alpha_loss": 0.0010904990271665155, "alpha_value": 0.033521580896395474, "duration": 15.383120775222778, "step": 77501}
{"episode_reward": -3.0, "episode": 312.0, "batch_reward": -0.008390625, "critic_loss": 37.77666646194458, "ae_transition_loss": 0.004607840937096625, "ae_encoder_loss": 0.010185524240136147, "actor_loss": -133.11162097167968, "actor_target_entropy": -6.0, "actor_entropy": 4.122755142211914, "alpha_loss": -0.0016254219324328006, "alpha_value": 0.03357056786335436, "duration": 15.46837329864502, "step": 77751}
{"episode_reward": -3.0, "episode": 313.0, "batch_reward": -0.008203125, "critic_loss": 35.13694638442993, "ae_transition_loss": 0.004545903852209448, "ae_encoder_loss": 0.01020429333858192, "actor_loss": -133.13531066894532, "actor_target_entropy": -6.0, "actor_entropy": 3.9758564472198485, "alpha_loss": -0.00027504489570856093, "alpha_value": 0.03369031133756443, "duration": 15.314009189605713, "step": 78001}
{"episode_reward": -3.0, "episode": 314.0, "batch_reward": -0.007921875, "critic_loss": 33.5650404586792, "ae_transition_loss": 0.004450274368282408, "ae_encoder_loss": 0.009847934935241937, "actor_loss": -133.08432580566407, "actor_target_entropy": -6.0, "actor_entropy": 3.9232975845336915, "alpha_loss": -0.0005464427054394037, "alpha_value": 0.03389912160552063, "duration": 15.525408744812012, "step": 78251}
{"episode_reward": 0.0, "episode": 315.0, "batch_reward": -0.00859375, "critic_loss": 33.68736276245117, "ae_transition_loss": 0.004630659786984325, "ae_encoder_loss": 0.00991297128237784, "actor_loss": -133.3092752685547, "actor_target_entropy": -6.0, "actor_entropy": 3.8843948764801026, "alpha_loss": -0.0014347431804053485, "alpha_value": 0.03438105617538492, "duration": 15.413695335388184, "step": 78501}
{"episode_reward": -3.0, "episode": 316.0, "batch_reward": -0.0084453125, "critic_loss": 32.66591339111328, "ae_transition_loss": 0.004429599051363766, "ae_encoder_loss": 0.009871799733489752, "actor_loss": -133.12628015136718, "actor_target_entropy": -6.0, "actor_entropy": 3.899628532409668, "alpha_loss": -0.0007869961191900075, "alpha_value": 0.034499950632026176, "duration": 15.421319007873535, "step": 78751}
{"episode_reward": -3.0, "episode": 317.0, "batch_reward": -0.008078125, "critic_loss": 31.197811714172364, "ae_transition_loss": 0.0043331233174540105, "ae_encoder_loss": 0.009456105309538543, "actor_loss": -133.23141540527342, "actor_target_entropy": -6.0, "actor_entropy": 3.8615352783203125, "alpha_loss": -0.005626156321726739, "alpha_value": 0.03568622348169901, "duration": 15.33427381515503, "step": 79001}
{"episode_reward": -3.0, "episode": 318.0, "batch_reward": -0.0076875, "critic_loss": 31.281635532379152, "ae_transition_loss": 0.004428729191422462, "ae_encoder_loss": 0.010010368891991675, "actor_loss": -132.99466796875, "actor_target_entropy": -6.0, "actor_entropy": 3.8922249813079834, "alpha_loss": -8.559942711144686e-05, "alpha_value": 0.036273207925748376, "duration": 15.828577518463135, "step": 79251}
{"episode_reward": -3.0, "episode": 319.0, "batch_reward": -0.0088671875, "critic_loss": 33.42076625442505, "ae_transition_loss": 0.004419783295597881, "ae_encoder_loss": 0.00996710715163499, "actor_loss": -132.75048852539064, "actor_target_entropy": -6.0, "actor_entropy": 4.006337308883667, "alpha_loss": 0.0017624865914694964, "alpha_value": 0.03631431173687456, "duration": 15.396643877029419, "step": 79501}
{"episode_reward": -1.0, "episode": 320.0, "batch_reward": -0.0082421875, "critic_loss": 33.30006204223633, "ae_transition_loss": 0.0045414511440321804, "ae_encoder_loss": 0.009842608639039099, "actor_loss": -132.45032885742188, "actor_target_entropy": -6.0, "actor_entropy": 4.0934895935058595, "alpha_loss": -0.0008004100695252419, "alpha_value": 0.03593017887187258, "duration": 16.002847909927368, "step": 79751}
{"episode_reward": 0.0, "episode": 321.0, "batch_reward": -0.00815625, "critic_loss": 33.77022631454468, "ae_transition_loss": 0.004577108983881772, "ae_encoder_loss": 0.010147465475834906, "actor_loss": -132.43319921875, "actor_target_entropy": -6.0, "actor_entropy": 4.105329818725586, "alpha_loss": 0.002785528020001948, "alpha_value": 0.03589282988087996, "duration": 214.22572588920593, "step": 80001}
{"episode_reward": 0.0, "episode": 322.0, "batch_reward": -0.0084453125, "critic_loss": 34.96666992950439, "ae_transition_loss": 0.00469362411973998, "ae_encoder_loss": 0.010310047997161747, "actor_loss": -132.0826895751953, "actor_target_entropy": -6.0, "actor_entropy": 4.107243310928345, "alpha_loss": -0.0013632111116312444, "alpha_value": 0.03611850781310559, "duration": 15.147518157958984, "step": 80251}
{"episode_reward": 0.0, "episode": 323.0, "batch_reward": -0.0085, "critic_loss": 35.122780529022215, "ae_transition_loss": 0.004570707980543375, "ae_encoder_loss": 0.009756669719703496, "actor_loss": -132.01474035644532, "actor_target_entropy": -6.0, "actor_entropy": 4.0792003326416015, "alpha_loss": -0.003968372724484652, "alpha_value": 0.036158829138907736, "duration": 15.2595055103302, "step": 80501}
{"episode_reward": 0.0, "episode": 324.0, "batch_reward": -0.0084296875, "critic_loss": 35.134752040863034, "ae_transition_loss": 0.004818224785849452, "ae_encoder_loss": 0.010247851395979524, "actor_loss": -131.61453576660156, "actor_target_entropy": -6.0, "actor_entropy": 4.1716005325317385, "alpha_loss": 0.005016295713372528, "alpha_value": 0.036982051892850976, "duration": 15.239500284194946, "step": 80751}
{"episode_reward": 0.0, "episode": 325.0, "batch_reward": -0.0092109375, "critic_loss": 37.80688128662109, "ae_transition_loss": 0.0046803048076108095, "ae_encoder_loss": 0.010186619237996637, "actor_loss": -130.80889465332032, "actor_target_entropy": -6.0, "actor_entropy": 4.113199317932129, "alpha_loss": 0.004370300740934909, "alpha_value": 0.03491515501603912, "duration": 15.875562906265259, "step": 81001}
{"episode_reward": 0.0, "episode": 326.0, "batch_reward": -0.008578125, "critic_loss": 38.03220027160644, "ae_transition_loss": 0.004747771731112152, "ae_encoder_loss": 0.010287027321755886, "actor_loss": -131.04014819335939, "actor_target_entropy": -6.0, "actor_entropy": 3.9811918411254883, "alpha_loss": 0.00147817567223683, "alpha_value": 0.03399624898471753, "duration": 15.034205675125122, "step": 81251}
{"episode_reward": 0.0, "episode": 327.0, "batch_reward": -0.008171875, "critic_loss": 37.29808385467529, "ae_transition_loss": 0.004678675887640565, "ae_encoder_loss": 0.010057712560519577, "actor_loss": -130.3272025756836, "actor_target_entropy": -6.0, "actor_entropy": 4.010453708648682, "alpha_loss": 0.003941483481787145, "alpha_value": 0.03291421362964861, "duration": 15.251582145690918, "step": 81501}
{"episode_reward": 0.0, "episode": 328.0, "batch_reward": -0.0084375, "critic_loss": 39.9053330154419, "ae_transition_loss": 0.0046489593968726695, "ae_encoder_loss": 0.009798341396264731, "actor_loss": -130.26593658447266, "actor_target_entropy": -6.0, "actor_entropy": 3.8702041664123534, "alpha_loss": -0.0008938855296000838, "alpha_value": 0.03208845359102029, "duration": 15.245471477508545, "step": 81751}
{"episode_reward": 0.0, "episode": 329.0, "batch_reward": -0.008140625, "critic_loss": 38.38110755157471, "ae_transition_loss": 0.004515811052639037, "ae_encoder_loss": 0.009956859383732081, "actor_loss": -129.90161993408202, "actor_target_entropy": -6.0, "actor_entropy": 3.7853022785186767, "alpha_loss": 0.0014646714595146478, "alpha_value": 0.032154261263878776, "duration": 15.251418113708496, "step": 82001}
{"episode_reward": 0.0, "episode": 330.0, "batch_reward": -0.0081640625, "critic_loss": 40.13303525543213, "ae_transition_loss": 0.004635027014650404, "ae_encoder_loss": 0.009762658859603106, "actor_loss": -129.64366497802735, "actor_target_entropy": -6.0, "actor_entropy": 3.7786133842468264, "alpha_loss": -0.0067691721324808895, "alpha_value": 0.032741036192168435, "duration": 15.152209520339966, "step": 82251}
{"episode_reward": 0.0, "episode": 331.0, "batch_reward": -0.0085546875, "critic_loss": 39.673903488159176, "ae_transition_loss": 0.004617500937543809, "ae_encoder_loss": 0.00964327632356435, "actor_loss": -129.215634765625, "actor_target_entropy": -6.0, "actor_entropy": 3.8662673034667967, "alpha_loss": 0.0035749749229289592, "alpha_value": 0.0330629012155865, "duration": 15.304090738296509, "step": 82501}
{"episode_reward": 0.0, "episode": 332.0, "batch_reward": -0.008265625, "critic_loss": 42.98135257720947, "ae_transition_loss": 0.0047524194996804, "ae_encoder_loss": 0.009805179215967656, "actor_loss": -128.68813006591796, "actor_target_entropy": -6.0, "actor_entropy": 3.7572578926086426, "alpha_loss": 0.0011719380458816886, "alpha_value": 0.032494311599974804, "duration": 15.285068273544312, "step": 82751}
{"episode_reward": 0.0, "episode": 333.0, "batch_reward": -0.0081796875, "critic_loss": 42.425187423706056, "ae_transition_loss": 0.004958141572773456, "ae_encoder_loss": 0.010244899235665798, "actor_loss": -128.1999789428711, "actor_target_entropy": -6.0, "actor_entropy": 3.6964687442779542, "alpha_loss": 0.003550457743462175, "alpha_value": 0.03193217557187721, "duration": 15.228392124176025, "step": 83001}
{"episode_reward": 0.0, "episode": 334.0, "batch_reward": -0.0083125, "critic_loss": 42.57153573608399, "ae_transition_loss": 0.004833061599172652, "ae_encoder_loss": 0.009826075324788689, "actor_loss": -127.47502545166016, "actor_target_entropy": -6.0, "actor_entropy": 3.8604235038757326, "alpha_loss": 0.004094298128969967, "alpha_value": 0.030847889516496883, "duration": 15.134190082550049, "step": 83251}
{"episode_reward": 0.0, "episode": 335.0, "batch_reward": -0.008, "critic_loss": 40.06139601135254, "ae_transition_loss": 0.004853314258623868, "ae_encoder_loss": 0.009745834314264358, "actor_loss": -126.98351745605468, "actor_target_entropy": -6.0, "actor_entropy": 3.661395050048828, "alpha_loss": -0.005254253095248714, "alpha_value": 0.03041693343037391, "duration": 15.346725225448608, "step": 83501}
{"episode_reward": 0.0, "episode": 336.0, "batch_reward": -0.0080546875, "critic_loss": 42.509549346923826, "ae_transition_loss": 0.004903401585295796, "ae_encoder_loss": 0.009906848853453994, "actor_loss": -126.8398125, "actor_target_entropy": -6.0, "actor_entropy": 3.7687722091674805, "alpha_loss": 0.0014392116395756601, "alpha_value": 0.03109975776631852, "duration": 15.293444156646729, "step": 83751}
{"episode_reward": 0.0, "episode": 337.0, "batch_reward": -0.0084921875, "critic_loss": 39.82986584854126, "ae_transition_loss": 0.004914201236329973, "ae_encoder_loss": 0.01002501517534256, "actor_loss": -126.35717346191406, "actor_target_entropy": -6.0, "actor_entropy": 3.905395065307617, "alpha_loss": -0.0032728031519800426, "alpha_value": 0.03155467128416904, "duration": 15.221924304962158, "step": 84001}
{"episode_reward": 0.0, "episode": 338.0, "batch_reward": -0.0080703125, "critic_loss": 41.24953445434571, "ae_transition_loss": 0.0047875031917355955, "ae_encoder_loss": 0.009670243186876178, "actor_loss": -126.1640595703125, "actor_target_entropy": -6.0, "actor_entropy": 3.9867790718078613, "alpha_loss": -0.0012027643928304314, "alpha_value": 0.03188944440833128, "duration": 15.134949684143066, "step": 84251}
{"episode_reward": 0.0, "episode": 339.0, "batch_reward": -0.0082265625, "critic_loss": 39.196002517700194, "ae_transition_loss": 0.004768245786894113, "ae_encoder_loss": 0.009975089381448924, "actor_loss": -125.58020324707032, "actor_target_entropy": -6.0, "actor_entropy": 4.202442899703979, "alpha_loss": -0.004926689627580345, "alpha_value": 0.0339696391688729, "duration": 15.363107681274414, "step": 84501}
{"episode_reward": 0.0, "episode": 340.0, "batch_reward": -0.00784375, "critic_loss": 44.14348738098145, "ae_transition_loss": 0.0047907022051513195, "ae_encoder_loss": 0.00993597874045372, "actor_loss": -125.6356455078125, "actor_target_entropy": -6.0, "actor_entropy": 4.198365873336792, "alpha_loss": -0.0022524353805929423, "alpha_value": 0.03478661648308194, "duration": 15.34896993637085, "step": 84751}
{"episode_reward": 0.0, "episode": 341.0, "batch_reward": -0.008140625, "critic_loss": 43.67855772399902, "ae_transition_loss": 0.004954665057361126, "ae_encoder_loss": 0.009656772319227456, "actor_loss": -125.31668255615234, "actor_target_entropy": -6.0, "actor_entropy": 4.2282636318206785, "alpha_loss": 0.0005741452714428305, "alpha_value": 0.03490134991585176, "duration": 3106.0340251922607, "step": 85001}
