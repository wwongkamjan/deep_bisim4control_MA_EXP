{"episode_reward": 0.0, "episode": 1.0, "duration": 17.37030577659607, "step": 250}
{"episode_reward": 7.747346039045906, "episode": 2.0, "duration": 0.39629173278808594, "step": 500}
{"episode_reward": 4.427573337125385, "episode": 3.0, "duration": 0.3973729610443115, "step": 750}
{"episode_reward": 5.050386443524035, "episode": 4.0, "duration": 0.3939809799194336, "step": 1000}
{"episode_reward": 17.85499204884596, "episode": 5.0, "batch_reward": 0.035874925002014287, "critic_loss": 0.005948310648594454, "ae_transition_loss": 0.014027779698891981, "ae_encoder_loss": 0.0011022657760809673, "actor_loss": -0.2396030055443521, "actor_target_entropy": -6.0, "actor_entropy": 6.7430880634011405, "alpha_loss": 0.05800701299884433, "alpha_value": 0.006444311760373616, "duration": 282.4595944881439, "step": 1250}
{"episode_reward": 17.619753861327165, "episode": 6.0, "batch_reward": 0.04136460544168949, "critic_loss": 0.0028050049301236867, "ae_transition_loss": 0.004069329447112978, "ae_encoder_loss": 0.0019173097782768309, "actor_loss": -0.41922230315208436, "actor_target_entropy": -6.0, "actor_entropy": 5.578928707122802, "alpha_loss": 0.026885555669665336, "alpha_value": 0.004057415615215967, "duration": 54.364832162857056, "step": 1500}
{"episode_reward": 10.866817997196629, "episode": 7.0, "batch_reward": 0.04328456543385983, "critic_loss": 0.002703091130591929, "ae_transition_loss": 0.0029779774527996777, "ae_encoder_loss": 0.0014662432440090925, "actor_loss": -0.4609796366691589, "actor_target_entropy": -6.0, "actor_entropy": 5.612793739318848, "alpha_loss": 0.02275526751577854, "alpha_value": 0.003817596664756189, "duration": 54.38747692108154, "step": 1750}
{"episode_reward": 12.649727127788296, "episode": 8.0, "batch_reward": 0.046004964977502824, "critic_loss": 0.0035715569406747816, "ae_transition_loss": 0.003145639952272177, "ae_encoder_loss": 0.0016268126836512238, "actor_loss": -0.5225743927955627, "actor_target_entropy": -6.0, "actor_entropy": 5.556353904724121, "alpha_loss": 0.019665408313274384, "alpha_value": 0.0036147353718019638, "duration": 54.35116457939148, "step": 2000}
{"episode_reward": 20.61403232617061, "episode": 9.0, "batch_reward": 0.049378174424171445, "critic_loss": 0.0033891303930431603, "ae_transition_loss": 0.002622981236316264, "ae_encoder_loss": 0.0015724399210885168, "actor_loss": -0.6195255851745606, "actor_target_entropy": -6.0, "actor_entropy": 4.947355442047119, "alpha_loss": 0.016346029810607433, "alpha_value": 0.00343964390841624, "duration": 54.35305666923523, "step": 2250}
{"episode_reward": 14.969592769864532, "episode": 10.0, "batch_reward": 0.05027927093207836, "critic_loss": 0.005566174304112792, "ae_transition_loss": 0.003534275862388313, "ae_encoder_loss": 0.001637606360251084, "actor_loss": -0.674100932598114, "actor_target_entropy": -6.0, "actor_entropy": 4.929400981903076, "alpha_loss": 0.013086619451642037, "alpha_value": 0.0032873624713400753, "duration": 54.36544489860535, "step": 2500}
{"episode_reward": 13.273490632513349, "episode": 11.0, "batch_reward": 0.050062735572457315, "critic_loss": 0.0045376883074641225, "ae_transition_loss": 0.0026888803355395795, "ae_encoder_loss": 0.0015476497188210488, "actor_loss": -0.7279383974075317, "actor_target_entropy": -6.0, "actor_entropy": 4.4131842098236085, "alpha_loss": 0.009455240085721016, "alpha_value": 0.0031717041782766673, "duration": 70.18278622627258, "step": 2750}
{"episode_reward": 15.093911191594632, "episode": 12.0, "batch_reward": 0.05104514949023724, "critic_loss": 0.0647163082966581, "ae_transition_loss": 0.005325384760275483, "ae_encoder_loss": 0.0016611859586555511, "actor_loss": -0.7399349796772003, "actor_target_entropy": -6.0, "actor_entropy": 4.878324361801147, "alpha_loss": 0.006504980391007848, "alpha_value": 0.003083167770664748, "duration": 54.345189332962036, "step": 3000}
{"episode_reward": 13.666147693743884, "episode": 13.0, "batch_reward": 0.05162661217153072, "critic_loss": 0.006386505657806993, "ae_transition_loss": 0.0021944867484271526, "ae_encoder_loss": 0.0013321158238686621, "actor_loss": -0.7208265566825867, "actor_target_entropy": -6.0, "actor_entropy": 5.151317756652832, "alpha_loss": 0.010424594968557358, "alpha_value": 0.002996414358203872, "duration": 54.36609673500061, "step": 3250}
{"episode_reward": 12.484970346034839, "episode": 14.0, "batch_reward": 0.05281812627613545, "critic_loss": 0.004719136838801205, "ae_transition_loss": 0.002058270837645978, "ae_encoder_loss": 0.0011991005134768785, "actor_loss": -0.7412567610740661, "actor_target_entropy": -6.0, "actor_entropy": 4.53094140625, "alpha_loss": 0.007928811818361282, "alpha_value": 0.0028872657982097812, "duration": 54.34593343734741, "step": 3500}
{"episode_reward": 20.229850322032053, "episode": 15.0, "batch_reward": 0.05424199476838112, "critic_loss": 0.006812013083137572, "ae_transition_loss": 0.002393990875221789, "ae_encoder_loss": 0.0011482519234996289, "actor_loss": -0.761798315525055, "actor_target_entropy": -6.0, "actor_entropy": 4.202812215805054, "alpha_loss": 0.005426062736660242, "alpha_value": 0.002810270499487753, "duration": 54.35881567001343, "step": 3750}
{"episode_reward": 16.830397421925944, "episode": 16.0, "batch_reward": 0.0547325649857521, "critic_loss": 0.009305114816874266, "ae_transition_loss": 0.0028032187293283643, "ae_encoder_loss": 0.0012223742445930839, "actor_loss": -0.7968260102272033, "actor_target_entropy": -6.0, "actor_entropy": 4.408949188232421, "alpha_loss": 0.005208515438716858, "alpha_value": 0.0027434114607018446, "duration": 54.35569214820862, "step": 4000}
{"episode_reward": 16.827243734922547, "episode": 17.0, "batch_reward": 0.05753884750604629, "critic_loss": 0.0076560669671744104, "ae_transition_loss": 0.002356343172956258, "ae_encoder_loss": 0.0014635849720798432, "actor_loss": -0.8563093175888061, "actor_target_entropy": -6.0, "actor_entropy": 3.7700651721954346, "alpha_loss": 0.0030230580042116344, "alpha_value": 0.0026854952947348402, "duration": 54.3300518989563, "step": 4250}
{"episode_reward": 29.087130455294727, "episode": 18.0, "batch_reward": 0.05991148833930492, "critic_loss": 0.0059186196224763985, "ae_transition_loss": 0.0023307841839268805, "ae_encoder_loss": 0.0016842653141357004, "actor_loss": -0.9064232025146485, "actor_target_entropy": -6.0, "actor_entropy": 3.8014449424743653, "alpha_loss": 0.0019000492347986436, "alpha_value": 0.0026508711056496687, "duration": 54.33319044113159, "step": 4500}
{"episode_reward": 25.47513183063494, "episode": 19.0, "batch_reward": 0.06282917788624763, "critic_loss": 0.007014150200411677, "ae_transition_loss": 0.0026355965370312333, "ae_encoder_loss": 0.0020184774505905808, "actor_loss": -0.9761188082695007, "actor_target_entropy": -6.0, "actor_entropy": 3.826707326889038, "alpha_loss": -0.0010616820771247148, "alpha_value": 0.0026478640557429816, "duration": 54.35068702697754, "step": 4750}
{"episode_reward": 26.830486551154067, "episode": 20.0, "batch_reward": 0.06409993983805179, "critic_loss": 0.008263567136600614, "ae_transition_loss": 0.0026956348270177843, "ae_encoder_loss": 0.0020527898841537534, "actor_loss": -1.0552782955169677, "actor_target_entropy": -6.0, "actor_entropy": 3.6779011268615722, "alpha_loss": -0.0009112254939391278, "alpha_value": 0.0026624673220260187, "duration": 54.40652775764465, "step": 5000}
{"episode_reward": 30.518158273376635, "episode": 21.0, "batch_reward": 0.06771475167572498, "critic_loss": 0.009874068580567837, "ae_transition_loss": 0.002865776957012713, "ae_encoder_loss": 0.0022860559872351586, "actor_loss": -1.1522318592071534, "actor_target_entropy": -6.0, "actor_entropy": 3.6321352252960204, "alpha_loss": -0.001875468710262794, "alpha_value": 0.0026810434038860385, "duration": 70.3234531879425, "step": 5250}
{"episode_reward": 31.46176210637666, "episode": 22.0, "batch_reward": 0.07329168820381164, "critic_loss": 0.009844143832102418, "ae_transition_loss": 0.0031798760890960694, "ae_encoder_loss": 0.0033212104351259767, "actor_loss": -1.2460291719436645, "actor_target_entropy": -6.0, "actor_entropy": 3.738600326538086, "alpha_loss": -0.003437585049076006, "alpha_value": 0.0027279486715918156, "duration": 54.32984519004822, "step": 5500}
{"episode_reward": 49.590425120097706, "episode": 23.0, "batch_reward": 0.0758259412497282, "critic_loss": 0.011732996759936214, "ae_transition_loss": 0.0038218782786279917, "ae_encoder_loss": 0.004159051955677569, "actor_loss": -1.3478452825546265, "actor_target_entropy": -6.0, "actor_entropy": 3.8085876750946044, "alpha_loss": -0.004144396776799113, "alpha_value": 0.002801906791147335, "duration": 54.213451862335205, "step": 5750}
{"episode_reward": 18.8054634116654, "episode": 24.0, "batch_reward": 0.07618026898801326, "critic_loss": 0.011697388216853143, "ae_transition_loss": 0.0037905925661325455, "ae_encoder_loss": 0.004073526890948415, "actor_loss": -1.4501439361572266, "actor_target_entropy": -6.0, "actor_entropy": 3.7406408042907713, "alpha_loss": -0.004783914351603016, "alpha_value": 0.002896855145776217, "duration": 54.20433592796326, "step": 6000}
{"episode_reward": 26.529693118086964, "episode": 25.0, "batch_reward": 0.07779921415448189, "critic_loss": 0.012750555049628019, "ae_transition_loss": 0.00400253571383655, "ae_encoder_loss": 0.004305923771113157, "actor_loss": -1.5582861194610595, "actor_target_entropy": -6.0, "actor_entropy": 3.7167012252807616, "alpha_loss": -0.004369717510417104, "alpha_value": 0.003004424153103921, "duration": 54.192928314208984, "step": 6250}
{"episode_reward": 39.30456111301102, "episode": 26.0, "batch_reward": 0.07932324209809304, "critic_loss": 0.0138655672557652, "ae_transition_loss": 0.004192794012837112, "ae_encoder_loss": 0.004592511779628694, "actor_loss": -1.6535409049987793, "actor_target_entropy": -6.0, "actor_entropy": 3.5703888168334963, "alpha_loss": -0.0033882522601634264, "alpha_value": 0.0031045188993548438, "duration": 54.202528953552246, "step": 6500}
{"episode_reward": 17.095085747941297, "episode": 27.0, "batch_reward": 0.07939987227320672, "critic_loss": 0.014672975983470678, "ae_transition_loss": 0.004291674107313156, "ae_encoder_loss": 0.004409215342253446, "actor_loss": -1.745805742263794, "actor_target_entropy": -6.0, "actor_entropy": 3.594420316696167, "alpha_loss": -0.00318987931124866, "alpha_value": 0.0031973483875518353, "duration": 54.257272720336914, "step": 6750}
{"episode_reward": 17.768177419939718, "episode": 28.0, "batch_reward": 0.08054985657334328, "critic_loss": 0.01625350132957101, "ae_transition_loss": 0.004475509363226593, "ae_encoder_loss": 0.004777569802477956, "actor_loss": -1.8459977865219117, "actor_target_entropy": -6.0, "actor_entropy": 3.698402765274048, "alpha_loss": -0.0033579693250358105, "alpha_value": 0.0032952575000730774, "duration": 54.21088528633118, "step": 7000}
{"episode_reward": 34.15053429982318, "episode": 29.0, "batch_reward": 0.08219990983605385, "critic_loss": 0.015491511296480895, "ae_transition_loss": 0.004529584808275104, "ae_encoder_loss": 0.004831091916188598, "actor_loss": -1.9624947299957276, "actor_target_entropy": -6.0, "actor_entropy": 3.71278314781189, "alpha_loss": -0.0032730595879256726, "alpha_value": 0.0034056859994504324, "duration": 54.2090950012207, "step": 7250}
{"episode_reward": 30.138071386224347, "episode": 30.0, "batch_reward": 0.08411877191066743, "critic_loss": 0.021277958836406468, "ae_transition_loss": 0.005219978235661983, "ae_encoder_loss": 0.005355877639725805, "actor_loss": -2.0679117183685305, "actor_target_entropy": -6.0, "actor_entropy": 3.88811386680603, "alpha_loss": -0.0027910776122007518, "alpha_value": 0.003516908545108431, "duration": 54.24444270133972, "step": 7500}
